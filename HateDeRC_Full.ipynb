{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamesalv/HateDeRC/blob/master/HateDeRC_Full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "24658442",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24658442",
        "outputId": "0bf9502e-fbd3-4a1f-915b-556e4ffc42c3"
      },
      "outputs": [],
      "source": [
        "# import shutil\n",
        "# import os\n",
        "\n",
        "# # Delete HateDeRC directory if it exists\n",
        "# if os.path.exists('HateDeRC'):\n",
        "#   shutil.rmtree('HateDeRC')\n",
        "# !git clone https://github.com/jamesalv/HateDeRC\n",
        "# %cd HateDeRC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8bccce96",
      "metadata": {
        "id": "8bccce96"
      },
      "outputs": [],
      "source": [
        "from TrainingConfig import TrainingConfig\n",
        "from typing import Dict, Any, Tuple, List\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1f654143",
      "metadata": {
        "id": "1f654143"
      },
      "outputs": [],
      "source": [
        "data_path = 'Data/dataset.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b3618d6e",
      "metadata": {
        "id": "b3618d6e"
      },
      "outputs": [],
      "source": [
        "config = TrainingConfig()\n",
        "\n",
        "config.class_weighting = True\n",
        "config.num_epochs = 2\n",
        "config.hidden_dropout_prob = 0.2\n",
        "\n",
        "# Attention Training Configurations\n",
        "config.train_attention = True\n",
        "config.lambda_attn = 1\n",
        "config.ranking_margin = 0.1        # Minimum margin between token pairs\n",
        "config.ranking_threshold = 0.05    # Min difference to consider pairs significant\n",
        "\n",
        "# \n",
        "config.use_multi_layer_loss = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "487ff027",
      "metadata": {
        "id": "487ff027"
      },
      "outputs": [],
      "source": [
        "# Seed all randomness for reproducibility\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(config.seed)\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.manual_seed_all(config.seed)\n",
        "np.random.seed(config.seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53d93027",
      "metadata": {
        "id": "53d93027"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f81645fa",
      "metadata": {
        "id": "f81645fa"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def deobfuscate_text(text):\n",
        "    \"\"\"\n",
        "    Normalize common text obfuscation patterns to reveal original words.\n",
        "    Useful for hate speech detection and content analysis.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text with potential obfuscations\n",
        "\n",
        "    Returns:\n",
        "        str: Text with obfuscations normalized\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "\n",
        "    # Make a copy to work with\n",
        "    result = text.lower()\n",
        "\n",
        "    # 1. Handle asterisk/symbol replacements\n",
        "    symbol_patterns = {\n",
        "        # Common profanity\n",
        "        r'f\\*+c?k': 'fuck',\n",
        "        r'f\\*+': 'fuck',\n",
        "        r's\\*+t': 'shit',\n",
        "        r'b\\*+ch': 'bitch',\n",
        "        r'a\\*+s': 'ass',\n",
        "        r'd\\*+n': 'damn',\n",
        "        r'h\\*+l': 'hell',\n",
        "        r'c\\*+p': 'crap',\n",
        "\n",
        "        # Slurs and hate speech terms (be comprehensive for detection)\n",
        "        r'n\\*+g+[aer]+': 'nigger',  # Various n-word obfuscations\n",
        "        r'f\\*+g+[ot]*': 'faggot',\n",
        "        r'r\\*+[dt]ard': 'retard',\n",
        "        r'sp\\*+c': 'spic',\n",
        "\n",
        "        # Other symbols\n",
        "        r'@ss': 'ass',\n",
        "        r'b@tch': 'bitch',\n",
        "        r'sh!t': 'shit',\n",
        "        r'f#ck': 'fuck',\n",
        "        r'd@mn': 'damn',\n",
        "    }\n",
        "\n",
        "    for pattern, replacement in symbol_patterns.items():\n",
        "        result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)\n",
        "\n",
        "    # 2. Handle character spacing (f u c k -> fuck)\n",
        "    spacing_patterns = {\n",
        "        r'\\bf\\s+u\\s+c\\s+k\\b': 'fuck',\n",
        "        r'\\bs\\s+h\\s+i\\s+t\\b': 'shit',\n",
        "        r'\\bd\\s+a\\s+m\\s+n\\b': 'damn',\n",
        "        r'\\bh\\s+e\\s+l\\s+l\\b': 'hell',\n",
        "        r'\\ba\\s+s\\s+s\\b': 'ass',\n",
        "        r'\\bc\\s+r\\s+a\\s+p\\b': 'crap',\n",
        "    }\n",
        "\n",
        "    for pattern, replacement in spacing_patterns.items():\n",
        "        result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)\n",
        "\n",
        "    # 3. Handle number/letter substitutions\n",
        "    leet_patterns = {\n",
        "        # Basic leet speak\n",
        "        r'\\b3\\s*1\\s*1\\s*3\\b': 'elle',  # 3113 -> elle\n",
        "        r'\\bf4g\\b': 'fag',\n",
        "        r'\\bf4gg0t\\b': 'faggot',\n",
        "        r'\\bn00b\\b': 'noob',\n",
        "        r'\\bl33t\\b': 'leet',\n",
        "        r'\\bh4t3\\b': 'hate',\n",
        "        r'\\b5h1t\\b': 'shit',\n",
        "        r'\\bf0ck\\b': 'fock',\n",
        "    }\n",
        "\n",
        "    for pattern, replacement in leet_patterns.items():\n",
        "        result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)\n",
        "\n",
        "    # 4. Handle repeated characters and separators\n",
        "    # Remove excessive punctuation between letters\n",
        "    result = re.sub(r'([a-z])[^\\w\\s]+([a-z])', r'\\1\\2', result)\n",
        "\n",
        "    # Handle underscore separation\n",
        "    result = re.sub(r'([a-z])_+([a-z])', r'\\1\\2', result)\n",
        "\n",
        "    # Handle dot separation\n",
        "    result = re.sub(r'([a-z])\\.+([a-z])', r'\\1\\2', result)\n",
        "\n",
        "    # 5. Handle common misspellings/variations used for evasion\n",
        "    evasion_patterns = {\n",
        "        r'\\bfuk\\b': 'fuck',\n",
        "        r'\\bfuq\\b': 'fuck',\n",
        "        r'\\bfck\\b': 'fuck',\n",
        "        r'\\bshyt\\b': 'shit',\n",
        "        r'\\bshit\\b': 'shit',\n",
        "        r'\\bbiatch\\b': 'bitch',\n",
        "        r'\\bbeatch\\b': 'bitch',\n",
        "        r'\\basshole\\b': 'asshole',\n",
        "        r'\\ba55hole\\b': 'asshole',\n",
        "        r'\\btard\\b': 'retard',\n",
        "        r'\\bfagg\\b': 'fag',\n",
        "    }\n",
        "\n",
        "    for pattern, replacement in evasion_patterns.items():\n",
        "        result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)\n",
        "\n",
        "    # 6. Clean up multiple spaces\n",
        "    result = re.sub(r'\\s+', ' ', result).strip()\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f070b12b",
      "metadata": {
        "id": "f070b12b"
      },
      "outputs": [],
      "source": [
        "def aggregate_rationales(rationales, labels, post_length, drop_abnormal=False):\n",
        "    \"\"\"\n",
        "    If all 3 annotators are normal → 3 zero spans → average (all zeros).\n",
        "    If k annotators are non-normal and k spans exist → average the k spans (no added zeros).\n",
        "    If k non-normal but fewer than k spans:\n",
        "        If the missing annotators are non-normal → do not fill with zeros; average only existing spans and record rationale_support = #spans.\n",
        "        If the missing annotators are normal (e.g., 2 hate + 1 normal + 2 spans) → append one zero span for the normal.\n",
        "    \"\"\"\n",
        "    count_normal = labels.count(0)\n",
        "    count_hate = labels.count(1)\n",
        "    count_rationales = len(rationales)\n",
        "    pad = np.zeros(post_length, dtype=\"int\").tolist()\n",
        "\n",
        "    # If there are hate labels but no rationales, something is wrong\n",
        "    if count_hate > 0 and count_rationales == 0:\n",
        "        if drop_abnormal:\n",
        "            return None\n",
        "\n",
        "        # Else just fill with 0\n",
        "        return np.zeros(post_length).tolist()\n",
        "\n",
        "    # If all annotators are normal, return all zeros\n",
        "    if count_normal == 3:\n",
        "        return np.zeros(post_length).tolist()\n",
        "\n",
        "    # If we have hate annotators\n",
        "    if count_hate > 0:\n",
        "        # Case 1: Number of rationales matches number of hate annotators\n",
        "        if count_rationales == count_hate:\n",
        "            return np.average(rationales, axis=0).tolist()\n",
        "\n",
        "        # Case 2: Fewer rationales than hate annotators\n",
        "        elif count_rationales < count_hate:\n",
        "            # Add zero padding for normal annotators only\n",
        "            rationales_copy = rationales.copy()\n",
        "            zeros_to_add = count_normal\n",
        "            for _ in range(zeros_to_add):\n",
        "                rationales_copy.append(pad)\n",
        "            return np.average(rationales_copy, axis=0).tolist()\n",
        "\n",
        "        # Case 3: More rationales than hate annotators (shouldn't happen normally)\n",
        "        else:\n",
        "            # Just average what we have\n",
        "            return np.average(rationales, axis=0).tolist()\n",
        "\n",
        "    # Fallback: return zeros if no clear case matches\n",
        "    return np.zeros(post_length).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8d446ddb",
      "metadata": {
        "id": "8d446ddb"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "def preprocess_text(raw_text):\n",
        "    preprocessed_text = raw_text\n",
        "    # # Remove HTML tags <>\n",
        "    preprocessed_text = preprocessed_text.replace(\"<\", \"\").replace(\">\", \"\")\n",
        "    # # De-Obsfucate Patterns\n",
        "    preprocessed_text = deobfuscate_text(preprocessed_text)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "\n",
        "def create_text_segment(\n",
        "    text_tokens: List[str], rationale_mask: List[int]\n",
        ") -> List[Tuple[List[str], int]]:\n",
        "    \"\"\"\n",
        "    Process a rationale mask to identify contiguous segments of highlighted text.\n",
        "    Then create a segmented representation of the tokens\n",
        "\n",
        "    Args:\n",
        "        text_tokens: Original text tokens\n",
        "        mask: Binary mask where 1 indicates a highlighted token (this consists of mask from 3 annotators)\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples (text segment, mask value)\n",
        "    \"\"\"\n",
        "    # Handle case where mask is empty (no rationale provided), usually this is normal classification\n",
        "    mask = rationale_mask\n",
        "\n",
        "    # for mask in all_rationale_mask:\n",
        "    # Find breakpoints (transitions between highlighted/1 and non-highlighted/0)\n",
        "    breakpoints = []\n",
        "    mask_values = []\n",
        "\n",
        "    # Always start with position 0\n",
        "    breakpoints.append(0)\n",
        "    mask_values.append(mask[0])\n",
        "\n",
        "    # Find transitions in the mask\n",
        "    for i in range(1, len(mask)):\n",
        "        if mask[i] != mask[i - 1]:\n",
        "            breakpoints.append(i)\n",
        "            mask_values.append(mask[i])\n",
        "\n",
        "    # Always end with the length of the text\n",
        "    if breakpoints[-1] != len(mask):\n",
        "        breakpoints.append(len(mask))\n",
        "\n",
        "    # Create segments based on breakpoints\n",
        "    segments = []\n",
        "    for i in range(len(breakpoints) - 1):\n",
        "        start = breakpoints[i]\n",
        "        end = breakpoints[i + 1]\n",
        "        segments.append((text_tokens[start:end], mask_values[i]))\n",
        "\n",
        "    return segments\n",
        "\n",
        "\n",
        "def align_rationales(tokens, rationales, tokenizer, max_length=128):\n",
        "    \"\"\"\n",
        "    Align rationales with tokenized text while handling different tokenizer formats.\n",
        "\n",
        "    Args:\n",
        "        tokens: Original text tokens\n",
        "        rationales: Original rationale masks\n",
        "        tokenizer: The tokenizer to use\n",
        "        max_length: Maximum sequence length\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with tokenized inputs and aligned rationale masks\n",
        "    \"\"\"\n",
        "    segments = create_text_segment(tokens, rationales)\n",
        "    all_human_rationales = []\n",
        "    all_input_ids = []\n",
        "    all_attention_mask = []\n",
        "    all_token_type_ids = []\n",
        "    all_rationales = []\n",
        "    for text_segment, rationale_value in segments:\n",
        "        inputs = {}\n",
        "        concatenated_text = \" \".join(text_segment)\n",
        "        processed_segment = preprocess_text(concatenated_text)\n",
        "        tokenized = tokenizer(\n",
        "            processed_segment, add_special_tokens=False, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Extract the relevant data\n",
        "        segment_input_ids = tokenized[\"input_ids\"][0]\n",
        "        segment_attention_mask = tokenized[\"attention_mask\"][0]\n",
        "        # Handle token_type_ids if present\n",
        "        if \"token_type_ids\" in tokenized:\n",
        "            segment_token_type_ids = tokenized[\"token_type_ids\"][0]\n",
        "            all_token_type_ids.extend(segment_token_type_ids)\n",
        "\n",
        "        # Add input IDs and attention mask\n",
        "        all_input_ids.extend(segment_input_ids)\n",
        "        all_attention_mask.extend(segment_attention_mask)\n",
        "\n",
        "        # Add rationales (excluding special tokens)\n",
        "        segment_rationales = [rationale_value] * len(segment_input_ids)\n",
        "        all_rationales.extend(segment_rationales)\n",
        "    # Get special token IDs\n",
        "    cls_token_id = tokenizer.cls_token_id\n",
        "    sep_token_id = tokenizer.sep_token_id\n",
        "\n",
        "    # Add special tokens at the beginning and end\n",
        "    all_input_ids = [cls_token_id] + all_input_ids + [sep_token_id]\n",
        "    all_attention_mask = [1] + all_attention_mask + [1]\n",
        "\n",
        "    # Handle token_type_ids if the model requires it\n",
        "    if hasattr(tokenizer, \"create_token_type_ids_from_sequences\"):\n",
        "        all_token_type_ids = tokenizer.create_token_type_ids_from_sequences(\n",
        "            all_input_ids[1:-1]\n",
        "        )\n",
        "    elif all_token_type_ids:\n",
        "        all_token_type_ids = [0] + all_token_type_ids + [0]\n",
        "    else:\n",
        "        all_token_type_ids = [0] * len(all_input_ids)\n",
        "\n",
        "    # Check tokenized vs rationales length\n",
        "    if len(all_input_ids) != len(all_attention_mask):\n",
        "        print(\"Warning: length of tokens and rationales do not match\")\n",
        "\n",
        "    # Add zero rationale values for special tokens\n",
        "    all_rationales = [0] + all_rationales + [0]\n",
        "\n",
        "    # Truncate to max length if needed\n",
        "    if len(all_input_ids) > max_length:\n",
        "        print(\"WARNING: NEED TO TRUNCATE\")\n",
        "        all_input_ids = all_input_ids[:max_length]\n",
        "        all_attention_mask = all_attention_mask[:max_length]\n",
        "        all_token_type_ids = all_token_type_ids[:max_length]\n",
        "        all_rationales = all_rationales[:max_length]\n",
        "\n",
        "    # Pad to max_length if needed\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "    padding_length = max_length - len(all_input_ids)\n",
        "\n",
        "    if padding_length > 0:\n",
        "        all_input_ids = all_input_ids + [pad_token_id] * padding_length\n",
        "        all_attention_mask = all_attention_mask + [0] * padding_length\n",
        "        all_token_type_ids = all_token_type_ids + [0] * padding_length\n",
        "        all_rationales = all_rationales + [0] * padding_length\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    inputs = {\n",
        "        \"input_ids\": torch.tensor([all_input_ids], dtype=torch.long),\n",
        "        \"attention_mask\": torch.tensor([all_attention_mask], dtype=torch.long),\n",
        "        \"token_type_ids\": (\n",
        "            torch.tensor([all_token_type_ids], dtype=torch.long)\n",
        "            if \"token_type_ids\" in tokenizer.model_input_names\n",
        "            else None\n",
        "        ),\n",
        "        \"rationales\": torch.tensor([all_rationales], dtype=torch.float32),\n",
        "    }\n",
        "\n",
        "    # Remove None values\n",
        "    inputs = {k: v for k, v in inputs.items() if v is not None}\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "pEC9l6Mr6xo0",
      "metadata": {
        "id": "pEC9l6Mr6xo0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import os\n",
        "import string\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import more_itertools as mit\n",
        "\n",
        "\n",
        "def find_ranges(iterable):\n",
        "    \"\"\"Yield range of consecutive numbers.\"\"\"\n",
        "    for group in mit.consecutive_groups(iterable):\n",
        "        group = list(group)\n",
        "        if len(group) == 1:\n",
        "            yield group[0]\n",
        "        else:\n",
        "            yield group[0], group[-1]\n",
        "\n",
        "\n",
        "def process_and_convert_data(\n",
        "    data,\n",
        "    tokenizer,\n",
        "    post_id_divisions,\n",
        "    save_path=\"Data/explanations/\",\n",
        "    drop_abnormal=False,\n",
        "):\n",
        "    \"\"\"\n",
        "    Combined function that processes raw entries and converts to ERASER format in one pass.\n",
        "    Also splits data into train/val/test sets.\n",
        "    \"\"\"\n",
        "    print(\"Processing and converting data...\")\n",
        "\n",
        "    # Initialize outputs\n",
        "    train_data = []\n",
        "    val_data = []\n",
        "    test_data = []\n",
        "    dropped = 0\n",
        "\n",
        "    # Create directories if saving splits\n",
        "    if save_path:\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        os.makedirs(os.path.join(save_path, \"docs\"), exist_ok=True)\n",
        "        train_fp = open(os.path.join(save_path, \"train.jsonl\"), \"w\")\n",
        "        val_fp = open(os.path.join(save_path, \"val.jsonl\"), \"w\")\n",
        "        test_fp = open(os.path.join(save_path, \"test.jsonl\"), \"w\")\n",
        "\n",
        "    for key, value in tqdm(data.items()):\n",
        "        try:\n",
        "            # Extract labels\n",
        "            labels = [\n",
        "                1 if annot[\"label\"] in [\"hatespeech\", \"offensive\"] else 0\n",
        "                for annot in value[\"annotators\"]\n",
        "            ]\n",
        "\n",
        "            # Process rationales\n",
        "            rationales = value.get(\"rationales\", [])\n",
        "            aggregated_rationale = aggregate_rationales(\n",
        "                rationales,\n",
        "                labels,\n",
        "                len(value[\"post_tokens\"]),\n",
        "                drop_abnormal=drop_abnormal,\n",
        "            )\n",
        "\n",
        "            if aggregated_rationale is None:\n",
        "                dropped += 1\n",
        "                continue\n",
        "\n",
        "            inputs = align_rationales(\n",
        "                value[\"post_tokens\"], aggregated_rationale, tokenizer\n",
        "            )\n",
        "\n",
        "            # Calculate labels\n",
        "            hard_label = Counter(labels).most_common(1)[0][0]\n",
        "            soft_label = sum(labels) / len(labels)\n",
        "\n",
        "            # Determine target groups (mentioned at least 3 times)\n",
        "            target_groups = [\n",
        "                t for annot in value[\"annotators\"] for t in annot[\"target\"]\n",
        "            ]\n",
        "            filtered_targets = [k for k, v in Counter(target_groups).items() if v > 2]\n",
        "\n",
        "            # Create processed entry\n",
        "            processed_entry = {\n",
        "                \"post_id\": key,\n",
        "                \"input_ids\": inputs[\"input_ids\"],\n",
        "                \"attention_mask\": inputs[\"attention_mask\"],\n",
        "                \"rationales\": inputs[\"rationales\"],\n",
        "                \"raw_text\": \" \".join(value[\"post_tokens\"]),\n",
        "                \"hard_label\": hard_label,\n",
        "                \"soft_label\": soft_label,\n",
        "                \"target_groups\": filtered_targets,\n",
        "            }\n",
        "\n",
        "            # Convert to ERASER format if it's hateful/offensive content\n",
        "            if hard_label == 1 and save_path:\n",
        "                input_ids_list = inputs[\"input_ids\"].squeeze().tolist()\n",
        "                rationales_list = inputs[\"rationales\"].squeeze().ceil().int().tolist()\n",
        "\n",
        "                # Build evidences\n",
        "                evidences = []\n",
        "                indexes = sorted(\n",
        "                    [i for i, each in enumerate(rationales_list) if each == 1]\n",
        "                )\n",
        "                for span in find_ranges(indexes):\n",
        "                    if isinstance(span, int):\n",
        "                        start, end = span, span + 1\n",
        "                    else:\n",
        "                        start, end = span[0], span[1] + 1\n",
        "\n",
        "                    evidences.append(\n",
        "                        {\n",
        "                            \"docid\": key,\n",
        "                            \"end_sentence\": -1,\n",
        "                            \"end_token\": end,\n",
        "                            \"start_sentence\": -1,\n",
        "                            \"start_token\": start,\n",
        "                            \"text\": \" \".join(\n",
        "                                [str(x) for x in input_ids_list[start:end]]\n",
        "                            ),\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "                eraser_entry = {\n",
        "                    \"annotation_id\": key,\n",
        "                    \"classification\": str(hard_label),\n",
        "                    \"evidences\": [evidences],\n",
        "                    \"query\": \"What is the class?\",\n",
        "                    \"query_type\": None,\n",
        "                }\n",
        "\n",
        "                # Save document\n",
        "                with open(os.path.join(save_path, \"docs\", key), \"w\") as fp:\n",
        "                    fp.write(\" \".join([str(x) for x in input_ids_list if x > 0]))\n",
        "\n",
        "                # Write to appropriate split\n",
        "                if key in post_id_divisions[\"train\"]:\n",
        "                    train_fp.write(json.dumps(eraser_entry) + \"\\n\")\n",
        "                elif key in post_id_divisions[\"val\"]:\n",
        "                    val_fp.write(json.dumps(eraser_entry) + \"\\n\")\n",
        "                elif key in post_id_divisions[\"test\"]:\n",
        "                    test_fp.write(json.dumps(eraser_entry) + \"\\n\")\n",
        "\n",
        "            # Add to appropriate split list\n",
        "            if key in post_id_divisions[\"train\"]:\n",
        "                train_data.append(processed_entry)\n",
        "            elif key in post_id_divisions[\"val\"]:\n",
        "                val_data.append(processed_entry)\n",
        "            elif key in post_id_divisions[\"test\"]:\n",
        "                test_data.append(processed_entry)\n",
        "\n",
        "        except Exception as e:\n",
        "            dropped += 1\n",
        "            print(f\"Error processing {key}: {e}\")\n",
        "\n",
        "    if save_path:\n",
        "        train_fp.close()\n",
        "        val_fp.close()\n",
        "        test_fp.close()\n",
        "\n",
        "    print(\n",
        "        f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}, Dropped: {dropped}\"\n",
        "    )\n",
        "\n",
        "    return {\"train\": train_data, \"val\": val_data, \"test\": test_data}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "43NMFuwv62Q4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43NMFuwv62Q4",
        "outputId": "8a478371-0746-4c85-f2a5-031189f77780"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing and converting data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 37%|███▋      | 7406/20148 [00:09<00:16, 774.86it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: NEED TO TRUNCATE\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 89%|████████▉ | 17990/20148 [00:24<00:03, 692.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error processing 24439295_gab: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20148/20148 [00:27<00:00, 738.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 15382, Val: 1922, Test: 1924, Dropped: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "with open(data_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "with open('Data/post_id_divisions.json') as file:\n",
        "    post_id_divisions = json.load(file)\n",
        "\n",
        "# Process everything in one pass\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "splits = process_and_convert_data(\n",
        "    data=data,\n",
        "    tokenizer=tokenizer,\n",
        "    post_id_divisions=post_id_divisions,\n",
        "    save_path='Data/explanations/',\n",
        "    drop_abnormal=False\n",
        ")\n",
        "\n",
        "# Access splits directly\n",
        "train_data = splits['train']\n",
        "val_data = splits['val']\n",
        "test_data = splits['test']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "HJjDXt3QGbrQ",
      "metadata": {
        "id": "HJjDXt3QGbrQ"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "87c63ecf",
      "metadata": {
        "id": "87c63ecf"
      },
      "outputs": [],
      "source": [
        "from HateDataset import HateDataset\n",
        "\n",
        "# Create datasets with pre-tokenized data\n",
        "train_dataset = HateDataset(data=train_data)\n",
        "val_dataset = HateDataset(data=val_data)\n",
        "test_dataset = HateDataset(data=test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f4887f43",
      "metadata": {
        "id": "f4887f43"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False) # Use shuffle=False for validation\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False) # Use shuffle=False for testing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0caf972c",
      "metadata": {
        "id": "0caf972c"
      },
      "source": [
        "## Bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "40280671",
      "metadata": {
        "id": "40280671"
      },
      "outputs": [],
      "source": [
        "def get_bias_evaluation_samples(data, method, group):\n",
        "    \"\"\"\n",
        "    Get positive and negative sample IDs for bias evaluation based on method and group\n",
        "\n",
        "    Args:\n",
        "        data: list of data entries\n",
        "        method: Bias evaluation method ('subgroup', 'bpsn', or 'bnsp')\n",
        "        group: Target group to evaluate\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (positive_ids, negative_ids)\n",
        "    \"\"\"\n",
        "    positive_ids = []\n",
        "    negative_ids = []\n",
        "\n",
        "    for idx, row in enumerate(data):\n",
        "        target_groups = row['target_groups']\n",
        "        if target_groups is None:\n",
        "            continue\n",
        "\n",
        "        is_in_group = group in target_groups\n",
        "\n",
        "        # Convert various label formats to binary toxic/non-toxic\n",
        "        if 'hard_label' in row:\n",
        "            is_toxic = row['hard_label'] == 1\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        if method == 'subgroup':\n",
        "            # Only consider samples mentioning the group\n",
        "            if is_in_group:\n",
        "                if is_toxic:\n",
        "                    positive_ids.append(idx)\n",
        "                else:\n",
        "                    negative_ids.append(idx)\n",
        "\n",
        "        elif method == 'bpsn':\n",
        "            # Compare non-toxic posts mentioning the group with toxic posts NOT mentioning the group\n",
        "            if is_in_group and not is_toxic:\n",
        "                negative_ids.append(idx)\n",
        "            elif not is_in_group and is_toxic:\n",
        "                positive_ids.append(idx)\n",
        "\n",
        "        elif method == 'bnsp':\n",
        "            # Compare toxic posts mentioning the group with non-toxic posts NOT mentioning the group\n",
        "            if is_in_group and is_toxic:\n",
        "                positive_ids.append(idx)\n",
        "            elif not is_in_group and not is_toxic:\n",
        "                negative_ids.append(idx)\n",
        "\n",
        "    return positive_ids, negative_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "79edfa7d",
      "metadata": {
        "id": "79edfa7d"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def calculate_gmb_metrics(\n",
        "    test_data: List[Dict[str, Any]],\n",
        "    probabilities: np.ndarray,\n",
        "    target_groups: List[str]\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculate GMB (Generalized Mean of Bias) AUC metrics from model predictions\n",
        "\n",
        "    Args:\n",
        "        probabilities: Model's probability outputs\n",
        "        test_data: List of test data entries\n",
        "        target_groups: List of target groups to evaluate\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with GMB metrics\n",
        "    \"\"\"\n",
        "    # Create mappings from post_id to predictions and ground truth\n",
        "    prediction_scores = defaultdict(lambda: defaultdict(dict))\n",
        "    ground_truth = {}\n",
        "\n",
        "    for idx, row in enumerate(test_data):\n",
        "        prediction_scores[idx] = probabilities[idx, 1]\n",
        "        ground_truth[idx] = row['hard_label']\n",
        "\n",
        "    # Calculate metrics for each target group and method\n",
        "    bias_metrics = {}\n",
        "    methods = ['subgroup', 'bpsn', 'bnsp']\n",
        "\n",
        "    for method in methods:\n",
        "        bias_metrics[method] = {}  # Initialize nested dictionary for each method\n",
        "        for group in target_groups:\n",
        "            # Get positive and negative samples based on the method\n",
        "            positive_ids, negative_ids = get_bias_evaluation_samples(test_data, method, group)\n",
        "\n",
        "            if len(positive_ids) == 0 or len(negative_ids) == 0:\n",
        "                print(f\"Skipping {method} for group {group}: no samples found\")\n",
        "                continue  # Skip if no samples for this group/method\n",
        "\n",
        "            # Collect ground truth and predictions\n",
        "            y_true = []\n",
        "            y_score = []\n",
        "\n",
        "            for post_id in positive_ids:\n",
        "                if post_id in ground_truth and post_id in prediction_scores:\n",
        "                    y_true.append(ground_truth[post_id])\n",
        "                    y_score.append(prediction_scores[post_id])\n",
        "\n",
        "            for post_id in negative_ids:\n",
        "                if post_id in ground_truth and post_id in prediction_scores:\n",
        "                    y_true.append(ground_truth[post_id])\n",
        "                    y_score.append(prediction_scores[post_id])\n",
        "\n",
        "            # Calculate AUC if we have enough samples with both classes\n",
        "            if len(y_true) > 10 and len(set(y_true)) > 1:\n",
        "                try:\n",
        "                    auc = roc_auc_score(y_true, y_score)\n",
        "                    bias_metrics[method][group] = auc\n",
        "                except ValueError:\n",
        "                    print(f\"Could not compute AUC for {method} and group {group} due to ValueError\")\n",
        "                    pass\n",
        "\n",
        "    # Calculate GMB for each method\n",
        "    gmb_metrics = {}\n",
        "    power = -5  # Power parameter for generalized mean\n",
        "\n",
        "    for method in methods:\n",
        "        if not bias_metrics[method]:\n",
        "            continue\n",
        "\n",
        "        scores = list(bias_metrics[method].values())\n",
        "        if not scores:\n",
        "            continue\n",
        "\n",
        "        # Calculate generalized mean with p=-5\n",
        "        power_mean = np.mean([score ** power for score in scores]) ** (1/power)\n",
        "        gmb_metrics[f'GMB-{method.upper()}-AUC'] = power_mean\n",
        "\n",
        "    # Calculate a combined GMB score that includes all methods\n",
        "    all_scores = []\n",
        "    for method in methods:\n",
        "        all_scores.extend(list(bias_metrics[method].values()))\n",
        "\n",
        "    if all_scores:\n",
        "        gmb_metrics['GMB-COMBINED-AUC'] = np.mean([score ** power for score in all_scores]) ** (1/power)\n",
        "\n",
        "    return gmb_metrics, bias_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39b1ac3f",
      "metadata": {
        "id": "39b1ac3f"
      },
      "source": [
        "## XAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8toApSDiIVlT",
      "metadata": {
        "id": "8toApSDiIVlT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import List, Dict, Tuple\n",
        "import json\n",
        "from sklearn.metrics import (\n",
        "    precision_recall_curve,\n",
        "    auc,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "\n",
        "\n",
        "class FaithfulnessMetrics:\n",
        "    \"\"\"\n",
        "    Compute faithfulness metrics using the model's existing predict() method.\n",
        "    Creates modified datasets and uses DataLoader for efficient batched processing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer, dataset_class, batch_size=32):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.dataset_class = dataset_class\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Get special token IDs\n",
        "        self.special_token_ids = {\n",
        "            tokenizer.cls_token_id,\n",
        "            tokenizer.sep_token_id,\n",
        "        }\n",
        "        self.special_token_ids = {x for x in self.special_token_ids if x is not None}\n",
        "\n",
        "    def compute_all_metrics(\n",
        "        self,\n",
        "        test_data: List[Dict],  # Your original test data\n",
        "        test_results: Dict,  # Results from prediction\n",
        "        k: int = 5,  # Number of top tokens to consider\n",
        "        eraser_save_path: str = \"Data/eraser_formatted_results.jsonl\",\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Compute all ERASER metrics efficiently using DataLoader approach\n",
        "\n",
        "        Args:\n",
        "            test_data: List of test instances (each with input_ids, attention_mask, rationales, labels)\n",
        "            test_results: List of dictionaries containing attention scores for each instance\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with all metrics\n",
        "        \"\"\"\n",
        "        print(\"Computing ERASER metrics using DataLoader approach...\")\n",
        "\n",
        "        # Extract lists for easier processing\n",
        "        input_ids_list = [item[\"input_ids\"] for item in test_data]\n",
        "        attention_masks_list = [item[\"attention_mask\"] for item in test_data]\n",
        "        human_rationales = [item[\"rationales\"] for item in test_data]\n",
        "        attention_scores = [item for item in test_results[\"attentions\"]]\n",
        "\n",
        "        # 1. Extract top-k as hard predictions\n",
        "        hard_predictions = self._extract_top_k_tokens(\n",
        "            attention_scores, attention_masks_list, input_ids_list, k\n",
        "        )\n",
        "\n",
        "        hard_rationale_predictions, soft_rationale_predictions = self._convert_attention_to_evidence_format(input_ids_list, attention_scores, hard_predictions)\n",
        "\n",
        "        # 2. PLAUSIBILITY METRICS\n",
        "        print(\"\\n[1/3] Computing plausibility metrics...\")\n",
        "        auprc = self._compute_auprc(\n",
        "            attention_scores, human_rationales, attention_masks_list, input_ids_list\n",
        "        )\n",
        "        token_f1, token_prec, token_rec = self._compute_token_f1(\n",
        "            hard_predictions, human_rationales, attention_masks_list\n",
        "        )\n",
        "\n",
        "        # 3. FAITHFULNESS METRICS\n",
        "        print(\"[2/3] Computing comprehensiveness scores...\")\n",
        "        raw_comprehensiveness, comprehensiveness_scores = (\n",
        "            self._compute_comprehensiveness(test_data, test_results, hard_predictions)\n",
        "        )\n",
        "\n",
        "        print(\"[3/3] Computing sufficiency scores...\")\n",
        "        raw_sufficiency, sufficiency_scores = self._compute_sufficiency(\n",
        "            test_data, test_results, hard_predictions\n",
        "        )\n",
        "\n",
        "        # 4. Convert to eraser format\n",
        "        results_eraser = self._convert_result_to_eraser_format(test_results, hard_rationale_predictions, soft_rationale_predictions, raw_sufficiency, raw_comprehensiveness)\n",
        "        # Convert to JSONL format\n",
        "        jsonl_output = '\\n'.join([json.dumps(entry) for entry in results_eraser])\n",
        "        with open(eraser_save_path, 'w') as f:\n",
        "            f.write(jsonl_output)\n",
        "\n",
        "        return {\n",
        "            # Plausibility\n",
        "            \"auprc\": auprc,\n",
        "            \"token_f1\": token_f1,\n",
        "            \"token_precision\": token_prec,\n",
        "            \"token_recall\": token_rec,\n",
        "            # Faithfulness\n",
        "            \"comprehensiveness\": float(np.mean(comprehensiveness_scores)),\n",
        "            \"sufficiency\": float(np.mean(sufficiency_scores)),\n",
        "            # Additional\n",
        "            \"avg_rationale_length\": k,\n",
        "        }\n",
        "\n",
        "    def _convert_attention_to_evidence_format(self, input_ids_list, attention_scores, hard_predictions):\n",
        "        # 2. Collect evidence\n",
        "        hard_rationale_predictions = []\n",
        "        for idx, hp in enumerate(hard_predictions):\n",
        "            evidences = []\n",
        "            indexes = sorted([i for i, each in enumerate(hp.tolist()) if each == 1])\n",
        "            for span in find_ranges(indexes):\n",
        "                if isinstance(span, int):\n",
        "                    start, end = span, span + 1\n",
        "                else:\n",
        "                    start, end = span[0], span[1] + 1\n",
        "\n",
        "                evidences.append({\n",
        "                    \"start_token\": start,\n",
        "                    \"end_token\": end,\n",
        "                })\n",
        "            hard_rationale_predictions.append(evidences)\n",
        "\n",
        "        soft_rationale_predictions = []\n",
        "        for att in attention_scores:\n",
        "            pred = [x for x in att if x > 0]\n",
        "            soft_rationale_predictions.append(pred)\n",
        "\n",
        "        return hard_rationale_predictions, soft_rationale_predictions\n",
        "\n",
        "    def _convert_result_to_eraser_format(\n",
        "        self,\n",
        "        test_result: Dict,\n",
        "        hard_rationale_predictions,\n",
        "        soft_rationale_predictions,\n",
        "        sufficiency_scores: np.ndarray,\n",
        "        comprehensiveness_scores: np.ndarray,\n",
        "    ):\n",
        "        all_entries = []\n",
        "        for idx, data in enumerate(test_result[\"post_id\"]):\n",
        "            entry = {\n",
        "            'annotation_id': data,\n",
        "            'classification': str(int(test_result[\"predictions\"][idx])),\n",
        "            'classification_scores': {\n",
        "                0: float(test_result[\"probabilities\"][idx][0]),\n",
        "                1: float(test_result[\"probabilities\"][idx][1]),\n",
        "            },\n",
        "            'rationales': [\n",
        "                {\n",
        "                    \"docid\": data,\n",
        "                    \"hard_rationale_predictions\": hard_rationale_predictions[idx],\n",
        "                    \"soft_rationale_predictions\": [float(x) for x in soft_rationale_predictions[idx]],\n",
        "                }\n",
        "            ],\n",
        "            'sufficiency_classification_scores': {\n",
        "                0: float(sufficiency_scores[idx][0]),\n",
        "                1: float(sufficiency_scores[idx][1])\n",
        "            },\n",
        "            'comprehensiveness_classification_scores': {\n",
        "                0: float(comprehensiveness_scores[idx][0]),\n",
        "                1: float(comprehensiveness_scores[idx][1])\n",
        "            }\n",
        "            }\n",
        "            all_entries.append(entry)\n",
        "\n",
        "        return all_entries\n",
        "\n",
        "    def _calculate_average_rationale_length(\n",
        "        self,\n",
        "        human_rationales: List[torch.Tensor],\n",
        "        attention_masks_list: List[torch.Tensor],\n",
        "        input_ids_list: List[torch.Tensor],\n",
        "    ) -> int:\n",
        "        \"\"\"Calculate average number of content rationale tokens\"\"\"\n",
        "        lengths = []\n",
        "        for idx, (rat, mask) in enumerate(zip(human_rationales, attention_masks_list)):\n",
        "            valid_positions = mask.bool().cpu().numpy().flatten()\n",
        "\n",
        "            # Exclude special tokens\n",
        "            input_ids = input_ids_list[idx].cpu().numpy().flatten()\n",
        "            is_special = np.isin(input_ids, list(self.special_token_ids))\n",
        "            content_positions = valid_positions & ~is_special\n",
        "\n",
        "            rat_count = (rat.cpu().numpy().flatten()[content_positions] == 1).sum()\n",
        "            lengths.append(rat_count)\n",
        "\n",
        "        return max(1, int(np.mean(lengths)))\n",
        "\n",
        "    def _extract_top_k_tokens(\n",
        "        self,\n",
        "        attention_scores: List[np.ndarray],\n",
        "        attention_masks_list: List[torch.Tensor],\n",
        "        input_ids_list: List[torch.Tensor],\n",
        "        k: int,\n",
        "    ) -> List[np.ndarray]:\n",
        "        \"\"\"Extract top-k content tokens as hard predictions\"\"\"\n",
        "        hard_predictions = []\n",
        "\n",
        "        for idx, (attn, mask) in enumerate(zip(attention_scores, attention_masks_list)):\n",
        "            pred_mask = np.zeros_like(attn, dtype=int)\n",
        "            valid_positions = mask.bool().cpu().numpy().flatten()\n",
        "\n",
        "            # Exclude special tokens\n",
        "            input_ids = input_ids_list[idx].cpu().numpy().flatten()\n",
        "            is_special = np.isin(input_ids, list(self.special_token_ids))\n",
        "            content_positions = valid_positions & ~is_special\n",
        "\n",
        "            content_attn = attn[content_positions]\n",
        "\n",
        "            if k > 0 and len(content_attn) > 0:\n",
        "                k_actual = min(k, len(content_attn))\n",
        "                top_k_within_content = np.argsort(content_attn)[-k_actual:]\n",
        "                content_indices = np.where(content_positions)[0]\n",
        "                top_k_indices = content_indices[top_k_within_content]\n",
        "                pred_mask[top_k_indices] = 1\n",
        "\n",
        "            hard_predictions.append(pred_mask)\n",
        "\n",
        "        return hard_predictions\n",
        "\n",
        "    def _compute_auprc(\n",
        "        self,\n",
        "        attention_scores: List[np.ndarray],\n",
        "        human_rationales: List[torch.Tensor],\n",
        "        attention_masks_list: List[torch.Tensor],\n",
        "        input_ids_list: List[torch.Tensor],\n",
        "    ) -> float:\n",
        "        \"\"\"Compute AUPRC for soft attention scores\"\"\"\n",
        "        all_scores = []\n",
        "        all_labels = []\n",
        "\n",
        "        for idx, (attn, rat, mask) in enumerate(\n",
        "            zip(attention_scores, human_rationales, attention_masks_list)\n",
        "        ):\n",
        "            valid_positions = mask.bool().cpu().numpy().flatten()\n",
        "\n",
        "            # Exclude special tokens\n",
        "            input_ids = input_ids_list[idx].cpu().numpy().flatten()\n",
        "            is_special = np.isin(input_ids, list(self.special_token_ids))\n",
        "            content_positions = valid_positions & ~is_special\n",
        "\n",
        "            all_scores.extend(attn[content_positions].tolist())\n",
        "            all_labels.extend(\n",
        "                rat.cpu().numpy().flatten()[content_positions].astype(int).tolist()\n",
        "            )\n",
        "\n",
        "        all_scores = np.array(all_scores, dtype=float)\n",
        "        all_labels = np.array(all_labels, dtype=int)\n",
        "\n",
        "        if len(np.unique(all_labels)) < 2:\n",
        "            print(f\"Warning: Only one class in labels: {np.unique(all_labels)}\")\n",
        "            return 0.0\n",
        "\n",
        "        precision, recall, _ = precision_recall_curve(all_labels, all_scores)\n",
        "        return auc(recall, precision)\n",
        "\n",
        "    def _compute_token_f1(\n",
        "        self,\n",
        "        hard_predictions: List[np.ndarray],\n",
        "        human_rationales: List[torch.Tensor],\n",
        "        attention_masks_list: List[torch.Tensor],\n",
        "    ) -> Tuple[float, float, float]:\n",
        "        \"\"\"Compute token-level F1, Precision, Recall\"\"\"\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        for pred, rat, mask in zip(\n",
        "            hard_predictions, human_rationales, attention_masks_list\n",
        "        ):\n",
        "            valid_positions = mask.bool().cpu().numpy().flatten()\n",
        "            all_preds.extend(pred[valid_positions].astype(int).tolist())\n",
        "            all_labels.extend(\n",
        "                rat.cpu().numpy().flatten()[valid_positions].astype(int).tolist()\n",
        "            )\n",
        "\n",
        "        all_preds = np.array(all_preds, dtype=int)\n",
        "        all_labels = np.array(all_labels, dtype=int)\n",
        "\n",
        "        f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
        "        precision = precision_score(all_labels, all_preds, zero_division=0)\n",
        "        recall = recall_score(all_labels, all_preds, zero_division=0)\n",
        "\n",
        "        return f1, precision, recall\n",
        "\n",
        "    def _compute_comprehensiveness(\n",
        "        self,\n",
        "        test_data: List[Dict],\n",
        "        test_results: Dict,\n",
        "        hard_predictions: List[np.ndarray],\n",
        "    ) -> Tuple[float, List[float]]:\n",
        "        \"\"\"\n",
        "        Compute comprehensiveness: how much does REMOVING rationales hurt?\n",
        "        Uses DataLoader approach for efficiency\n",
        "        \"\"\"\n",
        "        # Create modified dataset (remove rationales from attention mask)\n",
        "        modified_data = []\n",
        "        for item, rationale_mask in zip(test_data, hard_predictions):\n",
        "            modified_item = self._create_comprehensiveness_instance(\n",
        "                item, rationale_mask\n",
        "            )\n",
        "            modified_data.append(modified_item)\n",
        "\n",
        "        # Create DataLoader\n",
        "        modified_dataset = self.dataset_class(modified_data)\n",
        "        modified_loader = DataLoader(\n",
        "            modified_dataset, batch_size=self.batch_size, shuffle=False\n",
        "        )\n",
        "\n",
        "        # Get predictions using model's predict method\n",
        "        results = self.model.predict(modified_loader, return_attentions=False)\n",
        "        modified_probs = results[\"probabilities\"]\n",
        "\n",
        "        # Calculate comprehensiveness scores\n",
        "        comprehensiveness_scores = []\n",
        "        for idx, (prob, label) in enumerate(zip(test_results[\"probabilities\"], test_results['labels'])):\n",
        "            original_prob = prob[\n",
        "                label\n",
        "            ]  # Probability from normal prediction process for the label\n",
        "            modified_prob = modified_probs[idx][label]\n",
        "\n",
        "            # Comprehensiveness = original - modified (higher is better)\n",
        "            comp_score = original_prob - modified_prob\n",
        "            comprehensiveness_scores.append(comp_score)\n",
        "\n",
        "        return modified_probs, comprehensiveness_scores\n",
        "\n",
        "    def _compute_sufficiency(\n",
        "        self,\n",
        "        test_data: List[Dict],\n",
        "        test_results: Dict,\n",
        "        hard_predictions: List[np.ndarray],\n",
        "    ) -> Tuple[List[float], List[float]]:\n",
        "        \"\"\"\n",
        "        Compute sufficiency: how well do ONLY rationales predict?\n",
        "        Uses DataLoader approach for efficiency\n",
        "        \"\"\"\n",
        "        # Create modified dataset (keep only rationales in attention mask)\n",
        "        modified_data = []\n",
        "        for item, rationale_mask in zip(test_data, hard_predictions):\n",
        "            modified_item = self._create_sufficiency_instance(item, rationale_mask)\n",
        "            modified_data.append(modified_item)\n",
        "\n",
        "        # Create DataLoader\n",
        "        modified_dataset = self.dataset_class(modified_data)\n",
        "        modified_loader = DataLoader(\n",
        "            modified_dataset, batch_size=self.batch_size, shuffle=False\n",
        "        )\n",
        "\n",
        "        # Get predictions using model's predict method\n",
        "        results = self.model.predict(modified_loader, return_attentions=False)\n",
        "        modified_probs = results[\"probabilities\"]\n",
        "\n",
        "        # Calculate sufficiency scores\n",
        "        sufficiency_scores = []\n",
        "        for idx, (prob, label) in enumerate(zip(test_results[\"probabilities\"], test_results['labels'])):\n",
        "            original_prob = prob[\n",
        "                label\n",
        "            ]  # Probability from normal prediction process for the label\n",
        "            modified_prob = modified_probs[idx][label]\n",
        "\n",
        "            # Sufficiency = original - modified (lower/negative is better)\n",
        "            suff_score = original_prob - modified_prob\n",
        "            sufficiency_scores.append(suff_score)\n",
        "\n",
        "        return modified_probs, sufficiency_scores\n",
        "\n",
        "    def _create_comprehensiveness_instance(\n",
        "        self, item: Dict, rationale_mask: np.ndarray\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Create instance for comprehensiveness: REMOVE rationales from attention\n",
        "        Keep: CLS + non-rationale content tokens + SEP\n",
        "        \"\"\"\n",
        "        input_ids = item[\"input_ids\"].cpu().numpy().flatten()\n",
        "        orig_mask = item[\"attention_mask\"].cpu().numpy().flatten()\n",
        "\n",
        "        # Start with original mask\n",
        "        new_mask = orig_mask.copy()\n",
        "\n",
        "        # Zero out rationale positions (except CLS and SEP)\n",
        "        for i in range(len(new_mask)):\n",
        "            if rationale_mask[i] == 1:  # This is a rationale\n",
        "                # Don't mask if it's CLS or SEP\n",
        "                if input_ids[i] not in self.special_token_ids:\n",
        "                    new_mask[i] = 0\n",
        "\n",
        "        return {\n",
        "            \"post_id\": item[\"post_id\"],\n",
        "            \"input_ids\": torch.tensor(input_ids).unsqueeze(0),\n",
        "            \"attention_mask\": torch.tensor(new_mask).unsqueeze(0),\n",
        "            \"rationales\": item[\"rationales\"],\n",
        "            \"hard_label\": item[\"hard_label\"],\n",
        "        }\n",
        "\n",
        "    def _create_sufficiency_instance(\n",
        "        self, item: Dict, rationale_mask: np.ndarray\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Create instance for sufficiency: Keep ONLY rationales in attention\n",
        "        Keep: CLS + rationale tokens + SEP\n",
        "        \"\"\"\n",
        "        input_ids = item[\"input_ids\"].cpu().numpy().flatten()\n",
        "        orig_mask = item[\"attention_mask\"].cpu().numpy().flatten()\n",
        "\n",
        "        # Start with zeros\n",
        "        new_mask = np.zeros_like(orig_mask)\n",
        "\n",
        "        # Always keep CLS and SEP\n",
        "        for i in range(len(new_mask)):\n",
        "            if input_ids[i] in self.special_token_ids:\n",
        "                new_mask[i] = 1\n",
        "\n",
        "        # Keep rationale positions\n",
        "        for i in range(len(new_mask)):\n",
        "            if rationale_mask[i] == 1 and orig_mask[i] == 1:\n",
        "                new_mask[i] = 1\n",
        "\n",
        "        return {\n",
        "            \"post_id\": item[\"post_id\"],\n",
        "            \"input_ids\": torch.tensor(input_ids).unsqueeze(0),\n",
        "            \"attention_mask\": torch.tensor(new_mask).unsqueeze(0),\n",
        "            \"rationales\": item[\"rationales\"],\n",
        "            \"hard_label\": item[\"hard_label\"],\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5af905f2",
      "metadata": {
        "id": "5af905f2"
      },
      "source": [
        "# Experiment Management System\n",
        "\n",
        "This notebook now uses a systematic experiment tracking system that organizes all outputs by experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "4c5b1ffe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c5b1ffe",
        "outputId": "3d9b22ab-4c18-4fbc-ccba-1e04d266d752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "Created new experiment: 20251218_212137_baseline_distilbert_455539da\n",
            "Directory: experiments\\20251218_212137_baseline_distilbert_455539da\n",
            "Description: Baseline model with distilbert-base-uncased, standard hyperparameters\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# FULL EXPERIMENT PIPELINE WITH TRACKING\n",
        "# ============================================================================\n",
        "from ExperimentManager import ExperimentManager\n",
        "from HateClassifier import HateClassifier\n",
        "from HateDataset import HateDataset\n",
        "# 1. CREATE EXPERIMENT\n",
        "experiment_manager = ExperimentManager(base_dir=\"./experiments\")\n",
        "experiment_dir = experiment_manager.create_experiment(\n",
        "    config=config,\n",
        "    custom_name=\"baseline_distilbert\",  # Change this for each experiment\n",
        "    description=\"Baseline model with distilbert-base-uncased, standard hyperparameters\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "lZ6YXEta0_pi",
      "metadata": {
        "id": "lZ6YXEta0_pi"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "y = [int(td['hard_label']) for td in train_data]\n",
        "\n",
        "class_weights = torch.tensor(compute_class_weight(\n",
        "    class_weight='balanced',\n",
        "    classes=np.array([0, 1]),  # Ensure consistent order\n",
        "    y=y\n",
        "), dtype=torch.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "2Ir9hUuDJ9nR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "id": "2Ir9hUuDJ9nR",
        "outputId": "9f2820d0-a569-4b1f-8779-81ef9aeaa3f4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using class weighting for loss function.\n",
            "Training on device: cuda\n",
            "Model: distilbert-base-uncased\n",
            "Epochs: 2\n",
            "Batch size: 32\n",
            "Gradient accumulation steps: 1\n",
            "Effective batch size: 32\n",
            "Learning rate: 1e-05\n",
            "Mixed precision (AMP): True\n",
            "Gradient clipping: 1.0\n",
            "\n",
            "Loss Configuration:\n",
            "  Multi-layer loss: True\n",
            "    - Auxiliary (layer 3): α=0.5\n",
            "    - Main (final layer): β=0.5\n",
            "  Attention supervision: True\n",
            "    - Ranking loss: λ=1\n",
            "    - Margin: 0.1, Threshold: 0.05\n",
            "============================================================\n",
            "\n",
            "Epoch 1/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training:   4%|▍         | 21/481 [00:05<02:02,  3.76batch/s, total=0.826, main=0.694, aux=0.713, attn=0.122]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 2. TRAIN MODEL (config.save_dir is automatically updated)\u001b[39;00m\n\u001b[32m      2\u001b[39m model = HateClassifier(config, class_weight=class_weights)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m history = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m=\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# Save training history\u001b[39;00m\n\u001b[32m      6\u001b[39m experiment_manager.save_training_history(history)\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\Campus\\HateXplain - DeRC\\HateDeRC\\HateClassifier.py:406\u001b[39m, in \u001b[36mHateClassifier.train\u001b[39m\u001b[34m(self, train_dataloader, val_dataloader)\u001b[39m\n\u001b[32m    403\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.config.num_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    405\u001b[39m \u001b[38;5;66;03m# Train for one epoch\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m406\u001b[39m train_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    408\u001b[39m \u001b[38;5;66;03m# Evaluate on validation set\u001b[39;00m\n\u001b[32m    409\u001b[39m val_loss, val_accuracy, val_f1 = \u001b[38;5;28mself\u001b[39m.evaluate(val_dataloader)\n",
            "\u001b[36mFile \u001b[39m\u001b[32me:\\Campus\\HateXplain - DeRC\\HateDeRC\\HateClassifier.py:221\u001b[39m, in \u001b[36mHateClassifier.train_epoch\u001b[39m\u001b[34m(self, train_dataloader)\u001b[39m\n\u001b[32m    219\u001b[39m         logits = \u001b[38;5;28mself\u001b[39m.classifier_list[i](combined)\n\u001b[32m    220\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m         logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclassifier_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpooled_output\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m     logits_list.append(logits)\n\u001b[32m    224\u001b[39m \u001b[38;5;66;03m# Calculate unified loss with all components\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\miniconda3\\envs\\transformersv2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\miniconda3\\envs\\transformersv2\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\USER\\miniconda3\\envs\\transformersv2\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[39m, in \u001b[36mLinear.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# 2. TRAIN MODEL (config.save_dir is automatically updated)\n",
        "model = HateClassifier(config, class_weight=class_weights)\n",
        "history = model.train(train_dataloader=train_loader, val_dataloader=val_loader)\n",
        "\n",
        "# Save training history\n",
        "experiment_manager.save_training_history(history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hEi5QafbKAtl",
      "metadata": {
        "id": "hEi5QafbKAtl"
      },
      "outputs": [],
      "source": [
        "# 3. EVALUATE MODEL\n",
        "# Load best model\n",
        "model.load_model('best_model')\n",
        "\n",
        "# Evaluate model\n",
        "result = model.predict(test_dataloader=test_loader, return_attentions=True)\n",
        "\n",
        "# Save predictions\n",
        "experiment_manager.save_predictions(result, filename=\"test_predictions.pkl\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oXBmyGzAKDYw",
      "metadata": {
        "id": "oXBmyGzAKDYw"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "from itertools import chain\n",
        "\n",
        "all_target_groups = chain.from_iterable([group['target_groups'] for group in train_data])\n",
        "all_target_groups = [group for group in all_target_groups if group != 'None' and group != 'Other']\n",
        "counter = Counter(all_target_groups)\n",
        "\n",
        "n_common = 10\n",
        "bias_target_groups = [tg[0] for tg in counter.most_common(n_common)]\n",
        "\n",
        "# 4. BIAS EVALUATION\n",
        "gmb_metrics, bias_details = calculate_gmb_metrics(\n",
        "    test_data=test_data,\n",
        "    probabilities=result['probabilities'],\n",
        "    target_groups=bias_target_groups\n",
        ")\n",
        "\n",
        "# Save bias metrics\n",
        "experiment_manager.save_bias_metrics(gmb_metrics, bias_details)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VbR3vJyBKFx2",
      "metadata": {
        "id": "VbR3vJyBKFx2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 5. XAI EVALUATION (only on hate samples)\n",
        "test_data_hate_only = []\n",
        "test_results_hate_only = {'attentions': [], 'probabilities': [], 'predictions': [], 'post_id': [], 'labels': []}\n",
        "for idx, td in enumerate(test_data):\n",
        "    if td['hard_label'] == 1:\n",
        "        test_data_hate_only.append(td)\n",
        "        test_results_hate_only['attentions'].append(result['attentions'][idx])\n",
        "        test_results_hate_only['probabilities'].append(result['probabilities'][idx])\n",
        "        test_results_hate_only['predictions'].append(result['predictions'][idx])\n",
        "        test_results_hate_only['post_id'].append(result['post_ids'][idx])\n",
        "        test_results_hate_only['labels'].append(result['labels'][idx])\n",
        "\n",
        "calculator = FaithfulnessMetrics(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset_class=HateDataset,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "k = 5\n",
        "eraser_save_path = f\"{experiment_dir}/results/test_explain_output.jsonl\"\n",
        "xai_results = calculator.compute_all_metrics(test_data_hate_only, test_results_hate_only, k, eraser_save_path)\n",
        "\n",
        "# Save XAI metrics\n",
        "experiment_manager.save_xai_metrics(xai_results)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "m4B2sa8GQLIZ",
      "metadata": {
        "id": "m4B2sa8GQLIZ"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/jayded/eraserbenchmark.git\n",
        "!sed -i \"285s/.*/    labels=['0', '1']/\" eraserbenchmark/rationale_benchmark/metrics.py\n",
        "!sed -i \"286s/.*/    label_to_int = {'0':0, '1': 1}/\" eraserbenchmark/rationale_benchmark/metrics.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gAa1nG10MRA9",
      "metadata": {
        "id": "gAa1nG10MRA9"
      },
      "outputs": [],
      "source": [
        "score_file = f\"{experiment_dir}/results/eraser_result.json\"\n",
        "!PYTHONPATH=./eraserbenchmark:%PYTHONPATH% && python eraserbenchmark/rationale_benchmark/metrics.py --split test --strict --data_dir Data/explanations --results {eraser_save_path} --score_file {score_file}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n4MkudEzKIKd",
      "metadata": {
        "id": "n4MkudEzKIKd"
      },
      "outputs": [],
      "source": [
        "# 6. CREATE FINAL SUMMARY\n",
        "final_summary = {\n",
        "    \"test_accuracy\": float(result['accuracy']),\n",
        "    \"test_f1\": float(result['f1']),\n",
        "    \"test_loss\": float(result['loss']),\n",
        "    \"gmb_metrics\": gmb_metrics,\n",
        "    \"xai_metrics\": xai_results,\n",
        "    \"total_params\": sum(p.numel() for p in model.base_model.parameters()),\n",
        "}\n",
        "\n",
        "experiment_manager.save_final_metrics(final_summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ICc6xVN1wATg",
      "metadata": {
        "id": "ICc6xVN1wATg"
      },
      "outputs": [],
      "source": [
        "for key, value in final_summary.items():\n",
        "    if isinstance(value, dict):\n",
        "        for k, v in value.items():\n",
        "            print(f\"{key}.{k}: {v}\")\n",
        "        print()\n",
        "    else:\n",
        "        print(f\"{key}: {value}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4iUu9LN2KKRv",
      "metadata": {
        "id": "4iUu9LN2KKRv"
      },
      "outputs": [],
      "source": [
        "# 7. MARK EXPERIMENT AS COMPLETE\n",
        "experiment_manager.mark_complete(\n",
        "    status=\"completed\",\n",
        "    notes=\"Baseline experiment completed successfully\"\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPERIMENT COMPLETED!\")\n",
        "print(f\"All results saved to: {experiment_dir}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9789d842",
      "metadata": {
        "id": "9789d842"
      },
      "source": [
        "## Experiment Management Utilities\n",
        "\n",
        "Useful commands for managing and comparing experiments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba59f95b",
      "metadata": {
        "id": "ba59f95b"
      },
      "outputs": [],
      "source": [
        "# View all experiments\n",
        "experiment_manager = ExperimentManager()\n",
        "experiment_manager.print_experiment_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2ae2540",
      "metadata": {
        "id": "b2ae2540"
      },
      "outputs": [],
      "source": [
        "# List only completed experiments\n",
        "completed_experiments = experiment_manager.list_experiments(status=\"completed\")\n",
        "print(f\"Found {len(completed_experiments)} completed experiments\")\n",
        "for exp in completed_experiments:\n",
        "    print(f\"  - {exp['experiment_id']}: {exp.get('description', 'No description')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pMA6Ti7Hu-nX",
      "metadata": {
        "id": "pMA6Ti7Hu-nX"
      },
      "outputs": [],
      "source": [
        "experiment_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bf5f661",
      "metadata": {
        "id": "5bf5f661"
      },
      "outputs": [],
      "source": [
        "# Compare multiple experiments\n",
        "experiment_ids = [exp['experiment_id'] for exp in completed_experiments]\n",
        "comparison = experiment_manager.compare_experiments(experiment_ids)\n",
        "\n",
        "# Display comparison\n",
        "for exp in comparison[\"experiments\"]:\n",
        "    print(f\"\\nExperiment: {exp['experiment_id']}\")\n",
        "    print(f\"  Model: {exp['config'].get('model_name', 'N/A')}\")\n",
        "    print(f\"  Learning Rate: {exp['config'].get('learning_rate', 'N/A')}\")\n",
        "    print(f\"  Test F1: {exp['metrics'].get('test_f1', 'N/A')}\")\n",
        "    print(f\"  Test Accuracy: {exp['metrics'].get('test_accuracy', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd18f25a",
      "metadata": {
        "id": "fd18f25a"
      },
      "source": [
        "## Visualization Tools\n",
        "\n",
        "Visualize and compare experiment results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a866b568",
      "metadata": {
        "id": "a866b568"
      },
      "outputs": [],
      "source": [
        "from experiment_visualization import (\n",
        "    plot_training_curves,\n",
        "    plot_metrics_comparison,\n",
        "    plot_bias_metrics,\n",
        "    plot_xai_metrics,\n",
        "    create_experiment_report\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R9MrOwBSLckI",
      "metadata": {
        "id": "R9MrOwBSLckI"
      },
      "outputs": [],
      "source": [
        "# Uncomment to use:\n",
        "plot_training_curves(experiment_ids, save_path=\"training_curves.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5452b4ff",
      "metadata": {
        "id": "5452b4ff"
      },
      "outputs": [],
      "source": [
        "# Compare final metrics across experiments\n",
        "plot_metrics_comparison(experiment_ids, save_path=\"metrics_comparison.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fbfc7ee",
      "metadata": {
        "id": "4fbfc7ee"
      },
      "outputs": [],
      "source": [
        "# Visualize bias metrics\n",
        "plot_bias_metrics(experiment_ids, save_path=\"bias_comparison.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fda726e",
      "metadata": {
        "id": "0fda726e"
      },
      "outputs": [],
      "source": [
        "# Visualize XAI metrics\n",
        "plot_xai_metrics(experiment_ids, save_path=\"xai_comparison.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fec2ac71",
      "metadata": {
        "id": "fec2ac71"
      },
      "source": [
        "# Task\n",
        "Enable class weighting for the `hard_label` by calculating inverse class frequency weights from `train_data`, updating `config.class_weighting` to `True`, modifying the `HateClassifier` class definition in `HateClassifier.py` to accept and utilize these weights in `nn.CrossEntropyLoss`, and finally, passing the calculated weights to the `HateClassifier` during its instantiation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3368eac",
      "metadata": {
        "id": "a3368eac"
      },
      "source": [
        "## Calculate Class Weights\n",
        "\n",
        "### Subtask:\n",
        "Calculate the inverse class frequency weights for the `hard_label` in the `train_data` using `sklearn.utils.class_weight.compute_class_weight` and convert them to a PyTorch tensor. This code will be added to the selected cell.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47c7a2f0",
      "metadata": {
        "id": "47c7a2f0"
      },
      "source": [
        "**Reasoning**:\n",
        "To calculate the inverse class frequency weights, I need to import the `compute_class_weight` function, extract the `hard_label`s from the training data, define the classes, compute the weights, and then convert them to a PyTorch tensor.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ca9a278",
      "metadata": {
        "id": "1ca9a278"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# Extract hard_label values from train_data\n",
        "labels = [item['hard_label'] for item in train_data]\n",
        "\n",
        "# Define unique classes\n",
        "classes = [0, 1]\n",
        "\n",
        "# Calculate inverse class frequency weights\n",
        "class_weights_np = compute_class_weight(class_weight='balanced', classes=classes, y=labels)\n",
        "\n",
        "# Convert to PyTorch tensor and move to device\n",
        "class_weights = torch.tensor(class_weights_np, dtype=torch.float32).to(device)\n",
        "\n",
        "print(\"Calculated class weights:\", class_weights)\n",
        "print(\"Class weights moved to device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ff34709",
      "metadata": {
        "id": "7ff34709"
      },
      "source": [
        "**Reasoning**:\n",
        "The error message indicates that the `classes` parameter of `compute_class_weight` expects a NumPy array, not a Python list. I will convert the `classes` list to a NumPy array to fix this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b61c466",
      "metadata": {
        "id": "3b61c466"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np # Import numpy\n",
        "\n",
        "# Extract hard_label values from train_data\n",
        "labels = [item['hard_label'] for item in train_data]\n",
        "\n",
        "# Define unique classes as a NumPy array\n",
        "classes = np.array([0, 1])\n",
        "\n",
        "# Calculate inverse class frequency weights\n",
        "class_weights_np = compute_class_weight(class_weight='balanced', classes=classes, y=labels)\n",
        "\n",
        "# Convert to PyTorch tensor and move to device\n",
        "class_weights = torch.tensor(class_weights_np, dtype=torch.float32).to(device)\n",
        "\n",
        "print(\"Calculated class weights:\", class_weights)\n",
        "print(\"Class weights moved to device:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83cabd22",
      "metadata": {
        "id": "83cabd22"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires setting `config.class_weighting` to `True`. This step updates the configuration object accordingly.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96429b33",
      "metadata": {
        "id": "96429b33"
      },
      "outputs": [],
      "source": [
        "config = TrainingConfig()\n",
        "config.train_attention = True\n",
        "config.num_epochs = 5\n",
        "config.class_weighting = True\n",
        "config.lambda_attn = 1\n",
        "config.use_multi_layer_loss = True\n",
        "config.hidden_dropout_prob = 0.2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be9d88c3",
      "metadata": {
        "id": "be9d88c3"
      },
      "source": [
        "## Update HateClassifier Class\n",
        "\n",
        "### Subtask:\n",
        "Modify the `HateClassifier` class definition (`HateClassifier.py`) to accept a `class_weights` parameter in its `__init__` method and use these weights when initializing `nn.CrossEntropyLoss` within the `_setup_loss_functions` method.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dae945e9",
      "metadata": {
        "id": "dae945e9"
      },
      "source": [
        "**Reasoning**:\n",
        "To guide the user in modifying the `HateClassifier.py` file, I need to first display its current content so they know what to edit. This helps confirm the current state of the file before providing the changes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a3fdf4e",
      "metadata": {
        "id": "4a3fdf4e"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "import os\n",
        "\n",
        "# Get the path to the HateClassifier.py file\n",
        "file_path = os.path.join('HateDeRC', 'HateClassifier.py')\n",
        "\n",
        "# Read and print the content of the file\n",
        "with open(file_path, 'r') as f:\n",
        "    print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaba0086",
      "metadata": {
        "id": "eaba0086"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to read `HateClassifier.py` failed because the path was constructed incorrectly relative to the current working directory. The working directory is already inside `HateDeRC`, so the file path should just be `HateClassifier.py`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09cdf97a",
      "metadata": {
        "id": "09cdf97a"
      },
      "outputs": [],
      "source": [
        "import inspect\n",
        "import os\n",
        "\n",
        "# Get the path to the HateClassifier.py file\n",
        "file_path = 'HateClassifier.py'\n",
        "\n",
        "# Read and print the content of the file\n",
        "with open(file_path, 'r') as f:\n",
        "    print(f.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e21bbe2f",
      "metadata": {
        "id": "e21bbe2f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous output showed the content of the `HateClassifier.py` file. Now, I will generate the code to modify this file by adding `class_weights` to the `__init__` method and ensuring it's used in the `CrossEntropyLoss` initialization. I will use a shell command to overwrite the file with the modified content.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "79c6b074",
      "metadata": {
        "id": "79c6b074"
      },
      "outputs": [],
      "source": [
        "%%writefile HateClassifier.py\n",
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.amp import autocast, GradScaler  # type: ignore\n",
        "from transformers import (\n",
        "    AutoModel,  # pyright: ignore[reportPrivateImportUsage]\n",
        "    AutoConfig,  # pyright: ignore[reportPrivateImportUsage]\n",
        ")\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from TrainingConfig import TrainingConfig\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from typing import Optional\n",
        "\n",
        "\n",
        "class HateClassifier:\n",
        "    \"\"\"\n",
        "    HateDeRC: Hate Speech Detection with Debiasing Residual Connections.\n",
        "\n",
        "    This classifier implements a novel architecture for binary hate speech detection\n",
        "    that reduces dependency on target-sensitive words (e.g., race, religion, gender).\n",
        "\n",
        "    Key Components:\n",
        "    ---------------\n",
        "    1. **DeRC Mechanism (Debiasing Residual Connection)**:\n",
        "       - Creates a residual connection from lower layer (layer 3) to final layer\n",
        "       - Lower layers capture shallow, bias-prone patterns (target words)\n",
        "       - Final layer learns bias-independent features by incorporating debiased residuals\n",
        "       - Prevents over-reliance on lexical shortcuts\n",
        "\n",
        "    2. **Multi-Layer Loss**:\n",
        "       - Auxiliary loss from debias layer (layer 3): guides early layers to learn useful representations\n",
        "       - Main loss from final layer: ensures correct final predictions\n",
        "       - Configurable weighting allows balancing between auxiliary and main objectives\n",
        "\n",
        "    3. **Ranking-Based Attention Supervision**:\n",
        "       - Uses human token-level annotations as supervision signal\n",
        "       - Employs pairwise ranking loss instead of cross-entropy (respects independent annotations)\n",
        "       - Enforces: tokens with higher human importance should receive higher attention\n",
        "       - Helps model focus on contextually relevant hate indicators\n",
        "\n",
        "    Loss Function:\n",
        "    --------------\n",
        "    Total Loss = α × lower_layer_loss + β × upper_layer_loss + λ × attention_ranking_loss\n",
        "\n",
        "    Where:\n",
        "    - α (lower_loss_weight): Weight for auxiliary classification loss\n",
        "    - β (upper_loss_weight): Weight for main classification loss\n",
        "    - λ (lambda_attn): Weight for attention supervision loss\n",
        "\n",
        "    Architecture:\n",
        "    -------------\n",
        "    Input → BERT-like Encoder (all hidden states) → Multi-layer Classifiers\n",
        "                                ↓\n",
        "                          Layer 3 (debias) -----(residual)----→ Final Layer\n",
        "                                ↓                                    ↓\n",
        "                         Auxiliary Loss                        Main Loss\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: TrainingConfig, class_weights: Optional[torch.Tensor] = None, **kwargs):\n",
        "        self.config = config\n",
        "        self.class_weights = class_weights # Store class weights\n",
        "\n",
        "        # Initialize device:\n",
        "        if torch.cuda.is_available():\n",
        "            self.device = \"cuda\"\n",
        "        else:\n",
        "            self.device = \"cpu\"\n",
        "\n",
        "        # Configure & initialize the base model\n",
        "        model_config = AutoConfig.from_pretrained(\n",
        "            config.model_name, output_attentions=True, output_hidden_states=True\n",
        "        )\n",
        "        self.base_model = AutoModel.from_pretrained(\n",
        "            config.model_name, config=model_config\n",
        "        )\n",
        "\n",
        "        # Get model dimensions\n",
        "        hidden_size = self.base_model.config.hidden_size\n",
        "        self.num_layers = self.base_model.config.num_hidden_layers\n",
        "\n",
        "        # Multi-layer classifier heads (one for each transformer layer)\n",
        "        self.classifier_list = nn.ModuleList(\n",
        "            [nn.Linear(hidden_size, config.num_labels) for _ in range(self.num_layers)]\n",
        "        )\n",
        "\n",
        "        # Dropout layer\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "        # Layer configuration for debiasing (similar to BertDistill)\n",
        "        self.debias_layer = 3  # Layer index for auxiliary loss\n",
        "        self.use_multi_layer_loss = getattr(config, \"use_multi_layer_loss\", False)\n",
        "\n",
        "        # Loss Weighting Configuration\n",
        "        self.lower_loss_weight = getattr(\n",
        "            config, \"lower_loss_weight\", 0.5\n",
        "        )  # α: auxiliary loss weight\n",
        "        self.upper_loss_weight = getattr(\n",
        "            config, \"upper_loss_weight\", 0.5\n",
        "        )  # β: main loss weight\n",
        "        self.lambda_attn = getattr(\n",
        "            config, \"lambda_attn\", 0.1\n",
        "        )  # λ: attention loss weight\n",
        "\n",
        "        # Attention Training Configuration\n",
        "        self.train_attention = getattr(config, \"train_attention\", False)\n",
        "        self.ranking_margin = getattr(\n",
        "            config, \"ranking_margin\", 0.1\n",
        "        )  # Margin for pairwise ranking\n",
        "        self.ranking_threshold = getattr(\n",
        "            config, \"ranking_threshold\", 0.05\n",
        "        )  # Threshold for significant pairs\n",
        "\n",
        "        # Move models to device\n",
        "        self.base_model.to(self.device)\n",
        "        self.classifier_list.to(self.device)\n",
        "\n",
        "        # Configure loss function\n",
        "        if config.class_weighting:\n",
        "            if self.class_weights is not None:\n",
        "                class_weight_on_device = self.class_weights.to(self.device)\n",
        "                self.cls_criterion = CrossEntropyLoss(weight=class_weight_on_device)\n",
        "            else:\n",
        "                raise ValueError(\"config.class_weighting is True but no class_weights were provided.\")\n",
        "        else:\n",
        "            self.cls_criterion = CrossEntropyLoss()\n",
        "\n",
        "        # Configure optimizer (for base model and all classifiers)\n",
        "        params = list(self.base_model.parameters()) + list(\n",
        "            self.classifier_list.parameters()\n",
        "        )\n",
        "        self.optimizer = AdamW(params, lr=config.learning_rate)\n",
        "\n",
        "        # Learning rate scheduler\n",
        "        self.scheduler = None\n",
        "\n",
        "        # Mixed precision training\n",
        "        self.use_amp = config.use_amp and torch.cuda.is_available()\n",
        "        self.scaler = GradScaler() if self.use_amp else None\n",
        "\n",
        "        # Gradient accumulation\n",
        "        self.gradient_accumulation_steps = config.gradient_accumulation_steps\n",
        "        self.max_grad_norm = config.max_grad_norm\n",
        "\n",
        "        # Torch compile (PyTorch 2.0+)\n",
        "        if hasattr(config, \"use_compile\") and config.use_compile:\n",
        "            try:\n",
        "                self.base_model = torch.compile(self.base_model)\n",
        "                print(\"✓ Model compiled with torch.compile\")\n",
        "            except Exception as e:\n",
        "                print(f\"Warning: torch.compile failed: {e}\")\n",
        "\n",
        "        # Training history\n",
        "        self.history = {\n",
        "            \"train_loss\": [],\n",
        "            \"val_loss\": [],\n",
        "            \"val_accuracy\": [],\n",
        "            \"val_f1\": [],\n",
        "        }\n",
        "\n",
        "    def train_epoch(self, train_dataloader):\n",
        "        \"\"\"\n",
        "        Train for one epoch using the HateDeRC architecture.\n",
        "\n",
        "        Returns:\n",
        "            float: Average total loss for the epoch\n",
        "        \"\"\"\n",
        "        self.base_model.train()\n",
        "        for classifier in self.classifier_list:\n",
        "            classifier.train()\n",
        "\n",
        "        # Track individual loss components for monitoring\n",
        "        total_loss = 0\n",
        "        total_cls_loss = 0\n",
        "        total_lower_loss = 0\n",
        "        total_attn_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        progress_bar = tqdm(train_dataloader, desc=\"Training\", unit=\"batch\")\n",
        "        for batch_idx, batch in enumerate(progress_bar):\n",
        "            input_ids = batch[\"input_ids\"].to(self.device, non_blocking=True)\n",
        "            attention_mask = batch[\"attention_mask\"].to(self.device, non_blocking=True)\n",
        "            labels = batch[\"labels\"].to(self.device, non_blocking=True)\n",
        "\n",
        "            # Mixed precision context\n",
        "            with autocast(device_type=\"cuda\", enabled=self.use_amp):\n",
        "                # Forward Pass through base model (get all hidden states)\n",
        "                outputs = self.base_model(\n",
        "                    input_ids=input_ids,\n",
        "                    attention_mask=attention_mask,\n",
        "                    output_hidden_states=True,\n",
        "                    output_attentions=self.train_attention,\n",
        "                )\n",
        "\n",
        "                # Get hidden states from all layers\n",
        "                hidden_states = outputs.hidden_states  # Tuple of (num_layers+1) tensors\n",
        "\n",
        "                # Apply pooling to each layer's hidden state (extract CLS token)\n",
        "                # Note: hidden_states[0] is embeddings, hidden_states[1:] are transformer layers\n",
        "                pooled_outputs = []\n",
        "                for i in range(1, len(hidden_states)):  # Skip embeddings layer\n",
        "                    cls_token = hidden_states[i][:, 0, :]  # Get CLS token\n",
        "                    pooled_outputs.append(cls_token)\n",
        "\n",
        "                # Get logits from all classifier heads\n",
        "                logits_list = []\n",
        "                for i, pooled_output in enumerate(pooled_outputs):\n",
        "                    if i == len(pooled_outputs) - 1 and self.use_multi_layer_loss:\n",
        "                        # Last layer: add residual connection from debias layer\n",
        "                        combined = (\n",
        "                            self.dropout(pooled_output)\n",
        "                            + self.dropout(pooled_outputs[self.debias_layer]).detach()\n",
        "                        )\n",
        "                        logits = self.classifier_list[i](combined)\n",
        "                    else:\n",
        "                        logits = self.classifier_list[i](self.dropout(pooled_output))\n",
        "                    logits_list.append(logits)\n",
        "\n",
        "                # Calculate unified loss with all components\n",
        "                loss_dict = self._calculate_loss(\n",
        "                    logits_list=logits_list,\n",
        "                    labels=labels,\n",
        "                    attention_mask=attention_mask,\n",
        "                    attentions=outputs.attentions if self.train_attention else None,\n",
        "                    human_rationales=(\n",
        "                        batch.get(\"rationales\") if self.train_attention else None\n",
        "                    ),\n",
        "                )\n",
        "\n",
        "                loss = loss_dict[\"total_loss\"]\n",
        "\n",
        "                # Track individual loss components\n",
        "                if \"cls_loss\" in loss_dict:\n",
        "                    total_cls_loss += loss_dict[\"cls_loss\"]\n",
        "                if \"lower_loss\" in loss_dict:\n",
        "                    total_lower_loss += loss_dict[\"lower_loss\"]\n",
        "                if \"attn_loss\" in loss_dict:\n",
        "                    total_attn_loss += loss_dict[\"attn_loss\"]\n",
        "\n",
        "                # Scale loss for gradient accumulation\n",
        "                loss = loss / self.gradient_accumulation_steps\n",
        "\n",
        "            total_loss += loss.item() * self.gradient_accumulation_steps\n",
        "            num_batches += 1\n",
        "\n",
        "            # Backward pass with gradient scaling\n",
        "            if self.use_amp and self.scaler is not None:\n",
        "                self.scaler.scale(loss).backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            # Gradient accumulation: only step optimizer every N batches\n",
        "            if (batch_idx + 1) % self.gradient_accumulation_steps == 0:\n",
        "                # Gradient clipping\n",
        "                if self.use_amp and self.scaler is not None:\n",
        "                    self.scaler.unscale_(self.optimizer)\n",
        "                    torch.nn.utils.clip_grad_norm_(\n",
        "                        self.base_model.parameters(), self.max_grad_norm\n",
        "                    )\n",
        "                    torch.nn.utils.clip_grad_norm_(\n",
        "                        self.classifier_list.parameters(), self.max_grad_norm\n",
        "                    )\n",
        "                    self.scaler.step(self.optimizer)\n",
        "                    self.scaler.update()\n",
        "                else:\n",
        "                    torch.nn.utils.clip_grad_norm_(\n",
        "                        self.base_model.parameters(), self.max_grad_norm\n",
        "                    )\n",
        "                    torch.nn.utils.clip_grad_norm_(\n",
        "                        self.classifier_list.parameters(), self.max_grad_norm\n",
        "                    )\n",
        "                    self.optimizer.step()\n",
        "\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                if self.scheduler:\n",
        "                    self.scheduler.step()\n",
        "\n",
        "            # Update progress bar with relevant loss components\n",
        "            postfix = {\"total\": total_loss / num_batches}\n",
        "\n",
        "            if self.use_multi_layer_loss:\n",
        "                postfix[\"main\"] = total_cls_loss / num_batches\n",
        "                postfix[\"aux\"] = total_lower_loss / num_batches\n",
        "            else:\n",
        "                postfix[\"cls\"] = total_cls_loss / num_batches\n",
        "\n",
        "            if self.train_attention and total_attn_loss > 0:\n",
        "                postfix[\"attn\"] = total_attn_loss / num_batches\n",
        "\n",
        "            progress_bar.set_postfix(postfix)\n",
        "\n",
        "        return total_loss / num_batches\n",
        "\n",
        "    def evaluate(self, val_dataloader):\n",
        "        self.base_model.eval()\n",
        "        for classifier in self.classifier_list:\n",
        "            classifier.eval()\n",
        "\n",
        "        total_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(val_dataloader, desc=\"Evaluating\", unit=\"batch\"):\n",
        "                input_ids = batch[\"input_ids\"].to(self.device, non_blocking=True)\n",
        "                attention_mask = batch[\"attention_mask\"].to(\n",
        "                    self.device, non_blocking=True\n",
        "                )\n",
        "                labels = batch[\"labels\"].to(self.device, non_blocking=True)\n",
        "\n",
        "                # Mixed precision context for evaluation\n",
        "                with autocast(device_type=\"cuda\", enabled=self.use_amp):\n",
        "                    # Forward pass through base model (no hidden states needed - more efficient)\n",
        "                    outputs = self.base_model(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                    )\n",
        "\n",
        "                    # Get final layer's CLS token (for evaluation, only use final classifier)\n",
        "                    cls_token = outputs.last_hidden_state[:, 0, :]\n",
        "\n",
        "                    # Get logits from final classifier (no dropout in eval mode)\n",
        "                    logits = self.classifier_list[-1](cls_token)\n",
        "\n",
        "                    # Calculate loss\n",
        "                    loss = self.cls_criterion(logits, labels)\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "                # Get predictions\n",
        "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "\n",
        "                # Store predictions and labels\n",
        "                all_preds.extend(preds)\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "        f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "\n",
        "        # Calculate loss\n",
        "        avg_loss = total_loss / len(val_dataloader)\n",
        "\n",
        "        return avg_loss, accuracy, f1\n",
        "\n",
        "    def train(self, train_dataloader, val_dataloader):\n",
        "        \"\"\"\n",
        "        Train the HateDeRC model with multi-component loss.\n",
        "\n",
        "        Training Configuration:\n",
        "        - DeRC: Residual connection from layer 3 to final layer\n",
        "        - Multi-layer loss: Auxiliary (layer 3) + Main (final layer)\n",
        "        - Attention supervision: Ranking-based loss from human annotations\n",
        "\n",
        "        Loss Weights:\n",
        "        - Lower loss: {:.2f}\n",
        "        - Upper loss: {:.2f}\n",
        "        - Attention: {:.2f}\n",
        "\n",
        "        Args:\n",
        "            train_dataloader: DataLoader for training data\n",
        "            val_dataloader: DataLoader for validation data\n",
        "\n",
        "        Returns:\n",
        "            dict: Training history with loss and metrics per epoch\n",
        "        \"\"\".format(\n",
        "            self.lower_loss_weight, self.upper_loss_weight, self.lambda_attn\n",
        "        )\n",
        "        print(f\"Training on device: {self.device}\")\n",
        "        print(f\"Model: {self.config.model_name}\")\n",
        "        print(f\"Epochs: {self.config.num_epochs}\")\n",
        "        print(f\"Batch size: {self.config.batch_size}\")\n",
        "        print(f\"Gradient accumulation steps: {self.gradient_accumulation_steps}\")\n",
        "        print(\n",
        "            f\"Effective batch size: {self.config.batch_size * self.gradient_accumulation_steps}\"\n",
        "        )\n",
        "        print(f\"Learning rate: {self.config.learning_rate}\")\n",
        "        print(f\"Mixed precision (AMP): {self.use_amp}\")\n",
        "        print(f\"Gradient clipping: {self.max_grad_norm}\")\n",
        "        print(f\"\\nLoss Configuration:\")\n",
        "        print(f\"  Multi-layer loss: {self.use_multi_layer_loss}\")\n",
        "        if self.use_multi_layer_loss:\n",
        "            print(\n",
        "                f\"    - Auxiliary (layer {self.debias_layer}): α={self.lower_loss_weight}\"\n",
        "            )\n",
        "            print(f\"    - Main (final layer): β={self.upper_loss_weight}\")\n",
        "        print(f\"  Attention supervision: {self.train_attention}\")\n",
        "        if self.train_attention:\n",
        "            print(f\"    - Ranking loss: λ={self.lambda_attn}\")\n",
        "            print(\n",
        "                f\"    - Margin: {self.ranking_margin}, Threshold: {self.ranking_threshold}\"\n",
        "            )\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        best_f1 = 0.0\n",
        "\n",
        "        for epoch in range(self.config.num_epochs):\n",
        "            print(f\"\\nEpoch {epoch + 1}/{self.config.num_epochs}\")\n",
        "\n",
        "            # Train for one epoch\n",
        "            train_loss = self.train_epoch(train_dataloader)\n",
        "\n",
        "            # Evaluate on validation set\n",
        "            val_loss, val_accuracy, val_f1 = self.evaluate(val_dataloader)\n",
        "\n",
        "            # Store metrics in history\n",
        "            self.history[\"train_loss\"].append(train_loss)\n",
        "            self.history[\"val_loss\"].append(val_loss)\n",
        "            self.history[\"val_accuracy\"].append(val_accuracy)\n",
        "            self.history[\"val_f1\"].append(val_f1)\n",
        "\n",
        "            # Print epoch summary\n",
        "            print(f\"\\nEpoch {epoch + 1} Summary:\")\n",
        "            print(f\"  Train Loss: {train_loss:.4f}\")\n",
        "            print(f\"  Val Loss:   {val_loss:.4f}\")\n",
        "            print(f\"  Val Acc:    {val_accuracy:.4f}\")\n",
        "            print(f\"  Val F1:     {val_f1:.4f}\")\n",
        "\n",
        "            # Save best model\n",
        "            if val_f1 > best_f1:\n",
        "                best_f1 = val_f1\n",
        "                self.save_model(\"best_model\")\n",
        "                print(f\"  ✓ New best model saved! (F1: {best_f1:.4f})\")\n",
        "\n",
        "            # Save checkpoint every epoch\n",
        "            self.save_model(f\"checkpoint_epoch_{epoch + 1}\")\n",
        "\n",
        "        # Save final model and training history\n",
        "        self.save_model(\"final_model\")\n",
        "        self.save_history()\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(f\"Training completed!\")\n",
        "        print(f\"Best F1 Score: {best_f1:.4f}\")\n",
        "        print(\n",
        "            f\"Training history saved to: {self.config.save_dir}/training_history.json\"\n",
        "        )\n",
        "\n",
        "        return self.history\n",
        "\n",
        "    def save_model(self, name: str):\n",
        "        \"\"\"\n",
        "        Save model checkpoint.\n",
        "        \"\"\"\n",
        "        save_path = Path(self.config.save_dir) / name\n",
        "        save_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Save base model\n",
        "        self.base_model.save_pretrained(save_path)\n",
        "\n",
        "        # Save all classifier heads and optimizer state\n",
        "        torch.save(\n",
        "            {\n",
        "                \"classifier_list_state_dict\": self.classifier_list.state_dict(),\n",
        "                \"optimizer_state_dict\": self.optimizer.state_dict(),\n",
        "                \"config\": self.config,\n",
        "            },\n",
        "            save_path / \"training_state.pt\",\n",
        "        )\n",
        "\n",
        "    def load_model(self, name: str):\n",
        "        \"\"\"\n",
        "        Load model checkpoint.\n",
        "        \"\"\"\n",
        "        load_path = Path(self.config.save_dir) / name\n",
        "\n",
        "        # Load base model\n",
        "        self.base_model = AutoModel.from_pretrained(load_path)\n",
        "        self.base_model.to(self.device)\n",
        "\n",
        "        # Load all classifier heads and optimizer state\n",
        "        # Note: weights_only=False is safe for your own checkpoints\n",
        "        checkpoint = torch.load(load_path / \"training_state.pt\", weights_only=False)\n",
        "        self.classifier_list.load_state_dict(checkpoint[\"classifier_list_state_dict\"])\n",
        "        self.optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "        print(f\"Model loaded from: {load_path}\")\n",
        "\n",
        "    def save_history(self):\n",
        "        \"\"\"\n",
        "        Save training history to JSON file.\n",
        "        \"\"\"\n",
        "        save_path = Path(self.config.save_dir)\n",
        "        save_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        history_path = save_path / \"training_history.json\"\n",
        "        with open(history_path, \"w\") as f:\n",
        "            json.dump(self.history, f, indent=2)\n",
        "\n",
        "    def load_history(self):\n",
        "        \"\"\"\n",
        "        Load training history from JSON file.\n",
        "        \"\"\"\n",
        "        history_path = Path(self.config.save_dir) / \"training_history.json\"\n",
        "\n",
        "        if history_path.exists():\n",
        "            with open(history_path, \"r\") as f:\n",
        "                self.history = json.load(f)\n",
        "            print(f\"Training history loaded from: {history_path}\")\n",
        "        else:\n",
        "            print(f\"No training history found at: {history_path}\")\n",
        "\n",
        "    def predict(\n",
        "        self, test_dataloader, return_layer_outputs=False, return_attentions=False\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Run inference on test data and return predictions with metrics.\n",
        "\n",
        "        Args:\n",
        "            test_dataloader: DataLoader for test data\n",
        "            return_layer_outputs: If True, returns predictions from all layers\n",
        "\n",
        "        Returns:\n",
        "            dict: Contains predictions, true labels, probabilities, loss, accuracy, and F1 score\n",
        "        \"\"\"\n",
        "        self.base_model.eval()\n",
        "        for classifier in self.classifier_list:\n",
        "            classifier.eval()\n",
        "\n",
        "        total_loss = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "        all_probs = []\n",
        "        all_attention_weights = []\n",
        "        all_post_ids = []\n",
        "        all_layer_preds = (\n",
        "            [[] for _ in range(self.num_layers)] if return_layer_outputs else None\n",
        "        )\n",
        "\n",
        "        print(f\"Running inference on {len(test_dataloader)} batches...\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in tqdm(test_dataloader, desc=\"Testing\", unit=\"batch\"):\n",
        "                input_ids = batch[\"input_ids\"].to(self.device, non_blocking=True)\n",
        "                attention_mask = batch[\"attention_mask\"].to(\n",
        "                    self.device, non_blocking=True\n",
        "                )\n",
        "                labels = batch[\"labels\"].to(self.device, non_blocking=True)\n",
        "                post_id = batch[\"post_id\"]\n",
        "\n",
        "                # Mixed precision context for inference\n",
        "                with autocast(device_type=\"cuda\", enabled=self.use_amp):\n",
        "                    # Forward pass through base model (only request hidden states if needed)\n",
        "                    outputs = self.base_model(\n",
        "                        input_ids=input_ids,\n",
        "                        attention_mask=attention_mask,\n",
        "                        output_hidden_states=return_layer_outputs,\n",
        "                        output_attentions=return_attentions,\n",
        "                    )\n",
        "\n",
        "                    cls_token = outputs.last_hidden_state[:, 0, :]\n",
        "                    logits = self.classifier_list[-1](cls_token)\n",
        "\n",
        "                    # Calculate loss\n",
        "                    loss = self.cls_criterion(logits, labels)\n",
        "                    total_loss += loss.item()\n",
        "\n",
        "                    # Get predictions and probabilities\n",
        "                    probs = F.softmax(logits, dim=1)\n",
        "                    preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "                # Store predictions, labels, and probabilities\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "                all_probs.extend(probs.cpu().numpy())\n",
        "                all_post_ids.extend(post_id)\n",
        "                # Store attentions if requested\n",
        "                if return_attentions:\n",
        "                    attention_result = self.extract_attention(outputs.attentions)\n",
        "                    if attention_result is not None:\n",
        "                        all_attention_weights.extend(attention_result)\n",
        "                    else:\n",
        "                        print(\"No attention weights extracted.\")\n",
        "\n",
        "                # Optionally get predictions from all layers\n",
        "                if return_layer_outputs and all_layer_preds is not None:\n",
        "                    hidden_states = outputs.hidden_states[1:]  # Skip embeddings\n",
        "                    for i, hidden_state in enumerate(hidden_states):\n",
        "                        layer_cls = hidden_state[:, 0, :]\n",
        "                        layer_logits = self.classifier_list[i](layer_cls)\n",
        "                        layer_preds = torch.argmax(layer_logits, dim=1)\n",
        "                        all_layer_preds[i].extend(layer_preds.cpu().numpy())\n",
        "\n",
        "        # Convert to numpy arrays\n",
        "        all_preds = np.array(all_preds)\n",
        "        all_labels = np.array(all_labels)\n",
        "        all_probs = np.array(all_probs)\n",
        "\n",
        "        # Calculate metrics\n",
        "        accuracy = accuracy_score(all_labels, all_preds)\n",
        "        f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
        "        avg_loss = total_loss / len(test_dataloader)\n",
        "\n",
        "        # Print summary\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(\"Test Results:\")\n",
        "        print(f\"  Test Loss:     {avg_loss:.4f}\")\n",
        "        print(f\"  Test Accuracy: {accuracy:.4f}\")\n",
        "        print(f\"  Test F1:       {f1:.4f}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        results = {\n",
        "            \"post_ids\": all_post_ids,\n",
        "            \"predictions\": all_preds,\n",
        "            \"labels\": all_labels,\n",
        "            \"probabilities\": all_probs,\n",
        "            \"loss\": avg_loss,\n",
        "            \"accuracy\": accuracy,\n",
        "            \"f1\": f1,\n",
        "        }\n",
        "\n",
        "        if return_layer_outputs and all_layer_preds is not None:\n",
        "            results[\"layer_predictions\"] = [\n",
        "                np.array(preds) for preds in all_layer_preds\n",
        "            ]\n",
        "\n",
        "        if return_attentions:\n",
        "            results[\"attentions\"] = all_attention_weights\n",
        "\n",
        "        return results\n",
        "\n",
        "    def save_predictions(self, results: dict, filename: str = \"test_results.json\"):\n",
        "        \"\"\"\n",
        "        Save prediction results to file.\n",
        "\n",
        "        Args:\n",
        "            results: Dictionary returned from predict() method\n",
        "            filename: Name of the file to save results\n",
        "        \"\"\"\n",
        "        save_path = Path(self.config.save_dir)\n",
        "        save_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Convert numpy arrays to lists for JSON serialization\n",
        "        results_serializable = {\n",
        "            \"predictions\": results[\"predictions\"].tolist(),\n",
        "            \"labels\": results[\"labels\"].tolist(),\n",
        "            \"probabilities\": results[\"probabilities\"].tolist(),\n",
        "            \"loss\": float(results[\"loss\"]),\n",
        "            \"accuracy\": float(results[\"accuracy\"]),\n",
        "            \"f1\": float(results[\"f1\"]),\n",
        "        }\n",
        "\n",
        "        results_path = save_path / filename\n",
        "        with open(results_path, \"w\") as f:\n",
        "            json.dump(results_serializable, f, indent=2)\n",
        "\n",
        "        print(f\"Test results saved to: {results_path}\")\n",
        "\n",
        "    def extract_attention(self, attentions, return_tensor=False):\n",
        "        \"\"\"\n",
        "        Extract CLS token attention from last layer, averaged across all heads.\n",
        "\n",
        "        Args:\n",
        "            attentions: Tuple of attention tensors from model\n",
        "            return_tensor: If True, returns tensor (for training). If False, returns numpy (for inference)\n",
        "\n",
        "        Returns:\n",
        "            (batch_size, seq_len) attention weights as tensor or numpy array\n",
        "        \"\"\"\n",
        "        if attentions is None or len(attentions) == 0:\n",
        "            print(\"WARNING: No attention data available.\")\n",
        "            return None\n",
        "\n",
        "        # Take CLS representation from the last layer's attentions\n",
        "        last_layer_attentions = attentions[\n",
        "            -1\n",
        "        ]  # Shape: (batch_size, num_heads, seq_len, seq_len)\n",
        "        cls_attentions = last_layer_attentions[\n",
        "            :, :, 0, :\n",
        "        ]  # Shape: (batch_size, num_heads, seq_len)\n",
        "        # Average over all heads\n",
        "        avg_cls_attention = cls_attentions.mean(dim=1)  # Shape: (batch_size, seq_len)\n",
        "\n",
        "        if return_tensor:\n",
        "            return (\n",
        "                avg_cls_attention  # Keep as tensor for training (preserves gradients)\n",
        "            )\n",
        "        else:\n",
        "            return avg_cls_attention.cpu().numpy()  # Convert to numpy for inference\n",
        "\n",
        "    def _calculate_loss(\n",
        "        self,\n",
        "        logits_list,\n",
        "        labels,\n",
        "        attention_mask,\n",
        "        attentions=None,\n",
        "        human_rationales=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Calculate unified loss with configurable component weights.\n",
        "\n",
        "        This method computes the total loss as a weighted combination of:\n",
        "        1. Classification loss (auxiliary from debias layer if multi-layer enabled)\n",
        "        2. Main classification loss (from final layer)\n",
        "        3. Attention ranking loss (if attention supervision enabled)\n",
        "\n",
        "        Args:\n",
        "            logits_list: List of logits from all classifier heads\n",
        "            labels: Ground truth labels (batch_size,)\n",
        "            attention_mask: Attention mask for padding (batch_size, seq_len)\n",
        "            attentions: Tuple of attention tensors from model (optional)\n",
        "            human_rationales: Human token annotations (batch_size, seq_len) (optional)\n",
        "\n",
        "        Returns:\n",
        "            dict: Dictionary containing:\n",
        "                - 'total_loss': Weighted sum of all loss components\n",
        "                - 'cls_loss': Main classification loss (if applicable)\n",
        "                - 'lower_loss': Auxiliary classification loss (if multi-layer enabled)\n",
        "                - 'attn_loss': Attention ranking loss (if attention training enabled)\n",
        "        \"\"\"\n",
        "        loss_dict = {}\n",
        "\n",
        "        # Main logits from final layer\n",
        "        final_logits = logits_list[-1]\n",
        "\n",
        "        # Component 1 & 2: Classification Losses\n",
        "        if self.use_multi_layer_loss:\n",
        "            # Auxiliary loss from debias layer (helps guide lower layer representations)\n",
        "            lower_loss = self.cls_criterion(logits_list[self.debias_layer], labels)\n",
        "\n",
        "            # Main loss from final layer (with residual connection)\n",
        "            upper_loss = self.cls_criterion(final_logits, labels)\n",
        "\n",
        "            # Weighted combination: Total = α × lower + β × upper\n",
        "            cls_loss = (\n",
        "                self.lower_loss_weight * lower_loss\n",
        "                + self.upper_loss_weight * upper_loss\n",
        "            )\n",
        "\n",
        "            loss_dict[\"lower_loss\"] = lower_loss.item()\n",
        "            loss_dict[\"cls_loss\"] = upper_loss.item()\n",
        "        else:\n",
        "            # Single loss from final layer only (baseline)\n",
        "            cls_loss = self.cls_criterion(final_logits, labels)\n",
        "            loss_dict[\"cls_loss\"] = cls_loss.item()\n",
        "\n",
        "        total_loss = cls_loss\n",
        "\n",
        "        # Component 3: Attention Ranking Loss\n",
        "        if (\n",
        "            self.train_attention\n",
        "            and attentions is not None\n",
        "            and human_rationales is not None\n",
        "        ):\n",
        "            if len(attentions) > 0:\n",
        "                # Extract model attention from last layer\n",
        "                model_attention = self.extract_attention(attentions, return_tensor=True)\n",
        "\n",
        "                # Move rationales to device\n",
        "                human_rationales = human_rationales.to(self.device, non_blocking=True)\n",
        "\n",
        "                # Calculate ranking loss (already weighted by lambda_attn internally)\n",
        "                attn_loss = self.calculate_attention_loss(\n",
        "                    human_rationales, model_attention, attention_mask\n",
        "                )\n",
        "\n",
        "                total_loss = total_loss + attn_loss\n",
        "                loss_dict[\"attn_loss\"] = attn_loss.item()\n",
        "\n",
        "        loss_dict[\"total_loss\"] = total_loss\n",
        "        return loss_dict\n",
        "\n",
        "    def calculate_attention_loss(\n",
        "        self, human_rationales, models_attentions, attention_mask\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Calculate pairwise margin ranking loss for attention supervision.\n",
        "\n",
        "        For each pair of tokens (i, j) where human_score[i] > human_score[j],\n",
        "        we enforce: attention[i] - attention[j] >= margin\n",
        "\n",
        "        This respects the independent nature of human annotations and focuses on\n",
        "        relative importance rather than absolute values.\n",
        "\n",
        "        Args:\n",
        "            human_rationales: (batch_size, seq_len) - Independent token importance scores [0-1]\n",
        "            models_attentions: (batch_size, seq_len) - Model attention weights (softmax tensor)\n",
        "            attention_mask: (batch_size, seq_len) - Mask for padding tokens\n",
        "\n",
        "        Returns:\n",
        "            Scalar ranking loss\n",
        "        \"\"\"\n",
        "        # Ensure models_attentions is a tensor (should be from extract_attention with return_tensor=True)\n",
        "        if isinstance(models_attentions, np.ndarray):\n",
        "            models_attentions = torch.from_numpy(models_attentions).to(self.device)\n",
        "\n",
        "        batch_size, seq_len = human_rationales.shape\n",
        "\n",
        "        # Mask out padding positions\n",
        "        human_rationales = human_rationales * attention_mask\n",
        "        models_attentions = models_attentions * attention_mask\n",
        "\n",
        "        total_loss = 0.0\n",
        "        total_pairs = 0\n",
        "\n",
        "        for b in range(batch_size):\n",
        "            # Get valid (non-padding) positions for this sample\n",
        "            valid_mask = attention_mask[b].bool()\n",
        "            valid_indices = torch.where(valid_mask)[0]\n",
        "\n",
        "            if len(valid_indices) < 2:\n",
        "                continue  # Skip if less than 2 valid tokens\n",
        "\n",
        "            # Get human scores and model attentions for valid tokens\n",
        "            human_scores = human_rationales[b, valid_indices]  # (num_valid,)\n",
        "            model_attn = models_attentions[b, valid_indices]  # (num_valid,)\n",
        "\n",
        "            # Create all pairs: (num_valid, num_valid)\n",
        "            # human_i: (num_valid, 1), human_j: (1, num_valid)\n",
        "            human_i = human_scores.unsqueeze(1)  # (num_valid, 1)\n",
        "            human_j = human_scores.unsqueeze(0)  # (1, num_valid)\n",
        "\n",
        "            model_i = model_attn.unsqueeze(1)  # (num_valid, 1)\n",
        "            model_j = model_attn.unsqueeze(0)  # (1, num_valid)\n",
        "\n",
        "            # Find pairs where human_i > human_j (should have model_i > model_j)\n",
        "            human_diff = human_i - human_j  # (num_valid, num_valid)\n",
        "            model_diff = model_i - model_j  # (num_valid, num_valid)\n",
        "\n",
        "            # Only consider pairs where there's a clear difference in human scores\n",
        "            # (avoid pairs with very similar scores)\n",
        "            significant_pairs = (\n",
        "                human_diff > self.ranking_threshold\n",
        "            ).float()  # Threshold to avoid noise\n",
        "\n",
        "            # Margin ranking loss: max(0, margin - (model_i - model_j)) when human_i > human_j\n",
        "            # We want: model_i - model_j >= margin when human_i > human_j\n",
        "            ranking_loss = torch.relu(self.ranking_margin - model_diff)\n",
        "\n",
        "            # Apply mask to only consider significant pairs\n",
        "            ranking_loss = ranking_loss * significant_pairs\n",
        "\n",
        "            # Accumulate\n",
        "            num_pairs = significant_pairs.sum()\n",
        "            if num_pairs > 0:\n",
        "                total_loss += ranking_loss.sum() / num_pairs\n",
        "                total_pairs += 1\n",
        "\n",
        "        if total_pairs == 0:\n",
        "            return torch.tensor(0.0, device=human_rationales.device)\n",
        "\n",
        "        avg_loss = total_loss / total_pairs\n",
        "        return self.lambda_attn * avg_loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfaddcb8",
      "metadata": {
        "id": "bfaddcb8"
      },
      "source": [
        "**Reasoning**:\n",
        "The `HateClassifier.py` file has been modified in the previous step. To ensure that the Python runtime uses the updated version of the class, I need to restart the Python interpreter. This action will clear the current kernel state and reload all modules, allowing the modified `HateClassifier` class to be imported correctly in subsequent steps.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8073aee",
      "metadata": {
        "id": "b8073aee"
      },
      "outputs": [],
      "source": [
        "exit()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf6800b3",
      "metadata": {
        "id": "cf6800b3"
      },
      "source": [
        "### Important: Restart and Re-run Cells\n",
        "\n",
        "The Python runtime has been restarted. To proceed with the task, you must re-run all the necessary setup cells from the beginning of the notebook until this point.\n",
        "\n",
        "Specifically, please re-execute the cells that perform the following actions:\n",
        "\n",
        "1.  **Clone the `HateDeRC` repository and change directory** (Cell `24658442` and `8bccce96`)\n",
        "2.  **Define `data_path`** (Cell `1f654143`)\n",
        "3.  **Load data, tokenizer, and process data splits** (Cells `f81645fa`, `f070b12b`, `8d446ddb`, `pEC9l6Mr6xo0`, `43NMFuwv62Q4`)\n",
        "4.  **Set `config` parameters** (Cell `b3618d6e` - ensure `config.class_weighting = True` is set again)\n",
        "5.  **Set random seeds for reproducibility** (Cell `487ff027`)\n",
        "6.  **Create `HateDataset` instances** (Cell `87c63ecf`)\n",
        "7.  **Create `DataLoader` instances** (Cell `f4887f43`)\n",
        "8.  **Calculate class weights** (Cell `3b61c466`)\n",
        "\n",
        "After re-running these cells, you can continue with the next step, which will involve instantiating the `HateClassifier` with the `class_weights`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be70d489",
      "metadata": {
        "id": "be70d489"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that the `HateClassifier.py` file has been modified to accept class weights, and assuming the runtime has been restarted and all previous setup cells (including `config` and `class_weights` calculation) have been re-executed, the next step is to instantiate the `HateClassifier` class, passing the `config` object and the calculated `class_weights`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf04ec2b",
      "metadata": {
        "id": "bf04ec2b"
      },
      "outputs": [],
      "source": [
        "model = HateClassifier(config, class_weights=class_weights)\n",
        "history = model.train(train_dataloader=train_loader, val_dataloader=val_loader)\n",
        "\n",
        "# Save training history\n",
        "experiment_manager.save_training_history(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ae637a1",
      "metadata": {
        "id": "4ae637a1"
      },
      "source": [
        "**Reasoning**:\n",
        "The `NameError` indicates that `HateClassifier` and `ExperimentManager` were not re-imported after the kernel restart. I need to explicitly import them before they can be used.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23c9f876",
      "metadata": {
        "id": "23c9f876"
      },
      "outputs": [],
      "source": [
        "from ExperimentManager import ExperimentManager\n",
        "from HateClassifier import HateClassifier\n",
        "from HateDataset import HateDataset\n",
        "\n",
        "# 1. CREATE EXPERIMENT\n",
        "experiment_manager = ExperimentManager(base_dir=\"./experiments\")\n",
        "experiment_dir = experiment_manager.create_experiment(\n",
        "    config=config,\n",
        "    custom_name=\"baseline_distilbert\",  # Change this for each experiment\n",
        "    description=\"Baseline model with distilbert-base-uncased, standard hyperparameters\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "transformersv2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
