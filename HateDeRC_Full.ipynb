{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jamesalv/HateDeRC/blob/master/HateDeRC_Full.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "24658442",
      "metadata": {
        "id": "24658442",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a934405b-c465-443b-eb15-913582aa80a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'HateDeRC'...\n",
            "remote: Enumerating objects: 79, done.\u001b[K\n",
            "remote: Counting objects: 100% (79/79), done.\u001b[K\n",
            "remote: Compressing objects: 100% (59/59), done.\u001b[K\n",
            "remote: Total 79 (delta 35), reused 53 (delta 18), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (79/79), 5.26 MiB | 5.31 MiB/s, done.\n",
            "Resolving deltas: 100% (35/35), done.\n",
            "/content/HateDeRC\n"
          ]
        }
      ],
      "source": [
        "import shutil\n",
        "import os\n",
        "\n",
        "# Delete HateDeRC directory if it exists\n",
        "if os.path.exists('HateDeRC'):\n",
        "  shutil.rmtree('HateDeRC')\n",
        "!git clone https://github.com/jamesalv/HateDeRC\n",
        "%cd HateDeRC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8bccce96",
      "metadata": {
        "id": "8bccce96"
      },
      "outputs": [],
      "source": [
        "from TrainingConfig import TrainingConfig\n",
        "from typing import Dict, Any, Tuple, List\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1f654143",
      "metadata": {
        "id": "1f654143"
      },
      "outputs": [],
      "source": [
        "data_path = 'Data/dataset.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b3618d6e",
      "metadata": {
        "id": "b3618d6e"
      },
      "outputs": [],
      "source": [
        "config = TrainingConfig()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "487ff027",
      "metadata": {
        "id": "487ff027"
      },
      "outputs": [],
      "source": [
        "# Seed all randomness for reproducibility\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(config.seed)\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.manual_seed_all(config.seed)\n",
        "np.random.seed(config.seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53d93027",
      "metadata": {
        "id": "53d93027"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f81645fa",
      "metadata": {
        "id": "f81645fa"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def deobfuscate_text(text):\n",
        "    \"\"\"\n",
        "    Normalize common text obfuscation patterns to reveal original words.\n",
        "    Useful for hate speech detection and content analysis.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text with potential obfuscations\n",
        "\n",
        "    Returns:\n",
        "        str: Text with obfuscations normalized\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "\n",
        "    # Make a copy to work with\n",
        "    result = text.lower()\n",
        "\n",
        "    # 1. Handle asterisk/symbol replacements\n",
        "    symbol_patterns = {\n",
        "        # Common profanity\n",
        "        r'f\\*+c?k': 'fuck',\n",
        "        r'f\\*+': 'fuck',\n",
        "        r's\\*+t': 'shit',\n",
        "        r'b\\*+ch': 'bitch',\n",
        "        r'a\\*+s': 'ass',\n",
        "        r'd\\*+n': 'damn',\n",
        "        r'h\\*+l': 'hell',\n",
        "        r'c\\*+p': 'crap',\n",
        "\n",
        "        # Slurs and hate speech terms (be comprehensive for detection)\n",
        "        r'n\\*+g+[aer]+': 'nigger',  # Various n-word obfuscations\n",
        "        r'f\\*+g+[ot]*': 'faggot',\n",
        "        r'r\\*+[dt]ard': 'retard',\n",
        "        r'sp\\*+c': 'spic',\n",
        "\n",
        "        # Other symbols\n",
        "        r'@ss': 'ass',\n",
        "        r'b@tch': 'bitch',\n",
        "        r'sh!t': 'shit',\n",
        "        r'f#ck': 'fuck',\n",
        "        r'd@mn': 'damn',\n",
        "    }\n",
        "\n",
        "    for pattern, replacement in symbol_patterns.items():\n",
        "        result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)\n",
        "\n",
        "    # 2. Handle character spacing (f u c k -> fuck)\n",
        "    spacing_patterns = {\n",
        "        r'\\bf\\s+u\\s+c\\s+k\\b': 'fuck',\n",
        "        r'\\bs\\s+h\\s+i\\s+t\\b': 'shit',\n",
        "        r'\\bd\\s+a\\s+m\\s+n\\b': 'damn',\n",
        "        r'\\bh\\s+e\\s+l\\s+l\\b': 'hell',\n",
        "        r'\\ba\\s+s\\s+s\\b': 'ass',\n",
        "        r'\\bc\\s+r\\s+a\\s+p\\b': 'crap',\n",
        "    }\n",
        "\n",
        "    for pattern, replacement in spacing_patterns.items():\n",
        "        result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)\n",
        "\n",
        "    # 3. Handle number/letter substitutions\n",
        "    leet_patterns = {\n",
        "        # Basic leet speak\n",
        "        r'\\b3\\s*1\\s*1\\s*3\\b': 'elle',  # 3113 -> elle\n",
        "        r'\\bf4g\\b': 'fag',\n",
        "        r'\\bf4gg0t\\b': 'faggot',\n",
        "        r'\\bn00b\\b': 'noob',\n",
        "        r'\\bl33t\\b': 'leet',\n",
        "        r'\\bh4t3\\b': 'hate',\n",
        "        r'\\b5h1t\\b': 'shit',\n",
        "        r'\\bf0ck\\b': 'fock',\n",
        "    }\n",
        "\n",
        "    for pattern, replacement in leet_patterns.items():\n",
        "        result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)\n",
        "\n",
        "    # 4. Handle repeated characters and separators\n",
        "    # Remove excessive punctuation between letters\n",
        "    result = re.sub(r'([a-z])[^\\w\\s]+([a-z])', r'\\1\\2', result)\n",
        "\n",
        "    # Handle underscore separation\n",
        "    result = re.sub(r'([a-z])_+([a-z])', r'\\1\\2', result)\n",
        "\n",
        "    # Handle dot separation\n",
        "    result = re.sub(r'([a-z])\\.+([a-z])', r'\\1\\2', result)\n",
        "\n",
        "    # 5. Handle common misspellings/variations used for evasion\n",
        "    evasion_patterns = {\n",
        "        r'\\bfuk\\b': 'fuck',\n",
        "        r'\\bfuq\\b': 'fuck',\n",
        "        r'\\bfck\\b': 'fuck',\n",
        "        r'\\bshyt\\b': 'shit',\n",
        "        r'\\bshit\\b': 'shit',\n",
        "        r'\\bbiatch\\b': 'bitch',\n",
        "        r'\\bbeatch\\b': 'bitch',\n",
        "        r'\\basshole\\b': 'asshole',\n",
        "        r'\\ba55hole\\b': 'asshole',\n",
        "        r'\\btard\\b': 'retard',\n",
        "        r'\\bfagg\\b': 'fag',\n",
        "    }\n",
        "\n",
        "    for pattern, replacement in evasion_patterns.items():\n",
        "        result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)\n",
        "\n",
        "    # 6. Clean up multiple spaces\n",
        "    result = re.sub(r'\\s+', ' ', result).strip()\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f070b12b",
      "metadata": {
        "id": "f070b12b"
      },
      "outputs": [],
      "source": [
        "def aggregate_rationales(rationales, labels, post_length, drop_abnormal=False):\n",
        "    \"\"\"\n",
        "    If all 3 annotators are normal → 3 zero spans → average (all zeros).\n",
        "    If k annotators are non-normal and k spans exist → average the k spans (no added zeros).\n",
        "    If k non-normal but fewer than k spans:\n",
        "        If the missing annotators are non-normal → do not fill with zeros; average only existing spans and record rationale_support = #spans.\n",
        "        If the missing annotators are normal (e.g., 2 hate + 1 normal + 2 spans) → append one zero span for the normal.\n",
        "    \"\"\"\n",
        "    count_normal = labels.count(0)\n",
        "    count_hate = labels.count(1)\n",
        "    count_rationales = len(rationales)\n",
        "    pad = np.zeros(post_length, dtype=\"int\").tolist()\n",
        "\n",
        "    # If there are hate labels but no rationales, something is wrong\n",
        "    if count_hate > 0 and count_rationales == 0:\n",
        "        if drop_abnormal:\n",
        "            return None\n",
        "\n",
        "        # Else just fill with 0\n",
        "        return np.zeros(post_length).tolist()\n",
        "\n",
        "    # If all annotators are normal, return all zeros\n",
        "    if count_normal == 3:\n",
        "        return np.zeros(post_length).tolist()\n",
        "\n",
        "    # If we have hate annotators\n",
        "    if count_hate > 0:\n",
        "        # Case 1: Number of rationales matches number of hate annotators\n",
        "        if count_rationales == count_hate:\n",
        "            return np.average(rationales, axis=0).tolist()\n",
        "\n",
        "        # Case 2: Fewer rationales than hate annotators\n",
        "        elif count_rationales < count_hate:\n",
        "            # Add zero padding for normal annotators only\n",
        "            rationales_copy = rationales.copy()\n",
        "            zeros_to_add = count_normal\n",
        "            for _ in range(zeros_to_add):\n",
        "                rationales_copy.append(pad)\n",
        "            return np.average(rationales_copy, axis=0).tolist()\n",
        "\n",
        "        # Case 3: More rationales than hate annotators (shouldn't happen normally)\n",
        "        else:\n",
        "            # Just average what we have\n",
        "            return np.average(rationales, axis=0).tolist()\n",
        "\n",
        "    # Fallback: return zeros if no clear case matches\n",
        "    return np.zeros(post_length).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8d446ddb",
      "metadata": {
        "id": "8d446ddb"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "def preprocess_text(raw_text):\n",
        "    preprocessed_text = raw_text\n",
        "    # # Remove HTML tags <>\n",
        "    preprocessed_text = preprocessed_text.replace(\"<\", \"\").replace(\">\", \"\")\n",
        "    # # De-Obsfucate Patterns\n",
        "    preprocessed_text = deobfuscate_text(preprocessed_text)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "\n",
        "def create_text_segment(\n",
        "    text_tokens: List[str], rationale_mask: List[int]\n",
        ") -> List[Tuple[List[str], int]]:\n",
        "    \"\"\"\n",
        "    Process a rationale mask to identify contiguous segments of highlighted text.\n",
        "    Then create a segmented representation of the tokens\n",
        "\n",
        "    Args:\n",
        "        text_tokens: Original text tokens\n",
        "        mask: Binary mask where 1 indicates a highlighted token (this consists of mask from 3 annotators)\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples (text segment, mask value)\n",
        "    \"\"\"\n",
        "    # Handle case where mask is empty (no rationale provided), usually this is normal classification\n",
        "    mask = rationale_mask\n",
        "\n",
        "    # for mask in all_rationale_mask:\n",
        "    # Find breakpoints (transitions between highlighted/1 and non-highlighted/0)\n",
        "    breakpoints = []\n",
        "    mask_values = []\n",
        "\n",
        "    # Always start with position 0\n",
        "    breakpoints.append(0)\n",
        "    mask_values.append(mask[0])\n",
        "\n",
        "    # Find transitions in the mask\n",
        "    for i in range(1, len(mask)):\n",
        "        if mask[i] != mask[i - 1]:\n",
        "            breakpoints.append(i)\n",
        "            mask_values.append(mask[i])\n",
        "\n",
        "    # Always end with the length of the text\n",
        "    if breakpoints[-1] != len(mask):\n",
        "        breakpoints.append(len(mask))\n",
        "\n",
        "    # Create segments based on breakpoints\n",
        "    segments = []\n",
        "    for i in range(len(breakpoints) - 1):\n",
        "        start = breakpoints[i]\n",
        "        end = breakpoints[i + 1]\n",
        "        segments.append((text_tokens[start:end], mask_values[i]))\n",
        "\n",
        "    return segments\n",
        "\n",
        "\n",
        "def align_rationales(tokens, rationales, tokenizer, max_length=128):\n",
        "    \"\"\"\n",
        "    Align rationales with tokenized text while handling different tokenizer formats.\n",
        "\n",
        "    Args:\n",
        "        tokens: Original text tokens\n",
        "        rationales: Original rationale masks\n",
        "        tokenizer: The tokenizer to use\n",
        "        max_length: Maximum sequence length\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with tokenized inputs and aligned rationale masks\n",
        "    \"\"\"\n",
        "    segments = create_text_segment(tokens, rationales)\n",
        "    all_human_rationales = []\n",
        "    all_input_ids = []\n",
        "    all_attention_mask = []\n",
        "    all_token_type_ids = []\n",
        "    all_rationales = []\n",
        "    for text_segment, rationale_value in segments:\n",
        "        inputs = {}\n",
        "        concatenated_text = \" \".join(text_segment)\n",
        "        processed_segment = preprocess_text(concatenated_text)\n",
        "        tokenized = tokenizer(\n",
        "            processed_segment, add_special_tokens=False, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Extract the relevant data\n",
        "        segment_input_ids = tokenized[\"input_ids\"][0]\n",
        "        segment_attention_mask = tokenized[\"attention_mask\"][0]\n",
        "        # Handle token_type_ids if present\n",
        "        if \"token_type_ids\" in tokenized:\n",
        "            segment_token_type_ids = tokenized[\"token_type_ids\"][0]\n",
        "            all_token_type_ids.extend(segment_token_type_ids)\n",
        "\n",
        "        # Add input IDs and attention mask\n",
        "        all_input_ids.extend(segment_input_ids)\n",
        "        all_attention_mask.extend(segment_attention_mask)\n",
        "\n",
        "        # Add rationales (excluding special tokens)\n",
        "        segment_rationales = [rationale_value] * len(segment_input_ids)\n",
        "        all_rationales.extend(segment_rationales)\n",
        "    # Get special token IDs\n",
        "    cls_token_id = tokenizer.cls_token_id\n",
        "    sep_token_id = tokenizer.sep_token_id\n",
        "\n",
        "    # Add special tokens at the beginning and end\n",
        "    all_input_ids = [cls_token_id] + all_input_ids + [sep_token_id]\n",
        "    all_attention_mask = [1] + all_attention_mask + [1]\n",
        "\n",
        "    # Handle token_type_ids if the model requires it\n",
        "    if hasattr(tokenizer, \"create_token_type_ids_from_sequences\"):\n",
        "        all_token_type_ids = tokenizer.create_token_type_ids_from_sequences(\n",
        "            all_input_ids[1:-1]\n",
        "        )\n",
        "    elif all_token_type_ids:\n",
        "        all_token_type_ids = [0] + all_token_type_ids + [0]\n",
        "    else:\n",
        "        all_token_type_ids = [0] * len(all_input_ids)\n",
        "\n",
        "    # Check tokenized vs rationales length\n",
        "    if len(all_input_ids) != len(all_attention_mask):\n",
        "        print(\"Warning: length of tokens and rationales do not match\")\n",
        "\n",
        "    # Add zero rationale values for special tokens\n",
        "    all_rationales = [0] + all_rationales + [0]\n",
        "\n",
        "    # Truncate to max length if needed\n",
        "    if len(all_input_ids) > max_length:\n",
        "        print(\"WARNING: NEED TO TRUNCATE\")\n",
        "        all_input_ids = all_input_ids[:max_length]\n",
        "        all_attention_mask = all_attention_mask[:max_length]\n",
        "        all_token_type_ids = all_token_type_ids[:max_length]\n",
        "        all_rationales = all_rationales[:max_length]\n",
        "\n",
        "    # Pad to max_length if needed\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "    padding_length = max_length - len(all_input_ids)\n",
        "\n",
        "    if padding_length > 0:\n",
        "        all_input_ids = all_input_ids + [pad_token_id] * padding_length\n",
        "        all_attention_mask = all_attention_mask + [0] * padding_length\n",
        "        all_token_type_ids = all_token_type_ids + [0] * padding_length\n",
        "        all_rationales = all_rationales + [0] * padding_length\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    inputs = {\n",
        "        \"input_ids\": torch.tensor([all_input_ids], dtype=torch.long),\n",
        "        \"attention_mask\": torch.tensor([all_attention_mask], dtype=torch.long),\n",
        "        \"token_type_ids\": (\n",
        "            torch.tensor([all_token_type_ids], dtype=torch.long)\n",
        "            if \"token_type_ids\" in tokenizer.model_input_names\n",
        "            else None\n",
        "        ),\n",
        "        \"rationales\": torch.tensor([all_rationales], dtype=torch.float32),\n",
        "    }\n",
        "\n",
        "    # Remove None values\n",
        "    inputs = {k: v for k, v in inputs.items() if v is not None}\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "pEC9l6Mr6xo0",
      "metadata": {
        "id": "pEC9l6Mr6xo0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import os\n",
        "import string\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import more_itertools as mit\n",
        "\n",
        "def find_ranges(iterable):\n",
        "    \"\"\"Yield range of consecutive numbers.\"\"\"\n",
        "    for group in mit.consecutive_groups(iterable):\n",
        "        group = list(group)\n",
        "        if len(group) == 1:\n",
        "            yield group[0]\n",
        "        else:\n",
        "            yield group[0], group[-1]\n",
        "\n",
        "def process_and_convert_data(data, tokenizer, post_id_divisions, save_path='Data/explanations/', drop_abnormal=False):\n",
        "    \"\"\"\n",
        "    Combined function that processes raw entries and converts to ERASER format in one pass.\n",
        "    Also splits data into train/val/test sets.\n",
        "    \"\"\"\n",
        "    print(\"Processing and converting data...\")\n",
        "\n",
        "    # Initialize outputs\n",
        "    train_data = []\n",
        "    val_data = []\n",
        "    test_data = []\n",
        "    dropped = 0\n",
        "\n",
        "    # Create directories if saving splits\n",
        "    if save_path:\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        os.makedirs(os.path.join(save_path, 'docs'), exist_ok=True)\n",
        "        train_fp = open(os.path.join(save_path, 'train.jsonl'), 'w')\n",
        "        val_fp = open(os.path.join(save_path, 'val.jsonl'), 'w')\n",
        "        test_fp = open(os.path.join(save_path, 'test.jsonl'), 'w')\n",
        "\n",
        "    for key, value in tqdm(data.items()):\n",
        "        try:\n",
        "            # Extract labels\n",
        "            labels = [1 if annot[\"label\"] in ['hatespeech', 'offensive'] else 0\n",
        "                     for annot in value[\"annotators\"]]\n",
        "\n",
        "            # Process rationales\n",
        "            rationales = value.get(\"rationales\", [])\n",
        "            aggregated_rationale = aggregate_rationales(\n",
        "                rationales, labels, len(value[\"post_tokens\"]), drop_abnormal=drop_abnormal\n",
        "            )\n",
        "\n",
        "            if aggregated_rationale is None:\n",
        "                dropped += 1\n",
        "                continue\n",
        "\n",
        "            inputs = align_rationales(value['post_tokens'], aggregated_rationale, tokenizer)\n",
        "\n",
        "            # Calculate labels\n",
        "            hard_label = Counter(labels).most_common(1)[0][0]\n",
        "            soft_label = sum(labels) / len(labels)\n",
        "\n",
        "            # Determine target groups (mentioned at least 3 times)\n",
        "            target_groups = [t for annot in value['annotators'] for t in annot['target']]\n",
        "            filtered_targets = [k for k, v in Counter(target_groups).items() if v > 2]\n",
        "\n",
        "            # Create processed entry\n",
        "            processed_entry = {\n",
        "                'post_id': key,\n",
        "                'input_ids': inputs['input_ids'],\n",
        "                'attention_mask': inputs['attention_mask'],\n",
        "                'rationales': inputs['rationales'],\n",
        "                'raw_text': \" \".join(value['post_tokens']),\n",
        "                'hard_label': hard_label,\n",
        "                'soft_label': soft_label,\n",
        "                'target_groups': filtered_targets\n",
        "            }\n",
        "\n",
        "            # Convert to ERASER format if it's hateful/offensive content\n",
        "            if hard_label == 1 and save_path:\n",
        "                input_ids_list = inputs['input_ids'].squeeze().tolist()\n",
        "                rationales_list = inputs['rationales'].squeeze().ceil().int().tolist()\n",
        "\n",
        "                # Build evidences\n",
        "                evidences = []\n",
        "                indexes = sorted([i for i, each in enumerate(rationales_list) if each == 1])\n",
        "                for span in find_ranges(indexes):\n",
        "                    if isinstance(span, int):\n",
        "                        start, end = span, span + 1\n",
        "                    else:\n",
        "                        start, end = span[0], span[1] + 1\n",
        "\n",
        "                    evidences.append({\n",
        "                        \"docid\": key,\n",
        "                        \"end_sentence\": -1,\n",
        "                        \"end_token\": end,\n",
        "                        \"start_sentence\": -1,\n",
        "                        \"start_token\": start,\n",
        "                        \"text\": ' '.join([str(x) for x in input_ids_list[start:end]])\n",
        "                    })\n",
        "\n",
        "                eraser_entry = {\n",
        "                    'annotation_id': key,\n",
        "                    'classification': str(hard_label),\n",
        "                    'evidences': [evidences],\n",
        "                    'query': \"What is the class?\",\n",
        "                    'query_type': None\n",
        "                }\n",
        "\n",
        "                # Save document\n",
        "                with open(os.path.join(save_path, 'docs', key), 'w') as fp:\n",
        "                    fp.write(' '.join([str(x) for x in input_ids_list if x > 0]))\n",
        "\n",
        "                # Write to appropriate split\n",
        "                if key in post_id_divisions['train']:\n",
        "                    train_fp.write(json.dumps(eraser_entry) + '\\n')\n",
        "                elif key in post_id_divisions['val']:\n",
        "                    val_fp.write(json.dumps(eraser_entry) + '\\n')\n",
        "                elif key in post_id_divisions['test']:\n",
        "                    test_fp.write(json.dumps(eraser_entry) + '\\n')\n",
        "\n",
        "            # Add to appropriate split list\n",
        "            if key in post_id_divisions['train']:\n",
        "                train_data.append(processed_entry)\n",
        "            elif key in post_id_divisions['val']:\n",
        "                val_data.append(processed_entry)\n",
        "            elif key in post_id_divisions['test']:\n",
        "                test_data.append(processed_entry)\n",
        "\n",
        "        except Exception as e:\n",
        "            dropped += 1\n",
        "            print(f\"Error processing {key}: {e}\")\n",
        "\n",
        "    if save_path:\n",
        "        train_fp.close()\n",
        "        val_fp.close()\n",
        "        test_fp.close()\n",
        "\n",
        "    print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}, Dropped: {dropped}\")\n",
        "\n",
        "    return {\n",
        "        'train': train_data,\n",
        "        'val': val_data,\n",
        "        'test': test_data\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "43NMFuwv62Q4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366,
          "referenced_widgets": [
            "4a2cdb2e0469408482f033c2b54c5285",
            "c5b5ec5c9729431b95b09ac9deb639a4",
            "921ac27b2716401d9c30a91f754e6efd",
            "ea6db4218aed4bad847cec4888f0bc73",
            "fd28242321e3401b99d6dc7673e49a34",
            "601fc433cef044028339febadaeabb4c",
            "8164902fa691445e9c9a1fa3c1dc3267",
            "56f946e5499b4930989d2a1ccd6322c6",
            "90b749f0d1e64a70b6dca9a2dff56c13",
            "508d38530fdb44839b819d00f40fe211",
            "d57aecc53ab14aa1ac0e8547fa5d2d61",
            "e5e43f25fd86444da9e6196d5c4c3739",
            "74b0d3bba1e442de86fda672e2afadc0",
            "d6bd5c3f917d4e3b94d884297a9cc7ad",
            "fe5e4e766b5f440696ebe6df00923b37",
            "9024cb8d3ad04a1f827babf2f1f1781c",
            "685a9314a1c64e87be54e2be16359df3",
            "d2b0cab60520428c956ea6cd5cf2ec44",
            "33405549d3e44410ae9c8379d044c861",
            "1659625325fb4872a32c178d885fbb96",
            "477617c871bb439ca17e7049b31cc9ab",
            "1a0aab2938dd42649d3187bd56793f19",
            "f11a71db483040dc9edc3475df3c53ba",
            "3f7c61c1dee448f68c627d7045c10b4b",
            "f7c6ed35073b45458d1b3608cea6e06d",
            "0be4123404e84c9bb16a5af9bbe51d76",
            "c53a8778b7ba4a249db56f3ad622d5d2",
            "6392708170254b42a7368c7ab93f2f1c",
            "41a18afb2eb54b948eec81126dcc7819",
            "4bf7a8d576a140beb9a37f0b0c869993",
            "2a49b41cec2f4c048ac964867ef0dd44",
            "37f66a80f6aa4d1cb9ef7610901280bc",
            "02355d6d66ed43518446dca35994f50e",
            "1cd9f68e106b4dadb8661d6d40afa91f",
            "ce56b1483d0a4468a6672b75d1f85a94",
            "69c758818c234a7ab75fc366e491bfde",
            "5eaa094f2b77421fb6ac0ac376f0f287",
            "ba63e6c02a414bf1997533e5bf420643",
            "5c9bfea25f5b499db480b0681cb2d276",
            "fe4e916f0c614c26bed6d9d828a9f508",
            "fd91e17ea8b24b0fb52b8be5b4ee2ba7",
            "6abdc665a9a048b1b93f9a2d1a542ebc",
            "189ae264855344f2a53590d3a166e3f1",
            "98b55b2706cf4d56b22e504aa3917444"
          ]
        },
        "id": "43NMFuwv62Q4",
        "outputId": "dfd88ec3-ef78-4b38-eda5-85c13f6a9733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a2cdb2e0469408482f033c2b54c5285"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5e43f25fd86444da9e6196d5c4c3739"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f11a71db483040dc9edc3475df3c53ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1cd9f68e106b4dadb8661d6d40afa91f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing and converting data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 36%|███▋      | 7338/20148 [00:14<00:22, 575.10it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: NEED TO TRUNCATE\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 89%|████████▉ | 17966/20148 [00:36<00:04, 486.56it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing 24439295_gab: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 20148/20148 [00:41<00:00, 491.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: 15382, Val: 1922, Test: 1924, Dropped: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "with open(data_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "with open('Data/post_id_divisions.json') as file:\n",
        "    post_id_divisions = json.load(file)\n",
        "\n",
        "# Process everything in one pass\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "splits = process_and_convert_data(\n",
        "    data=data,\n",
        "    tokenizer=tokenizer,\n",
        "    post_id_divisions=post_id_divisions,\n",
        "    save_path='Data/explanations/',\n",
        "    drop_abnormal=False\n",
        ")\n",
        "\n",
        "# Access splits directly\n",
        "train_data = splits['train']\n",
        "val_data = splits['val']\n",
        "test_data = splits['test']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data[3]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lyMHoD2_c0Rw",
        "outputId": "9a469c0f-6e63-40a2-e360-e33e0e619f01"
      },
      "id": "lyMHoD2_c0Rw",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'post_id': '24198545_gab',\n",
              " 'input_ids': tensor([[  101,  1998,  2023,  2003,  2339,  1045,  2203,  2039,  2007,  9152,\n",
              "          13327, 26758,  7435,  2040,  2064,  2025,  3713,  7919,  3768,  3937,\n",
              "           3716,  1997,  7366,  2009,  5621, 12459,  2065,  1996,  2270,  2069,\n",
              "           2354,   102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0]]),\n",
              " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
              " 'rationales': tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          1.0000, 1.0000, 0.3333, 0.0000, 0.3333, 0.3333, 0.3333, 0.3333, 0.3333,\n",
              "          0.3333, 0.3333, 0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
              "          0.0000, 0.0000]]),\n",
              " 'raw_text': 'and this is why i end up with nigger trainee doctors who can not speak properly lack basic knowledge of biology it truly scary if the public only knew',\n",
              " 'hard_label': 1,\n",
              " 'soft_label': 1.0,\n",
              " 'target_groups': ['African']}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "HJjDXt3QGbrQ"
      },
      "id": "HJjDXt3QGbrQ"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "87c63ecf",
      "metadata": {
        "id": "87c63ecf"
      },
      "outputs": [],
      "source": [
        "from HateDataset import HateDataset\n",
        "\n",
        "# Create datasets with pre-tokenized data\n",
        "train_dataset = HateDataset(data=train_data)\n",
        "val_dataset = HateDataset(data=val_data)\n",
        "test_dataset = HateDataset(data=test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f4887f43",
      "metadata": {
        "id": "f4887f43"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False) # Use shuffle=False for validation\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False) # Use shuffle=False for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "2f509bc0",
      "metadata": {
        "id": "2f509bc0"
      },
      "outputs": [],
      "source": [
        "# from HateClassifier import HateClassifier\n",
        "# model = HateClassifier(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "aab913f8",
      "metadata": {
        "id": "aab913f8"
      },
      "outputs": [],
      "source": [
        "# history = model.train(train_dataloader=train_loader, val_dataloader=val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4103f12a",
      "metadata": {
        "id": "4103f12a"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c2d21fc2",
      "metadata": {
        "id": "c2d21fc2"
      },
      "outputs": [],
      "source": [
        "# import pickle\n",
        "\n",
        "# result = model.predict(test_dataloader=test_loader, return_attentions=True)\n",
        "\n",
        "# # Save the result to a file\n",
        "# with open('prediction_results.pkl', 'wb') as f:\n",
        "#   pickle.dump(result, f)\n",
        "\n",
        "# print(\"Results saved to prediction_results.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0caf972c",
      "metadata": {
        "id": "0caf972c"
      },
      "source": [
        "## Bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "40280671",
      "metadata": {
        "id": "40280671"
      },
      "outputs": [],
      "source": [
        "def get_bias_evaluation_samples(data, method, group):\n",
        "    \"\"\"\n",
        "    Get positive and negative sample IDs for bias evaluation based on method and group\n",
        "\n",
        "    Args:\n",
        "        data: list of data entries\n",
        "        method: Bias evaluation method ('subgroup', 'bpsn', or 'bnsp')\n",
        "        group: Target group to evaluate\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (positive_ids, negative_ids)\n",
        "    \"\"\"\n",
        "    positive_ids = []\n",
        "    negative_ids = []\n",
        "\n",
        "    for idx, row in enumerate(data):\n",
        "        target_groups = row['target_groups']\n",
        "        if target_groups is None:\n",
        "            continue\n",
        "\n",
        "        is_in_group = group in target_groups\n",
        "\n",
        "        # Convert various label formats to binary toxic/non-toxic\n",
        "        if 'hard_label' in row:\n",
        "            is_toxic = row['hard_label'] == 1\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        if method == 'subgroup':\n",
        "            # Only consider samples mentioning the group\n",
        "            if is_in_group:\n",
        "                if is_toxic:\n",
        "                    positive_ids.append(idx)\n",
        "                else:\n",
        "                    negative_ids.append(idx)\n",
        "\n",
        "        elif method == 'bpsn':\n",
        "            # Compare non-toxic posts mentioning the group with toxic posts NOT mentioning the group\n",
        "            if is_in_group and not is_toxic:\n",
        "                negative_ids.append(idx)\n",
        "            elif not is_in_group and is_toxic:\n",
        "                positive_ids.append(idx)\n",
        "\n",
        "        elif method == 'bnsp':\n",
        "            # Compare toxic posts mentioning the group with non-toxic posts NOT mentioning the group\n",
        "            if is_in_group and is_toxic:\n",
        "                positive_ids.append(idx)\n",
        "            elif not is_in_group and not is_toxic:\n",
        "                negative_ids.append(idx)\n",
        "\n",
        "    return positive_ids, negative_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "79edfa7d",
      "metadata": {
        "id": "79edfa7d"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def calculate_gmb_metrics(\n",
        "    test_data: List[Dict[str, Any]],\n",
        "    probabilities: np.ndarray,\n",
        "    target_groups: List[str]\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculate GMB (Generalized Mean of Bias) AUC metrics from model predictions\n",
        "\n",
        "    Args:\n",
        "        probabilities: Model's probability outputs\n",
        "        test_data: List of test data entries\n",
        "        target_groups: List of target groups to evaluate\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with GMB metrics\n",
        "    \"\"\"\n",
        "    # Create mappings from post_id to predictions and ground truth\n",
        "    prediction_scores = defaultdict(lambda: defaultdict(dict))\n",
        "    ground_truth = {}\n",
        "\n",
        "    for idx, row in enumerate(test_data):\n",
        "        prediction_scores[idx] = probabilities[idx, 1]\n",
        "        ground_truth[idx] = row['hard_label']\n",
        "\n",
        "    # Calculate metrics for each target group and method\n",
        "    bias_metrics = {}\n",
        "    methods = ['subgroup', 'bpsn', 'bnsp']\n",
        "\n",
        "    for method in methods:\n",
        "        bias_metrics[method] = {}  # Initialize nested dictionary for each method\n",
        "        for group in target_groups:\n",
        "            # Get positive and negative samples based on the method\n",
        "            positive_ids, negative_ids = get_bias_evaluation_samples(test_data, method, group)\n",
        "\n",
        "            if len(positive_ids) == 0 or len(negative_ids) == 0:\n",
        "                print(f\"Skipping {method} for group {group}: no samples found\")\n",
        "                continue  # Skip if no samples for this group/method\n",
        "\n",
        "            # Collect ground truth and predictions\n",
        "            y_true = []\n",
        "            y_score = []\n",
        "\n",
        "            for post_id in positive_ids:\n",
        "                if post_id in ground_truth and post_id in prediction_scores:\n",
        "                    y_true.append(ground_truth[post_id])\n",
        "                    y_score.append(prediction_scores[post_id])\n",
        "\n",
        "            for post_id in negative_ids:\n",
        "                if post_id in ground_truth and post_id in prediction_scores:\n",
        "                    y_true.append(ground_truth[post_id])\n",
        "                    y_score.append(prediction_scores[post_id])\n",
        "\n",
        "            # Calculate AUC if we have enough samples with both classes\n",
        "            if len(y_true) > 10 and len(set(y_true)) > 1:\n",
        "                try:\n",
        "                    auc = roc_auc_score(y_true, y_score)\n",
        "                    bias_metrics[method][group] = auc\n",
        "                except ValueError:\n",
        "                    print(f\"Could not compute AUC for {method} and group {group} due to ValueError\")\n",
        "                    pass\n",
        "\n",
        "    # Calculate GMB for each method\n",
        "    gmb_metrics = {}\n",
        "    power = -5  # Power parameter for generalized mean\n",
        "\n",
        "    for method in methods:\n",
        "        if not bias_metrics[method]:\n",
        "            continue\n",
        "\n",
        "        scores = list(bias_metrics[method].values())\n",
        "        if not scores:\n",
        "            continue\n",
        "\n",
        "        # Calculate generalized mean with p=-5\n",
        "        power_mean = np.mean([score ** power for score in scores]) ** (1/power)\n",
        "        gmb_metrics[f'GMB-{method.upper()}-AUC'] = power_mean\n",
        "\n",
        "    # Calculate a combined GMB score that includes all methods\n",
        "    all_scores = []\n",
        "    for method in methods:\n",
        "        all_scores.extend(list(bias_metrics[method].values()))\n",
        "\n",
        "    if all_scores:\n",
        "        gmb_metrics['GMB-COMBINED-AUC'] = np.mean([score ** power for score in all_scores]) ** (1/power)\n",
        "\n",
        "    return gmb_metrics, bias_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ys17p1JQ_IUD",
      "metadata": {
        "id": "ys17p1JQ_IUD"
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "\n",
        "all_target_groups = list(chain.from_iterable(d['target_groups'] for d in train_data + val_data + test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "d156cab7",
      "metadata": {
        "id": "d156cab7"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "# Get top 10 most common target groups in the full dataset\n",
        "\n",
        "# Remove None\n",
        "all_target_groups = [group for group in all_target_groups if group != 'None' and group != 'Other']\n",
        "counter = Counter(all_target_groups)\n",
        "\n",
        "n_common = 10\n",
        "bias_target_groups = [tg[0] for tg in counter.most_common(n_common)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "ffc9f3e0",
      "metadata": {
        "id": "ffc9f3e0"
      },
      "outputs": [],
      "source": [
        "# gmb_metrics, bias_details = calculate_gmb_metrics(\n",
        "#   test_data=test_data,\n",
        "#   probabilities=result['probabilities'],\n",
        "#   target_groups=bias_target_groups\n",
        "# )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "9c02645d",
      "metadata": {
        "id": "9c02645d"
      },
      "outputs": [],
      "source": [
        "# print('GMB-Metrics')\n",
        "# for key, value in gmb_metrics.items():\n",
        "#   print(f'{key}: {value}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "814a2279",
      "metadata": {
        "id": "814a2279"
      },
      "outputs": [],
      "source": [
        "# print('Bias Details')\n",
        "# print()\n",
        "# for key, entry in bias_details.items():\n",
        "#   print(f\"Metrics: {key}\")\n",
        "#   for subgroup, value in entry.items():\n",
        "#     print(f'{subgroup}: {value}')\n",
        "#   print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39b1ac3f",
      "metadata": {
        "id": "39b1ac3f"
      },
      "source": [
        "## XAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "8toApSDiIVlT",
      "metadata": {
        "id": "8toApSDiIVlT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import List, Dict, Tuple\n",
        "import json\n",
        "from sklearn.metrics import (\n",
        "    precision_recall_curve,\n",
        "    auc,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "\n",
        "\n",
        "class FaithfulnessMetrics:\n",
        "    \"\"\"\n",
        "    Compute faithfulness metrics using the model's existing predict() method.\n",
        "    Creates modified datasets and uses DataLoader for efficient batched processing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer, dataset_class, batch_size=32):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.dataset_class = dataset_class\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Get special token IDs\n",
        "        self.special_token_ids = {\n",
        "            tokenizer.cls_token_id,\n",
        "            tokenizer.sep_token_id,\n",
        "        }\n",
        "        self.special_token_ids = {x for x in self.special_token_ids if x is not None}\n",
        "\n",
        "    def compute_all_metrics(\n",
        "        self,\n",
        "        test_data: List[Dict],  # Your original test data\n",
        "        test_results: Dict,  # Results from prediction\n",
        "        k: int = 5,  # Number of top tokens to consider\n",
        "        eraser_save_path: str = \"Data/eraser_formatted_results.jsonl\",\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Compute all ERASER metrics efficiently using DataLoader approach\n",
        "\n",
        "        Args:\n",
        "            test_data: List of test instances (each with input_ids, attention_mask, rationales, labels)\n",
        "            test_results: List of dictionaries containing attention scores for each instance\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with all metrics\n",
        "        \"\"\"\n",
        "        print(\"Computing ERASER metrics using DataLoader approach...\")\n",
        "\n",
        "        # Extract lists for easier processing\n",
        "        input_ids_list = [item[\"input_ids\"] for item in test_data]\n",
        "        attention_masks_list = [item[\"attention_mask\"] for item in test_data]\n",
        "        human_rationales = [item[\"rationales\"] for item in test_data]\n",
        "        attention_scores = [item for item in test_results[\"attentions\"]]\n",
        "\n",
        "        # 1. Extract top-k as hard predictions\n",
        "        hard_predictions = self._extract_top_k_tokens(\n",
        "            attention_scores, attention_masks_list, input_ids_list, k\n",
        "        )\n",
        "\n",
        "        hard_rationale_predictions, soft_rationale_predictions = self._convert_attention_to_evidence_format(input_ids_list, attention_scores, hard_predictions)\n",
        "\n",
        "        # 2. PLAUSIBILITY METRICS\n",
        "        print(\"\\n[1/3] Computing plausibility metrics...\")\n",
        "        auprc = self._compute_auprc(\n",
        "            attention_scores, human_rationales, attention_masks_list, input_ids_list\n",
        "        )\n",
        "        token_f1, token_prec, token_rec = self._compute_token_f1(\n",
        "            hard_predictions, human_rationales, attention_masks_list\n",
        "        )\n",
        "\n",
        "        # 3. FAITHFULNESS METRICS\n",
        "        print(\"[2/3] Computing comprehensiveness scores...\")\n",
        "        raw_comprehensiveness, comprehensiveness_scores = (\n",
        "            self._compute_comprehensiveness(test_data, test_results, hard_predictions)\n",
        "        )\n",
        "\n",
        "        print(\"[3/3] Computing sufficiency scores...\")\n",
        "        raw_sufficiency, sufficiency_scores = self._compute_sufficiency(\n",
        "            test_data, test_results, hard_predictions\n",
        "        )\n",
        "\n",
        "        # 4. Convert to eraser format\n",
        "        results_eraser = self._convert_result_to_eraser_format(test_results, hard_rationale_predictions, soft_rationale_predictions, raw_sufficiency, raw_comprehensiveness)\n",
        "        # Convert to JSONL format\n",
        "        jsonl_output = '\\n'.join([json.dumps(entry) for entry in results_eraser])\n",
        "        with open(eraser_save_path, 'w') as f:\n",
        "            f.write(jsonl_output)\n",
        "\n",
        "        return {\n",
        "            # Plausibility\n",
        "            \"auprc\": auprc,\n",
        "            \"token_f1\": token_f1,\n",
        "            \"token_precision\": token_prec,\n",
        "            \"token_recall\": token_rec,\n",
        "            # Faithfulness\n",
        "            \"comprehensiveness\": float(np.mean(comprehensiveness_scores)),\n",
        "            \"sufficiency\": float(np.mean(sufficiency_scores)),\n",
        "            # Additional\n",
        "            \"avg_rationale_length\": k,\n",
        "        }\n",
        "\n",
        "    def _convert_attention_to_evidence_format(self, input_ids_list, attention_scores, hard_predictions):\n",
        "        # 2. Collect evidence\n",
        "        hard_rationale_predictions = []\n",
        "        for idx, hp in enumerate(hard_predictions):\n",
        "            evidences = []\n",
        "            indexes = sorted([i for i, each in enumerate(hp.tolist()) if each == 1])\n",
        "            for span in find_ranges(indexes):\n",
        "                if isinstance(span, int):\n",
        "                    start, end = span, span + 1\n",
        "                else:\n",
        "                    start, end = span[0], span[1] + 1\n",
        "\n",
        "                evidences.append({\n",
        "                    \"start_token\": start,\n",
        "                    \"end_token\": end,\n",
        "                })\n",
        "            hard_rationale_predictions.append(evidences)\n",
        "\n",
        "        soft_rationale_predictions = []\n",
        "        for att in attention_scores:\n",
        "            pred = [x for x in att if x > 0]\n",
        "            soft_rationale_predictions.append(pred)\n",
        "\n",
        "        return hard_rationale_predictions, soft_rationale_predictions\n",
        "\n",
        "    def _convert_result_to_eraser_format(\n",
        "        self,\n",
        "        test_result: Dict,\n",
        "        hard_rationale_predictions,\n",
        "        soft_rationale_predictions,\n",
        "        sufficiency_scores: np.ndarray,\n",
        "        comprehensiveness_scores: np.ndarray,\n",
        "    ):\n",
        "        all_entries = []\n",
        "        for idx, data in enumerate(test_result[\"post_id\"]):\n",
        "            entry = {\n",
        "            'annotation_id': data,\n",
        "            'classification': str(int(test_result[\"predictions\"][idx])),\n",
        "            'classification_scores': {\n",
        "                0: float(test_result[\"probabilities\"][idx][0]),\n",
        "                1: float(test_result[\"probabilities\"][idx][1]),\n",
        "            },\n",
        "            'rationales': [\n",
        "                {\n",
        "                    \"docid\": data,\n",
        "                    \"hard_rationale_predictions\": hard_rationale_predictions[idx],\n",
        "                    \"soft_rationale_predictions\": [float(x) for x in soft_rationale_predictions[idx]],\n",
        "                }\n",
        "            ],\n",
        "            'sufficiency_classification_scores': {\n",
        "                0: float(sufficiency_scores[idx][0]),\n",
        "                1: float(sufficiency_scores[idx][1])\n",
        "            },\n",
        "            'comprehensiveness_classification_scores': {\n",
        "                0: float(comprehensiveness_scores[idx][0]),\n",
        "                1: float(comprehensiveness_scores[idx][1])\n",
        "            }\n",
        "            }\n",
        "            all_entries.append(entry)\n",
        "\n",
        "        return all_entries\n",
        "\n",
        "    def _calculate_average_rationale_length(\n",
        "        self,\n",
        "        human_rationales: List[torch.Tensor],\n",
        "        attention_masks_list: List[torch.Tensor],\n",
        "        input_ids_list: List[torch.Tensor],\n",
        "    ) -> int:\n",
        "        \"\"\"Calculate average number of content rationale tokens\"\"\"\n",
        "        lengths = []\n",
        "        for idx, (rat, mask) in enumerate(zip(human_rationales, attention_masks_list)):\n",
        "            valid_positions = mask.bool().cpu().numpy().flatten()\n",
        "\n",
        "            # Exclude special tokens\n",
        "            input_ids = input_ids_list[idx].cpu().numpy().flatten()\n",
        "            is_special = np.isin(input_ids, list(self.special_token_ids))\n",
        "            content_positions = valid_positions & ~is_special\n",
        "\n",
        "            rat_count = (rat.cpu().numpy().flatten()[content_positions] == 1).sum()\n",
        "            lengths.append(rat_count)\n",
        "\n",
        "        return max(1, int(np.mean(lengths)))\n",
        "\n",
        "    def _extract_top_k_tokens(\n",
        "        self,\n",
        "        attention_scores: List[np.ndarray],\n",
        "        attention_masks_list: List[torch.Tensor],\n",
        "        input_ids_list: List[torch.Tensor],\n",
        "        k: int,\n",
        "    ) -> List[np.ndarray]:\n",
        "        \"\"\"Extract top-k content tokens as hard predictions\"\"\"\n",
        "        hard_predictions = []\n",
        "\n",
        "        for idx, (attn, mask) in enumerate(zip(attention_scores, attention_masks_list)):\n",
        "            pred_mask = np.zeros_like(attn, dtype=int)\n",
        "            valid_positions = mask.bool().cpu().numpy().flatten()\n",
        "\n",
        "            # Exclude special tokens\n",
        "            input_ids = input_ids_list[idx].cpu().numpy().flatten()\n",
        "            is_special = np.isin(input_ids, list(self.special_token_ids))\n",
        "            content_positions = valid_positions & ~is_special\n",
        "\n",
        "            content_attn = attn[content_positions]\n",
        "\n",
        "            if k > 0 and len(content_attn) > 0:\n",
        "                k_actual = min(k, len(content_attn))\n",
        "                top_k_within_content = np.argsort(content_attn)[-k_actual:]\n",
        "                content_indices = np.where(content_positions)[0]\n",
        "                top_k_indices = content_indices[top_k_within_content]\n",
        "                pred_mask[top_k_indices] = 1\n",
        "\n",
        "            hard_predictions.append(pred_mask)\n",
        "\n",
        "        return hard_predictions\n",
        "\n",
        "    def _compute_auprc(\n",
        "        self,\n",
        "        attention_scores: List[np.ndarray],\n",
        "        human_rationales: List[torch.Tensor],\n",
        "        attention_masks_list: List[torch.Tensor],\n",
        "        input_ids_list: List[torch.Tensor],\n",
        "    ) -> float:\n",
        "        \"\"\"Compute AUPRC for soft attention scores\"\"\"\n",
        "        all_scores = []\n",
        "        all_labels = []\n",
        "\n",
        "        for idx, (attn, rat, mask) in enumerate(\n",
        "            zip(attention_scores, human_rationales, attention_masks_list)\n",
        "        ):\n",
        "            valid_positions = mask.bool().cpu().numpy().flatten()\n",
        "\n",
        "            # Exclude special tokens\n",
        "            input_ids = input_ids_list[idx].cpu().numpy().flatten()\n",
        "            is_special = np.isin(input_ids, list(self.special_token_ids))\n",
        "            content_positions = valid_positions & ~is_special\n",
        "\n",
        "            all_scores.extend(attn[content_positions].tolist())\n",
        "            all_labels.extend(\n",
        "                rat.cpu().numpy().flatten()[content_positions].astype(int).tolist()\n",
        "            )\n",
        "\n",
        "        all_scores = np.array(all_scores, dtype=float)\n",
        "        all_labels = np.array(all_labels, dtype=int)\n",
        "\n",
        "        if len(np.unique(all_labels)) < 2:\n",
        "            print(f\"Warning: Only one class in labels: {np.unique(all_labels)}\")\n",
        "            return 0.0\n",
        "\n",
        "        precision, recall, _ = precision_recall_curve(all_labels, all_scores)\n",
        "        return auc(recall, precision)\n",
        "\n",
        "    def _compute_token_f1(\n",
        "        self,\n",
        "        hard_predictions: List[np.ndarray],\n",
        "        human_rationales: List[torch.Tensor],\n",
        "        attention_masks_list: List[torch.Tensor],\n",
        "    ) -> Tuple[float, float, float]:\n",
        "        \"\"\"Compute token-level F1, Precision, Recall\"\"\"\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        for pred, rat, mask in zip(\n",
        "            hard_predictions, human_rationales, attention_masks_list\n",
        "        ):\n",
        "            valid_positions = mask.bool().cpu().numpy().flatten()\n",
        "            all_preds.extend(pred[valid_positions].astype(int).tolist())\n",
        "            all_labels.extend(\n",
        "                rat.cpu().numpy().flatten()[valid_positions].astype(int).tolist()\n",
        "            )\n",
        "\n",
        "        all_preds = np.array(all_preds, dtype=int)\n",
        "        all_labels = np.array(all_labels, dtype=int)\n",
        "\n",
        "        f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
        "        precision = precision_score(all_labels, all_preds, zero_division=0)\n",
        "        recall = recall_score(all_labels, all_preds, zero_division=0)\n",
        "\n",
        "        return f1, precision, recall\n",
        "\n",
        "    def _compute_comprehensiveness(\n",
        "        self,\n",
        "        test_data: List[Dict],\n",
        "        test_results: Dict,\n",
        "        hard_predictions: List[np.ndarray],\n",
        "    ) -> Tuple[float, List[float]]:\n",
        "        \"\"\"\n",
        "        Compute comprehensiveness: how much does REMOVING rationales hurt?\n",
        "        Uses DataLoader approach for efficiency\n",
        "        \"\"\"\n",
        "        # Create modified dataset (remove rationales from attention mask)\n",
        "        modified_data = []\n",
        "        for item, rationale_mask in zip(test_data, hard_predictions):\n",
        "            modified_item = self._create_comprehensiveness_instance(\n",
        "                item, rationale_mask\n",
        "            )\n",
        "            modified_data.append(modified_item)\n",
        "\n",
        "        # Create DataLoader\n",
        "        modified_dataset = self.dataset_class(modified_data)\n",
        "        modified_loader = DataLoader(\n",
        "            modified_dataset, batch_size=self.batch_size, shuffle=False\n",
        "        )\n",
        "\n",
        "        # Get predictions using model's predict method\n",
        "        results = self.model.predict(modified_loader, return_attentions=False)\n",
        "        modified_probs = results[\"probabilities\"]\n",
        "\n",
        "        # Calculate comprehensiveness scores\n",
        "        comprehensiveness_scores = []\n",
        "        for idx, (prob, label) in enumerate(zip(test_results[\"probabilities\"], test_results['labels'])):\n",
        "            original_prob = prob[\n",
        "                label\n",
        "            ]  # Probability from normal prediction process for the label\n",
        "            modified_prob = modified_probs[idx][label]\n",
        "\n",
        "            # Comprehensiveness = original - modified (higher is better)\n",
        "            comp_score = original_prob - modified_prob\n",
        "            comprehensiveness_scores.append(comp_score)\n",
        "\n",
        "        return modified_probs, comprehensiveness_scores\n",
        "\n",
        "    def _compute_sufficiency(\n",
        "        self,\n",
        "        test_data: List[Dict],\n",
        "        test_results: Dict,\n",
        "        hard_predictions: List[np.ndarray],\n",
        "    ) -> Tuple[List[float], List[float]]:\n",
        "        \"\"\"\n",
        "        Compute sufficiency: how well do ONLY rationales predict?\n",
        "        Uses DataLoader approach for efficiency\n",
        "        \"\"\"\n",
        "        # Create modified dataset (keep only rationales in attention mask)\n",
        "        modified_data = []\n",
        "        for item, rationale_mask in zip(test_data, hard_predictions):\n",
        "            modified_item = self._create_sufficiency_instance(item, rationale_mask)\n",
        "            modified_data.append(modified_item)\n",
        "\n",
        "        # Create DataLoader\n",
        "        modified_dataset = self.dataset_class(modified_data)\n",
        "        modified_loader = DataLoader(\n",
        "            modified_dataset, batch_size=self.batch_size, shuffle=False\n",
        "        )\n",
        "\n",
        "        # Get predictions using model's predict method\n",
        "        results = self.model.predict(modified_loader, return_attentions=False)\n",
        "        modified_probs = results[\"probabilities\"]\n",
        "\n",
        "        # Calculate sufficiency scores\n",
        "        sufficiency_scores = []\n",
        "        for idx, (prob, label) in enumerate(zip(test_results[\"probabilities\"], test_results['labels'])):\n",
        "            original_prob = prob[\n",
        "                label\n",
        "            ]  # Probability from normal prediction process for the label\n",
        "            modified_prob = modified_probs[idx][label]\n",
        "\n",
        "            # Sufficiency = original - modified (lower/negative is better)\n",
        "            suff_score = original_prob - modified_prob\n",
        "            sufficiency_scores.append(suff_score)\n",
        "\n",
        "        return modified_probs, sufficiency_scores\n",
        "\n",
        "    def _create_comprehensiveness_instance(\n",
        "        self, item: Dict, rationale_mask: np.ndarray\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Create instance for comprehensiveness: REMOVE rationales from attention\n",
        "        Keep: CLS + non-rationale content tokens + SEP\n",
        "        \"\"\"\n",
        "        input_ids = item[\"input_ids\"].cpu().numpy().flatten()\n",
        "        orig_mask = item[\"attention_mask\"].cpu().numpy().flatten()\n",
        "\n",
        "        # Start with original mask\n",
        "        new_mask = orig_mask.copy()\n",
        "\n",
        "        # Zero out rationale positions (except CLS and SEP)\n",
        "        for i in range(len(new_mask)):\n",
        "            if rationale_mask[i] == 1:  # This is a rationale\n",
        "                # Don't mask if it's CLS or SEP\n",
        "                if input_ids[i] not in self.special_token_ids:\n",
        "                    new_mask[i] = 0\n",
        "\n",
        "        return {\n",
        "            \"post_id\": item[\"post_id\"],\n",
        "            \"input_ids\": torch.tensor(input_ids).unsqueeze(0),\n",
        "            \"attention_mask\": torch.tensor(new_mask).unsqueeze(0),\n",
        "            \"rationales\": item[\"rationales\"],\n",
        "            \"hard_label\": item[\"hard_label\"],\n",
        "        }\n",
        "\n",
        "    def _create_sufficiency_instance(\n",
        "        self, item: Dict, rationale_mask: np.ndarray\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Create instance for sufficiency: Keep ONLY rationales in attention\n",
        "        Keep: CLS + rationale tokens + SEP\n",
        "        \"\"\"\n",
        "        input_ids = item[\"input_ids\"].cpu().numpy().flatten()\n",
        "        orig_mask = item[\"attention_mask\"].cpu().numpy().flatten()\n",
        "\n",
        "        # Start with zeros\n",
        "        new_mask = np.zeros_like(orig_mask)\n",
        "\n",
        "        # Always keep CLS and SEP\n",
        "        for i in range(len(new_mask)):\n",
        "            if input_ids[i] in self.special_token_ids:\n",
        "                new_mask[i] = 1\n",
        "\n",
        "        # Keep rationale positions\n",
        "        for i in range(len(new_mask)):\n",
        "            if rationale_mask[i] == 1 and orig_mask[i] == 1:\n",
        "                new_mask[i] = 1\n",
        "\n",
        "        return {\n",
        "            \"post_id\": item[\"post_id\"],\n",
        "            \"input_ids\": torch.tensor(input_ids).unsqueeze(0),\n",
        "            \"attention_mask\": torch.tensor(new_mask).unsqueeze(0),\n",
        "            \"rationales\": item[\"rationales\"],\n",
        "            \"hard_label\": item[\"hard_label\"],\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5af905f2",
      "metadata": {
        "id": "5af905f2"
      },
      "source": [
        "# Experiment Management System\n",
        "\n",
        "This notebook now uses a systematic experiment tracking system that organizes all outputs by experiment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d28e55c0",
      "metadata": {
        "id": "d28e55c0"
      },
      "source": [
        "## Example: Modified Training & Evaluation Pipeline with Experiment Tracking\n",
        "\n",
        "Below is how you would integrate the experiment manager into your existing pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "4c5b1ffe",
      "metadata": {
        "id": "4c5b1ffe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "015979b9-a9d5-42c3-c5b8-12a002d1aefa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "Created new experiment: 20251217_081342_baseline_distilbert_54083f69\n",
            "Directory: experiments/20251217_081342_baseline_distilbert_54083f69\n",
            "Description: Baseline model with distilbert-base-uncased, standard hyperparameters\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# ============================================================================\n",
        "# FULL EXPERIMENT PIPELINE WITH TRACKING\n",
        "# ============================================================================\n",
        "from ExperimentManager import ExperimentManager\n",
        "from HateClassifier import HateClassifier\n",
        "from HateDataset import HateDataset\n",
        "# 1. CREATE EXPERIMENT\n",
        "experiment_manager = ExperimentManager(base_dir=\"./experiments\")\n",
        "experiment_dir = experiment_manager.create_experiment(\n",
        "    config=config,\n",
        "    custom_name=\"baseline_distilbert\",  # Change this for each experiment\n",
        "    description=\"Baseline model with distilbert-base-uncased, standard hyperparameters\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. TRAIN MODEL (config.save_dir is automatically updated)\n",
        "model = HateClassifier(config)\n",
        "history = model.train(train_dataloader=train_loader, val_dataloader=val_loader)\n",
        "\n",
        "# Save training history\n",
        "experiment_manager.save_training_history(history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724,
          "referenced_widgets": [
            "b6dd71dee1a74a25868f8ea579c98557",
            "1c8e68029be74ef69b7b7a0966e67a25",
            "71c2bf6ab93e41c3833407fd44f9da96",
            "659d9f7184754256b8642412f50a86cd",
            "f635d108a8ab4514ad08b47c4d1f54f5",
            "40abd6d2ae5341d58d4a877b9b068d5d",
            "5f8f43a06d8e42a0a6df8a9687ed0473",
            "d1558ed090714e5e90ed1225b960827f",
            "9a62dd8c0c0c4cde9d9877eb36b1de4f",
            "66f73258fdd64512ae020b2f76097530",
            "f25bde9cfed74636856ea8702908b8d5"
          ]
        },
        "id": "2Ir9hUuDJ9nR",
        "outputId": "eb684b75-9cb5-4b86-d20f-49e75248fa14"
      },
      "id": "2Ir9hUuDJ9nR",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b6dd71dee1a74a25868f8ea579c98557"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on device: cuda\n",
            "Model: distilbert-base-uncased\n",
            "Epochs: 2\n",
            "Batch size: 32\n",
            "Gradient accumulation steps: 1\n",
            "Effective batch size: 32\n",
            "Learning rate: 1e-05\n",
            "Mixed precision (AMP): True\n",
            "Gradient clipping: 1.0\n",
            "============================================================\n",
            "\n",
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 481/481 [00:54<00:00,  8.77batch/s, loss=0.514]\n",
            "Evaluating: 100%|██████████| 61/61 [00:01<00:00, 31.18batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1 Summary:\n",
            "  Train Loss: 0.5140\n",
            "  Val Loss:   0.4677\n",
            "  Val Acc:    0.7711\n",
            "  Val F1:     0.7647\n",
            "  ✓ New best model saved! (F1: 0.7647)\n",
            "\n",
            "Epoch 2/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 481/481 [00:54<00:00,  8.89batch/s, loss=0.418]\n",
            "Evaluating: 100%|██████████| 61/61 [00:01<00:00, 32.58batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2 Summary:\n",
            "  Train Loss: 0.4179\n",
            "  Val Loss:   0.4680\n",
            "  Val Acc:    0.7695\n",
            "  Val F1:     0.7633\n",
            "\n",
            "============================================================\n",
            "Training completed!\n",
            "Best F1 Score: 0.7647\n",
            "Training history saved to: experiments/20251217_081342_baseline_distilbert_54083f69/checkpoints/training_history.json\n",
            "Training history saved to: experiments/20251217_081342_baseline_distilbert_54083f69/metrics/training_history.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. EVALUATE MODEL\n",
        "result = model.predict(test_dataloader=test_loader, return_attentions=True)\n",
        "\n",
        "# Save predictions\n",
        "experiment_manager.save_predictions(result, filename=\"test_predictions.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEi5QafbKAtl",
        "outputId": "6cd7bc34-b135-47c9-bcfc-f2da20e9000f"
      },
      "id": "hEi5QafbKAtl",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running inference on 61 batches...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 61/61 [00:01<00:00, 31.85batch/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Test Results:\n",
            "  Test Loss:     0.4493\n",
            "  Test Accuracy: 0.7968\n",
            "  Test F1:       0.7902\n",
            "============================================================\n",
            "Predictions saved to: experiments/20251217_081342_baseline_distilbert_54083f69/results/test_predictions.pkl\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 4. BIAS EVALUATION\n",
        "gmb_metrics, bias_details = calculate_gmb_metrics(\n",
        "    test_data=test_data,\n",
        "    probabilities=result['probabilities'],\n",
        "    target_groups=bias_target_groups\n",
        ")\n",
        "\n",
        "# Save bias metrics\n",
        "experiment_manager.save_bias_metrics(gmb_metrics, bias_details)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXBmyGzAKDYw",
        "outputId": "3aebbbe5-bac9-4f4e-fe54-b9549a1a393a"
      },
      "id": "oXBmyGzAKDYw",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bias metrics saved to: experiments/20251217_081342_baseline_distilbert_54083f69/metrics/bias_metrics.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# 5. XAI EVALUATION (only on hate samples)\n",
        "test_data_hate_only = []\n",
        "test_results_hate_only = {'attentions': [], 'probabilities': [], 'predictions': [], 'post_id': [], 'labels': []}\n",
        "for idx, td in enumerate(test_data):\n",
        "    if td['hard_label'] == 1:\n",
        "        test_data_hate_only.append(td)\n",
        "        test_results_hate_only['attentions'].append(result['attentions'][idx])\n",
        "        test_results_hate_only['probabilities'].append(result['probabilities'][idx])\n",
        "        test_results_hate_only['predictions'].append(result['predictions'][idx])\n",
        "        test_results_hate_only['post_id'].append(result['post_ids'][idx])\n",
        "        test_results_hate_only['labels'].append(result['labels'][idx])\n",
        "\n",
        "calculator = FaithfulnessMetrics(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset_class=HateDataset,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "k = 5\n",
        "eraser_save_path = f\"{experiment_dir}/results/test_explain_output.jsonl\"\n",
        "xai_results = calculator.compute_all_metrics(test_data_hate_only, test_results_hate_only, k, eraser_save_path)\n",
        "\n",
        "# Save XAI metrics\n",
        "experiment_manager.save_xai_metrics(xai_results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbR3vJyBKFx2",
        "outputId": "9f6631b7-6c18-4f11-8a10-a5ad360ee906"
      },
      "id": "VbR3vJyBKFx2",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Computing ERASER metrics using DataLoader approach...\n",
            "\n",
            "[1/3] Computing plausibility metrics...\n",
            "[2/3] Computing comprehensiveness scores...\n",
            "Running inference on 36 batches...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 36/36 [00:01<00:00, 23.98batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Test Results:\n",
            "  Test Loss:     1.1826\n",
            "  Test Accuracy: 0.3126\n",
            "  Test F1:       0.2382\n",
            "============================================================\n",
            "[3/3] Computing sufficiency scores...\n",
            "Running inference on 36 batches...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Testing: 100%|██████████| 36/36 [00:01<00:00, 28.36batch/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Test Results:\n",
            "  Test Loss:     0.5711\n",
            "  Test Accuracy: 0.6874\n",
            "  Test F1:       0.4074\n",
            "============================================================\n",
            "XAI metrics saved to: experiments/20251217_081342_baseline_distilbert_54083f69/metrics/xai_metrics.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/jayded/eraserbenchmark.git\n",
        "!sed -i \"285s/.*/    labels=['0', '1']/\" eraserbenchmark/rationale_benchmark/metrics.py\n",
        "!sed -i \"286s/.*/    label_to_int = {'0':0, '1': 1}/\" eraserbenchmark/rationale_benchmark/metrics.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m4B2sa8GQLIZ",
        "outputId": "c234dc0c-2cb0-47f7-d2e0-d23287e288b0"
      },
      "id": "m4B2sa8GQLIZ",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'eraserbenchmark'...\n",
            "remote: Enumerating objects: 90, done.\u001b[K\n",
            "remote: Counting objects: 100% (90/90), done.\u001b[K\n",
            "remote: Compressing objects: 100% (46/46), done.\u001b[K\n",
            "remote: Total 90 (delta 51), reused 79 (delta 43), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (90/90), 69.80 KiB | 3.17 MiB/s, done.\n",
            "Resolving deltas: 100% (51/51), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "score_file = f\"{experiment_dir}/results/eraser_result.json\"\n",
        "!PYTHONPATH=./eraserbenchmark:%PYTHONPATH% && python eraserbenchmark/rationale_benchmark/metrics.py --split test --strict --data_dir Data/explanations --results {eraser_save_path} --score_file {score_file}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAa1nG10MRA9",
        "outputId": "7f3d421e-57c8-4b2d-ea2b-d1801d28c0af"
      },
      "id": "gAa1nG10MRA9",
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  6919 MainThread Error in instances: 0 instances fail validation: set()\n",
            " 12982 MainThread No sentence level predictions detected, skipping sentence-level diagnostic\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "{'classification_scores': {'accuracy': 0.8204903677758318,\n",
            "                           'aopc_thresholds': None,\n",
            "                           'comprehensiveness': np.float64(0.36346780362289444),\n",
            "                           'comprehensiveness_aopc': None,\n",
            "                           'comprehensiveness_aopc_points': None,\n",
            "                           'comprehensiveness_entropy': np.float64(-0.1625215074979172),\n",
            "                           'comprehensiveness_kl': np.float64(1.0234625085906859),\n",
            "                           'prf': {'0': {'f1-score': 0.0,\n",
            "                                         'precision': 0.0,\n",
            "                                         'recall': 0.0,\n",
            "                                         'support': 0.0},\n",
            "                                   '1': {'f1-score': 0.9013949013949014,\n",
            "                                         'precision': 1.0,\n",
            "                                         'recall': 0.8204903677758318,\n",
            "                                         'support': 1142.0},\n",
            "                                   'accuracy': 0.8204903677758318,\n",
            "                                   'macro avg': {'f1-score': 0.4506974506974507,\n",
            "                                                 'precision': 0.5,\n",
            "                                                 'recall': 0.4102451838879159,\n",
            "                                                 'support': 1142.0},\n",
            "                                   'weighted avg': {'f1-score': 0.9013949013949014,\n",
            "                                                    'precision': 1.0,\n",
            "                                                    'recall': 0.8204903677758318,\n",
            "                                                    'support': 1142.0}},\n",
            "                           'sufficiency': np.float64(0.1135298509000688),\n",
            "                           'sufficiency_aopc': None,\n",
            "                           'sufficiency_aopc_points': None,\n",
            "                           'sufficiency_entropy': np.float64(-0.036248508263764155),\n",
            "                           'sufficiency_kl': np.float64(0.2448656215829926)},\n",
            " 'iou_scores': [{'macro': {'f1': 0.27010810887678566,\n",
            "                           'p': 0.21198190309398718,\n",
            "                           'r': 0.37215411558669004},\n",
            "                 'micro': {'f1': 0.22346153846153843,\n",
            "                           'p': 0.1659525849757212,\n",
            "                           'r': 0.34196586227192466},\n",
            "                 'threshold': 0.5}],\n",
            " 'rationale_prf': {'instance_macro': {'f1': 0.06558113029216357,\n",
            "                                      'p': 0.0539112667834209,\n",
            "                                      'r': 0.09795680093403385},\n",
            "                   'instance_micro': {'f1': 0.06346153846153846,\n",
            "                                      'p': 0.04712939160239932,\n",
            "                                      'r': 0.09711595055915244}},\n",
            " 'token_prf': {'instance_macro': {'f1': 0.45043241021213265,\n",
            "                                  'p': 0.6066549912434326,\n",
            "                                  'r': 0.4736719898564725},\n",
            "               'instance_micro': {'f1': 0.3721755971594577,\n",
            "                                  'p': 0.6063102541630149,\n",
            "                                  'r': 0.2684933633470465}},\n",
            " 'token_soft_metrics': {'auprc': np.float64(0.4516786086129337),\n",
            "                        'average_precision': np.float64(0.5118488003787499),\n",
            "                        'roc_auc_score': np.float64(0.5637366874072286)}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. CREATE FINAL SUMMARY\n",
        "final_summary = {\n",
        "    \"test_accuracy\": float(result['accuracy']),\n",
        "    \"test_f1\": float(result['f1']),\n",
        "    \"test_loss\": float(result['loss']),\n",
        "    \"gmb_metrics\": gmb_metrics,\n",
        "    \"xai_metrics\": xai_results,\n",
        "    \"total_params\": sum(p.numel() for p in model.base_model.parameters()),\n",
        "}\n",
        "\n",
        "experiment_manager.save_final_metrics(final_summary)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4MkudEzKIKd",
        "outputId": "ce9374af-e474-47a3-e2d2-25a2f28e7934"
      },
      "id": "n4MkudEzKIKd",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final metrics summary saved to: experiments/20251217_081342_baseline_distilbert_54083f69/metrics/final_summary.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. MARK EXPERIMENT AS COMPLETE\n",
        "experiment_manager.mark_complete(\n",
        "    status=\"completed\",\n",
        "    notes=\"Baseline experiment completed successfully\"\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPERIMENT COMPLETED!\")\n",
        "print(f\"All results saved to: {experiment_dir}\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iUu9LN2KKRv",
        "outputId": "95d7e7b9-1f55-4ee4-a2fb-05aa4215c332"
      },
      "id": "4iUu9LN2KKRv",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Experiment 20251217_081342_baseline_distilbert_54083f69 marked as: completed\n",
            "\n",
            "================================================================================\n",
            "EXPERIMENT COMPLETED!\n",
            "All results saved to: experiments/20251217_081342_baseline_distilbert_54083f69\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9789d842",
      "metadata": {
        "id": "9789d842"
      },
      "source": [
        "## Experiment Management Utilities\n",
        "\n",
        "Useful commands for managing and comparing experiments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "ba59f95b",
      "metadata": {
        "id": "ba59f95b"
      },
      "outputs": [],
      "source": [
        "# # View all experiments\n",
        "# experiment_manager = ExperimentManager()\n",
        "# experiment_manager.print_experiment_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "b2ae2540",
      "metadata": {
        "id": "b2ae2540"
      },
      "outputs": [],
      "source": [
        "# # List only completed experiments\n",
        "# completed_experiments = experiment_manager.list_experiments(status=\"completed\")\n",
        "# print(f\"Found {len(completed_experiments)} completed experiments\")\n",
        "# for exp in completed_experiments:\n",
        "#     print(f\"  - {exp['experiment_id']}: {exp.get('description', 'No description')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "5bf5f661",
      "metadata": {
        "id": "5bf5f661"
      },
      "outputs": [],
      "source": [
        "# # Compare multiple experiments\n",
        "# experiment_ids = [\n",
        "#     \"20251217_071414_baseline_distilbert_54083f69\",  # Replace with actual experiment IDs\n",
        "# ]\n",
        "# comparison = experiment_manager.compare_experiments(experiment_ids)\n",
        "\n",
        "# # Display comparison\n",
        "# for exp in comparison[\"experiments\"]:\n",
        "#     print(f\"\\nExperiment: {exp['experiment_id']}\")\n",
        "#     print(f\"  Model: {exp['config'].get('model_name', 'N/A')}\")\n",
        "#     print(f\"  Learning Rate: {exp['config'].get('learning_rate', 'N/A')}\")\n",
        "#     print(f\"  Test F1: {exp['metrics'].get('test_f1', 'N/A')}\")\n",
        "#     print(f\"  Test Accuracy: {exp['metrics'].get('test_accuracy', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "566b60e8",
      "metadata": {
        "id": "566b60e8"
      },
      "outputs": [],
      "source": [
        "# # Load results from a specific experiment\n",
        "# experiment_id = \"20251217_071414_baseline_distilbert_54083f69\"  # Replace with actual ID\n",
        "# exp_dir = experiment_manager.get_experiment_path(experiment_id)\n",
        "\n",
        "# if exp_dir:\n",
        "#     # Load config\n",
        "#     with open(exp_dir / \"config.json\", 'r') as f:\n",
        "#         loaded_config = json.load(f)\n",
        "\n",
        "#     # Load final metrics\n",
        "#     with open(exp_dir / \"metrics\" / \"final_summary.json\", 'r') as f:\n",
        "#         loaded_metrics = json.load(f)\n",
        "\n",
        "#     print(f\"Loaded experiment: {experiment_id}\")\n",
        "#     print(f\"Test F1: {loaded_metrics['test_f1']}\")\n",
        "#     print(f\"Test Accuracy: {loaded_metrics['test_accuracy']}\")\n",
        "# else:\n",
        "#     print(f\"Experiment {experiment_id} not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "42d559b8",
      "metadata": {
        "id": "42d559b8"
      },
      "outputs": [],
      "source": [
        "# # Export an experiment for sharing or backup\n",
        "# experiment_id = \"20251217_071414_baseline_distilbert_54083f69\"  # Replace with actual ID\n",
        "# experiment_manager.export_experiment(experiment_id, export_path=\"./exported_experiments\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd18f25a",
      "metadata": {
        "id": "fd18f25a"
      },
      "source": [
        "## Visualization Tools\n",
        "\n",
        "Visualize and compare experiment results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "a866b568",
      "metadata": {
        "id": "a866b568"
      },
      "outputs": [],
      "source": [
        "# from experiment_visualization import (\n",
        "#     plot_training_curves,\n",
        "#     plot_metrics_comparison,\n",
        "#     plot_bias_metrics,\n",
        "#     plot_xai_metrics,\n",
        "#     create_experiment_report\n",
        "# )\n",
        "\n",
        "# # Example: Compare training curves across experiments\n",
        "# experiment_ids = [\n",
        "#     \"20251217_071414_baseline_distilbert_54083f69\",  # Replace with your actual experiment IDs\n",
        "# ]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # Uncomment to use:\n",
        "# plot_training_curves(experiment_ids, save_path=\"training_curves.png\")"
      ],
      "metadata": {
        "id": "R9MrOwBSLckI"
      },
      "id": "R9MrOwBSLckI",
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "5452b4ff",
      "metadata": {
        "id": "5452b4ff"
      },
      "outputs": [],
      "source": [
        "# # Compare final metrics across experiments\n",
        "# plot_metrics_comparison(experiment_ids, save_path=\"metrics_comparison.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "4fbfc7ee",
      "metadata": {
        "id": "4fbfc7ee"
      },
      "outputs": [],
      "source": [
        "# # Visualize bias metrics\n",
        "# plot_bias_metrics(experiment_ids, save_path=\"bias_comparison.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "0fda726e",
      "metadata": {
        "id": "0fda726e"
      },
      "outputs": [],
      "source": [
        "# # Visualize XAI metrics\n",
        "# plot_xai_metrics(experiment_ids, save_path=\"xai_comparison.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "643918a9",
      "metadata": {
        "id": "643918a9"
      },
      "outputs": [],
      "source": [
        "# # Generate HTML report for a specific experiment\n",
        "# create_experiment_report(\"20241216_120000_baseline_a1b2c3d4\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4a2cdb2e0469408482f033c2b54c5285": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c5b5ec5c9729431b95b09ac9deb639a4",
              "IPY_MODEL_921ac27b2716401d9c30a91f754e6efd",
              "IPY_MODEL_ea6db4218aed4bad847cec4888f0bc73"
            ],
            "layout": "IPY_MODEL_fd28242321e3401b99d6dc7673e49a34"
          }
        },
        "c5b5ec5c9729431b95b09ac9deb639a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_601fc433cef044028339febadaeabb4c",
            "placeholder": "​",
            "style": "IPY_MODEL_8164902fa691445e9c9a1fa3c1dc3267",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "921ac27b2716401d9c30a91f754e6efd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56f946e5499b4930989d2a1ccd6322c6",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_90b749f0d1e64a70b6dca9a2dff56c13",
            "value": 48
          }
        },
        "ea6db4218aed4bad847cec4888f0bc73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_508d38530fdb44839b819d00f40fe211",
            "placeholder": "​",
            "style": "IPY_MODEL_d57aecc53ab14aa1ac0e8547fa5d2d61",
            "value": " 48.0/48.0 [00:00&lt;00:00, 1.18kB/s]"
          }
        },
        "fd28242321e3401b99d6dc7673e49a34": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "601fc433cef044028339febadaeabb4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8164902fa691445e9c9a1fa3c1dc3267": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56f946e5499b4930989d2a1ccd6322c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90b749f0d1e64a70b6dca9a2dff56c13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "508d38530fdb44839b819d00f40fe211": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d57aecc53ab14aa1ac0e8547fa5d2d61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5e43f25fd86444da9e6196d5c4c3739": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_74b0d3bba1e442de86fda672e2afadc0",
              "IPY_MODEL_d6bd5c3f917d4e3b94d884297a9cc7ad",
              "IPY_MODEL_fe5e4e766b5f440696ebe6df00923b37"
            ],
            "layout": "IPY_MODEL_9024cb8d3ad04a1f827babf2f1f1781c"
          }
        },
        "74b0d3bba1e442de86fda672e2afadc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_685a9314a1c64e87be54e2be16359df3",
            "placeholder": "​",
            "style": "IPY_MODEL_d2b0cab60520428c956ea6cd5cf2ec44",
            "value": "config.json: 100%"
          }
        },
        "d6bd5c3f917d4e3b94d884297a9cc7ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_33405549d3e44410ae9c8379d044c861",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1659625325fb4872a32c178d885fbb96",
            "value": 483
          }
        },
        "fe5e4e766b5f440696ebe6df00923b37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_477617c871bb439ca17e7049b31cc9ab",
            "placeholder": "​",
            "style": "IPY_MODEL_1a0aab2938dd42649d3187bd56793f19",
            "value": " 483/483 [00:00&lt;00:00, 14.8kB/s]"
          }
        },
        "9024cb8d3ad04a1f827babf2f1f1781c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "685a9314a1c64e87be54e2be16359df3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2b0cab60520428c956ea6cd5cf2ec44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33405549d3e44410ae9c8379d044c861": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1659625325fb4872a32c178d885fbb96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "477617c871bb439ca17e7049b31cc9ab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a0aab2938dd42649d3187bd56793f19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f11a71db483040dc9edc3475df3c53ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f7c61c1dee448f68c627d7045c10b4b",
              "IPY_MODEL_f7c6ed35073b45458d1b3608cea6e06d",
              "IPY_MODEL_0be4123404e84c9bb16a5af9bbe51d76"
            ],
            "layout": "IPY_MODEL_c53a8778b7ba4a249db56f3ad622d5d2"
          }
        },
        "3f7c61c1dee448f68c627d7045c10b4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6392708170254b42a7368c7ab93f2f1c",
            "placeholder": "​",
            "style": "IPY_MODEL_41a18afb2eb54b948eec81126dcc7819",
            "value": "vocab.txt: 100%"
          }
        },
        "f7c6ed35073b45458d1b3608cea6e06d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4bf7a8d576a140beb9a37f0b0c869993",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2a49b41cec2f4c048ac964867ef0dd44",
            "value": 231508
          }
        },
        "0be4123404e84c9bb16a5af9bbe51d76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37f66a80f6aa4d1cb9ef7610901280bc",
            "placeholder": "​",
            "style": "IPY_MODEL_02355d6d66ed43518446dca35994f50e",
            "value": " 232k/232k [00:00&lt;00:00, 4.23MB/s]"
          }
        },
        "c53a8778b7ba4a249db56f3ad622d5d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6392708170254b42a7368c7ab93f2f1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41a18afb2eb54b948eec81126dcc7819": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4bf7a8d576a140beb9a37f0b0c869993": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a49b41cec2f4c048ac964867ef0dd44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37f66a80f6aa4d1cb9ef7610901280bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02355d6d66ed43518446dca35994f50e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1cd9f68e106b4dadb8661d6d40afa91f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce56b1483d0a4468a6672b75d1f85a94",
              "IPY_MODEL_69c758818c234a7ab75fc366e491bfde",
              "IPY_MODEL_5eaa094f2b77421fb6ac0ac376f0f287"
            ],
            "layout": "IPY_MODEL_ba63e6c02a414bf1997533e5bf420643"
          }
        },
        "ce56b1483d0a4468a6672b75d1f85a94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5c9bfea25f5b499db480b0681cb2d276",
            "placeholder": "​",
            "style": "IPY_MODEL_fe4e916f0c614c26bed6d9d828a9f508",
            "value": "tokenizer.json: 100%"
          }
        },
        "69c758818c234a7ab75fc366e491bfde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fd91e17ea8b24b0fb52b8be5b4ee2ba7",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6abdc665a9a048b1b93f9a2d1a542ebc",
            "value": 466062
          }
        },
        "5eaa094f2b77421fb6ac0ac376f0f287": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_189ae264855344f2a53590d3a166e3f1",
            "placeholder": "​",
            "style": "IPY_MODEL_98b55b2706cf4d56b22e504aa3917444",
            "value": " 466k/466k [00:00&lt;00:00, 5.43MB/s]"
          }
        },
        "ba63e6c02a414bf1997533e5bf420643": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c9bfea25f5b499db480b0681cb2d276": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe4e916f0c614c26bed6d9d828a9f508": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fd91e17ea8b24b0fb52b8be5b4ee2ba7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6abdc665a9a048b1b93f9a2d1a542ebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "189ae264855344f2a53590d3a166e3f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98b55b2706cf4d56b22e504aa3917444": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b6dd71dee1a74a25868f8ea579c98557": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1c8e68029be74ef69b7b7a0966e67a25",
              "IPY_MODEL_71c2bf6ab93e41c3833407fd44f9da96",
              "IPY_MODEL_659d9f7184754256b8642412f50a86cd"
            ],
            "layout": "IPY_MODEL_f635d108a8ab4514ad08b47c4d1f54f5"
          }
        },
        "1c8e68029be74ef69b7b7a0966e67a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40abd6d2ae5341d58d4a877b9b068d5d",
            "placeholder": "​",
            "style": "IPY_MODEL_5f8f43a06d8e42a0a6df8a9687ed0473",
            "value": "model.safetensors: 100%"
          }
        },
        "71c2bf6ab93e41c3833407fd44f9da96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1558ed090714e5e90ed1225b960827f",
            "max": 267954768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9a62dd8c0c0c4cde9d9877eb36b1de4f",
            "value": 267954768
          }
        },
        "659d9f7184754256b8642412f50a86cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66f73258fdd64512ae020b2f76097530",
            "placeholder": "​",
            "style": "IPY_MODEL_f25bde9cfed74636856ea8702908b8d5",
            "value": " 268M/268M [00:03&lt;00:00, 140MB/s]"
          }
        },
        "f635d108a8ab4514ad08b47c4d1f54f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40abd6d2ae5341d58d4a877b9b068d5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f8f43a06d8e42a0a6df8a9687ed0473": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1558ed090714e5e90ed1225b960827f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a62dd8c0c0c4cde9d9877eb36b1de4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "66f73258fdd64512ae020b2f76097530": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f25bde9cfed74636856ea8702908b8d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}