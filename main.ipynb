{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "24658442",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24658442",
        "outputId": "5f8b1847-b0ee-4924-df7c-d2b69efad303"
      },
      "outputs": [],
      "source": [
        "# import shutil\n",
        "# import os\n",
        "\n",
        "# # Delete HateDeRC directory if it exists\n",
        "# if os.path.exists('HateDeRC'):\n",
        "#   shutil.rmtree('HateDeRC')\n",
        "# !git clone https://github.com/jamesalv/HateDeRC\n",
        "# %cd HateDeRC"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8bccce96",
      "metadata": {
        "id": "8bccce96"
      },
      "outputs": [],
      "source": [
        "from TrainingConfig import TrainingConfig\n",
        "from typing import Dict, Any, Tuple, List\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import AutoTokenizer\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "1f654143",
      "metadata": {
        "id": "1f654143"
      },
      "outputs": [],
      "source": [
        "data_path = 'Data/dataset.json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "b3618d6e",
      "metadata": {
        "id": "b3618d6e"
      },
      "outputs": [],
      "source": [
        "config = TrainingConfig()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "487ff027",
      "metadata": {
        "id": "487ff027"
      },
      "outputs": [],
      "source": [
        "# Seed all randomness for reproducibility\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "torch.manual_seed(config.seed)\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.manual_seed_all(config.seed)\n",
        "np.random.seed(config.seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53d93027",
      "metadata": {
        "id": "53d93027"
      },
      "source": [
        "# Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "f81645fa",
      "metadata": {
        "id": "f81645fa"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "def deobfuscate_text(text):\n",
        "    \"\"\"\n",
        "    Normalize common text obfuscation patterns to reveal original words.\n",
        "    Useful for hate speech detection and content analysis.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text with potential obfuscations\n",
        "\n",
        "    Returns:\n",
        "        str: Text with obfuscations normalized\n",
        "    \"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return text\n",
        "\n",
        "    # Make a copy to work with\n",
        "    result = text.lower()\n",
        "\n",
        "    # 1. Handle asterisk/symbol replacements\n",
        "    symbol_patterns = {\n",
        "        # Common profanity\n",
        "        r'f\\*+c?k': 'fuck',\n",
        "        r'f\\*+': 'fuck',\n",
        "        r's\\*+t': 'shit',\n",
        "        r'b\\*+ch': 'bitch',\n",
        "        r'a\\*+s': 'ass',\n",
        "        r'd\\*+n': 'damn',\n",
        "        r'h\\*+l': 'hell',\n",
        "        r'c\\*+p': 'crap',\n",
        "\n",
        "        # Slurs and hate speech terms (be comprehensive for detection)\n",
        "        r'n\\*+g+[aer]+': 'nigger',  # Various n-word obfuscations\n",
        "        r'f\\*+g+[ot]*': 'faggot',\n",
        "        r'r\\*+[dt]ard': 'retard',\n",
        "        r'sp\\*+c': 'spic',\n",
        "\n",
        "        # Other symbols\n",
        "        r'@ss': 'ass',\n",
        "        r'b@tch': 'bitch',\n",
        "        r'sh!t': 'shit',\n",
        "        r'f#ck': 'fuck',\n",
        "        r'd@mn': 'damn',\n",
        "    }\n",
        "\n",
        "    for pattern, replacement in symbol_patterns.items():\n",
        "        result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)\n",
        "\n",
        "    # 2. Handle character spacing (f u c k -> fuck)\n",
        "    spacing_patterns = {\n",
        "        r'\\bf\\s+u\\s+c\\s+k\\b': 'fuck',\n",
        "        r'\\bs\\s+h\\s+i\\s+t\\b': 'shit',\n",
        "        r'\\bd\\s+a\\s+m\\s+n\\b': 'damn',\n",
        "        r'\\bh\\s+e\\s+l\\s+l\\b': 'hell',\n",
        "        r'\\ba\\s+s\\s+s\\b': 'ass',\n",
        "        r'\\bc\\s+r\\s+a\\s+p\\b': 'crap',\n",
        "    }\n",
        "\n",
        "    for pattern, replacement in spacing_patterns.items():\n",
        "        result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)\n",
        "\n",
        "    # 3. Handle number/letter substitutions\n",
        "    leet_patterns = {\n",
        "        # Basic leet speak\n",
        "        r'\\b3\\s*1\\s*1\\s*3\\b': 'elle',  # 3113 -> elle\n",
        "        r'\\bf4g\\b': 'fag',\n",
        "        r'\\bf4gg0t\\b': 'faggot',\n",
        "        r'\\bn00b\\b': 'noob',\n",
        "        r'\\bl33t\\b': 'leet',\n",
        "        r'\\bh4t3\\b': 'hate',\n",
        "        r'\\b5h1t\\b': 'shit',\n",
        "        r'\\bf0ck\\b': 'fock',\n",
        "    }\n",
        "\n",
        "    for pattern, replacement in leet_patterns.items():\n",
        "        result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)\n",
        "\n",
        "    # 4. Handle repeated characters and separators\n",
        "    # Remove excessive punctuation between letters\n",
        "    result = re.sub(r'([a-z])[^\\w\\s]+([a-z])', r'\\1\\2', result)\n",
        "\n",
        "    # Handle underscore separation\n",
        "    result = re.sub(r'([a-z])_+([a-z])', r'\\1\\2', result)\n",
        "\n",
        "    # Handle dot separation\n",
        "    result = re.sub(r'([a-z])\\.+([a-z])', r'\\1\\2', result)\n",
        "\n",
        "    # 5. Handle common misspellings/variations used for evasion\n",
        "    evasion_patterns = {\n",
        "        r'\\bfuk\\b': 'fuck',\n",
        "        r'\\bfuq\\b': 'fuck',\n",
        "        r'\\bfck\\b': 'fuck',\n",
        "        r'\\bshyt\\b': 'shit',\n",
        "        r'\\bshit\\b': 'shit',\n",
        "        r'\\bbiatch\\b': 'bitch',\n",
        "        r'\\bbeatch\\b': 'bitch',\n",
        "        r'\\basshole\\b': 'asshole',\n",
        "        r'\\ba55hole\\b': 'asshole',\n",
        "        r'\\btard\\b': 'retard',\n",
        "        r'\\bfagg\\b': 'fag',\n",
        "    }\n",
        "\n",
        "    for pattern, replacement in evasion_patterns.items():\n",
        "        result = re.sub(pattern, replacement, result, flags=re.IGNORECASE)\n",
        "\n",
        "    # 6. Clean up multiple spaces\n",
        "    result = re.sub(r'\\s+', ' ', result).strip()\n",
        "\n",
        "    return result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f070b12b",
      "metadata": {
        "id": "f070b12b"
      },
      "outputs": [],
      "source": [
        "def aggregate_rationales(rationales, labels, post_length, drop_abnormal=False):\n",
        "    \"\"\"\n",
        "    If all 3 annotators are normal → 3 zero spans → average (all zeros).\n",
        "    If k annotators are non-normal and k spans exist → average the k spans (no added zeros).\n",
        "    If k non-normal but fewer than k spans:\n",
        "        If the missing annotators are non-normal → do not fill with zeros; average only existing spans and record rationale_support = #spans.\n",
        "        If the missing annotators are normal (e.g., 2 hate + 1 normal + 2 spans) → append one zero span for the normal.\n",
        "    \"\"\"\n",
        "    count_normal = labels.count(0)\n",
        "    count_hate = labels.count(1)\n",
        "    count_rationales = len(rationales)\n",
        "    pad = np.zeros(post_length, dtype=\"int\").tolist()\n",
        "\n",
        "    # If there are hate labels but no rationales, something is wrong\n",
        "    if count_hate > 0 and count_rationales == 0:\n",
        "        if drop_abnormal:\n",
        "            return None\n",
        "\n",
        "        # Else just fill with 0\n",
        "        return np.zeros(post_length).tolist()\n",
        "\n",
        "    # If all annotators are normal, return all zeros\n",
        "    if count_normal == 3:\n",
        "        return np.zeros(post_length).tolist()\n",
        "\n",
        "    # If we have hate annotators\n",
        "    if count_hate > 0:\n",
        "        # Case 1: Number of rationales matches number of hate annotators\n",
        "        if count_rationales == count_hate:\n",
        "            return np.average(rationales, axis=0).tolist()\n",
        "\n",
        "        # Case 2: Fewer rationales than hate annotators\n",
        "        elif count_rationales < count_hate:\n",
        "            # Add zero padding for normal annotators only\n",
        "            rationales_copy = rationales.copy()\n",
        "            zeros_to_add = count_normal\n",
        "            for _ in range(zeros_to_add):\n",
        "                rationales_copy.append(pad)\n",
        "            return np.average(rationales_copy, axis=0).tolist()\n",
        "\n",
        "        # Case 3: More rationales than hate annotators (shouldn't happen normally)\n",
        "        else:\n",
        "            # Just average what we have\n",
        "            return np.average(rationales, axis=0).tolist()\n",
        "\n",
        "    # Fallback: return zeros if no clear case matches\n",
        "    return np.zeros(post_length).tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8d446ddb",
      "metadata": {
        "id": "8d446ddb"
      },
      "outputs": [],
      "source": [
        "from typing import List, Tuple\n",
        "\n",
        "def preprocess_text(raw_text):\n",
        "    preprocessed_text = raw_text\n",
        "    # # Remove HTML tags <>\n",
        "    preprocessed_text = preprocessed_text.replace(\"<\", \"\").replace(\">\", \"\")\n",
        "    # # De-Obsfucate Patterns\n",
        "    preprocessed_text = deobfuscate_text(preprocessed_text)\n",
        "\n",
        "    return preprocessed_text\n",
        "\n",
        "\n",
        "def create_text_segment(\n",
        "    text_tokens: List[str], rationale_mask: List[int]\n",
        ") -> List[Tuple[List[str], int]]:\n",
        "    \"\"\"\n",
        "    Process a rationale mask to identify contiguous segments of highlighted text.\n",
        "    Then create a segmented representation of the tokens\n",
        "\n",
        "    Args:\n",
        "        text_tokens: Original text tokens\n",
        "        mask: Binary mask where 1 indicates a highlighted token (this consists of mask from 3 annotators)\n",
        "\n",
        "    Returns:\n",
        "        A list of tuples (text segment, mask value)\n",
        "    \"\"\"\n",
        "    # Handle case where mask is empty (no rationale provided), usually this is normal classification\n",
        "    mask = rationale_mask\n",
        "\n",
        "    # for mask in all_rationale_mask:\n",
        "    # Find breakpoints (transitions between highlighted/1 and non-highlighted/0)\n",
        "    breakpoints = []\n",
        "    mask_values = []\n",
        "\n",
        "    # Always start with position 0\n",
        "    breakpoints.append(0)\n",
        "    mask_values.append(mask[0])\n",
        "\n",
        "    # Find transitions in the mask\n",
        "    for i in range(1, len(mask)):\n",
        "        if mask[i] != mask[i - 1]:\n",
        "            breakpoints.append(i)\n",
        "            mask_values.append(mask[i])\n",
        "\n",
        "    # Always end with the length of the text\n",
        "    if breakpoints[-1] != len(mask):\n",
        "        breakpoints.append(len(mask))\n",
        "\n",
        "    # Create segments based on breakpoints\n",
        "    segments = []\n",
        "    for i in range(len(breakpoints) - 1):\n",
        "        start = breakpoints[i]\n",
        "        end = breakpoints[i + 1]\n",
        "        segments.append((text_tokens[start:end], mask_values[i]))\n",
        "\n",
        "    return segments\n",
        "\n",
        "\n",
        "def align_rationales(tokens, rationales, tokenizer, max_length=128):\n",
        "    \"\"\"\n",
        "    Align rationales with tokenized text while handling different tokenizer formats.\n",
        "\n",
        "    Args:\n",
        "        tokens: Original text tokens\n",
        "        rationales: Original rationale masks\n",
        "        tokenizer: The tokenizer to use\n",
        "        max_length: Maximum sequence length\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with tokenized inputs and aligned rationale masks\n",
        "    \"\"\"\n",
        "    segments = create_text_segment(tokens, rationales)\n",
        "    all_human_rationales = []\n",
        "    all_input_ids = []\n",
        "    all_attention_mask = []\n",
        "    all_token_type_ids = []\n",
        "    all_rationales = []\n",
        "    for text_segment, rationale_value in segments:\n",
        "        inputs = {}\n",
        "        concatenated_text = \" \".join(text_segment)\n",
        "        processed_segment = preprocess_text(concatenated_text)\n",
        "        tokenized = tokenizer(\n",
        "            processed_segment, add_special_tokens=False, return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # Extract the relevant data\n",
        "        segment_input_ids = tokenized[\"input_ids\"][0]\n",
        "        segment_attention_mask = tokenized[\"attention_mask\"][0]\n",
        "        # Handle token_type_ids if present\n",
        "        if \"token_type_ids\" in tokenized:\n",
        "            segment_token_type_ids = tokenized[\"token_type_ids\"][0]\n",
        "            all_token_type_ids.extend(segment_token_type_ids)\n",
        "\n",
        "        # Add input IDs and attention mask\n",
        "        all_input_ids.extend(segment_input_ids)\n",
        "        all_attention_mask.extend(segment_attention_mask)\n",
        "\n",
        "        # Add rationales (excluding special tokens)\n",
        "        segment_rationales = [rationale_value] * len(segment_input_ids)\n",
        "        all_rationales.extend(segment_rationales)\n",
        "    # Get special token IDs\n",
        "    cls_token_id = tokenizer.cls_token_id\n",
        "    sep_token_id = tokenizer.sep_token_id\n",
        "\n",
        "    # Add special tokens at the beginning and end\n",
        "    all_input_ids = [cls_token_id] + all_input_ids + [sep_token_id]\n",
        "    all_attention_mask = [1] + all_attention_mask + [1]\n",
        "\n",
        "    # Handle token_type_ids if the model requires it\n",
        "    if hasattr(tokenizer, \"create_token_type_ids_from_sequences\"):\n",
        "        all_token_type_ids = tokenizer.create_token_type_ids_from_sequences(\n",
        "            all_input_ids[1:-1]\n",
        "        )\n",
        "    elif all_token_type_ids:\n",
        "        all_token_type_ids = [0] + all_token_type_ids + [0]\n",
        "    else:\n",
        "        all_token_type_ids = [0] * len(all_input_ids)\n",
        "\n",
        "    # Check tokenized vs rationales length\n",
        "    if len(all_input_ids) != len(all_attention_mask):\n",
        "        print(\"Warning: length of tokens and rationales do not match\")\n",
        "\n",
        "    # Add zero rationale values for special tokens\n",
        "    all_rationales = [0] + all_rationales + [0]\n",
        "\n",
        "    # Truncate to max length if needed\n",
        "    if len(all_input_ids) > max_length:\n",
        "        print(\"WARNING: NEED TO TRUNCATE\")\n",
        "        all_input_ids = all_input_ids[:max_length]\n",
        "        all_attention_mask = all_attention_mask[:max_length]\n",
        "        all_token_type_ids = all_token_type_ids[:max_length]\n",
        "        all_rationales = all_rationales[:max_length]\n",
        "\n",
        "    # Pad to max_length if needed\n",
        "    pad_token_id = tokenizer.pad_token_id\n",
        "    padding_length = max_length - len(all_input_ids)\n",
        "\n",
        "    if padding_length > 0:\n",
        "        all_input_ids = all_input_ids + [pad_token_id] * padding_length\n",
        "        all_attention_mask = all_attention_mask + [0] * padding_length\n",
        "        all_token_type_ids = all_token_type_ids + [0] * padding_length\n",
        "        all_rationales = all_rationales + [0] * padding_length\n",
        "\n",
        "    # Convert lists to tensors\n",
        "    inputs = {\n",
        "        \"input_ids\": torch.tensor([all_input_ids], dtype=torch.long),\n",
        "        \"attention_mask\": torch.tensor([all_attention_mask], dtype=torch.long),\n",
        "        \"token_type_ids\": (\n",
        "            torch.tensor([all_token_type_ids], dtype=torch.long)\n",
        "            if \"token_type_ids\" in tokenizer.model_input_names\n",
        "            else None\n",
        "        ),\n",
        "        \"rationales\": torch.tensor([all_rationales], dtype=torch.float32),\n",
        "    }\n",
        "\n",
        "    # Remove None values\n",
        "    inputs = {k: v for k, v in inputs.items() if v is not None}\n",
        "    return inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "pEC9l6Mr6xo0",
      "metadata": {
        "id": "pEC9l6Mr6xo0"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import json\n",
        "import os\n",
        "import string\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import more_itertools as mit\n",
        "\n",
        "def find_ranges(iterable):\n",
        "    \"\"\"Yield range of consecutive numbers.\"\"\"\n",
        "    for group in mit.consecutive_groups(iterable):\n",
        "        group = list(group)\n",
        "        if len(group) == 1:\n",
        "            yield group[0]\n",
        "        else:\n",
        "            yield group[0], group[-1]\n",
        "\n",
        "def process_and_convert_data(data, tokenizer, post_id_divisions, save_path='Data/explanations/', drop_abnormal=False):\n",
        "    \"\"\"\n",
        "    Combined function that processes raw entries and converts to ERASER format in one pass.\n",
        "    Also splits data into train/val/test sets.\n",
        "    \"\"\"\n",
        "    print(\"Processing and converting data...\")\n",
        "\n",
        "    # Initialize outputs\n",
        "    train_data = []\n",
        "    val_data = []\n",
        "    test_data = []\n",
        "    dropped = 0\n",
        "\n",
        "    # Create directories if saving splits\n",
        "    if save_path:\n",
        "        os.makedirs(save_path, exist_ok=True)\n",
        "        os.makedirs(os.path.join(save_path, 'docs'), exist_ok=True)\n",
        "        train_fp = open(os.path.join(save_path, 'train.jsonl'), 'w')\n",
        "        val_fp = open(os.path.join(save_path, 'val.jsonl'), 'w')\n",
        "        test_fp = open(os.path.join(save_path, 'test.jsonl'), 'w')\n",
        "\n",
        "    for key, value in tqdm(data.items()):\n",
        "        try:\n",
        "            # Extract labels\n",
        "            labels = [1 if annot[\"label\"] in ['hatespeech', 'offensive'] else 0\n",
        "                     for annot in value[\"annotators\"]]\n",
        "\n",
        "            # Process rationales\n",
        "            rationales = value.get(\"rationales\", [])\n",
        "            aggregated_rationale = aggregate_rationales(\n",
        "                rationales, labels, len(value[\"post_tokens\"]), drop_abnormal=drop_abnormal\n",
        "            )\n",
        "\n",
        "            if aggregated_rationale is None:\n",
        "                dropped += 1\n",
        "                continue\n",
        "\n",
        "            inputs = align_rationales(value['post_tokens'], aggregated_rationale, tokenizer)\n",
        "\n",
        "            # Calculate labels\n",
        "            hard_label = Counter(labels).most_common(1)[0][0]\n",
        "            soft_label = sum(labels) / len(labels)\n",
        "\n",
        "            # Determine target groups (mentioned at least 3 times)\n",
        "            target_groups = [t for annot in value['annotators'] for t in annot['target']]\n",
        "            filtered_targets = [k for k, v in Counter(target_groups).items() if v > 2]\n",
        "\n",
        "            # Create processed entry\n",
        "            processed_entry = {\n",
        "                'post_id': key,\n",
        "                'input_ids': inputs['input_ids'],\n",
        "                'attention_mask': inputs['attention_mask'],\n",
        "                'rationales': inputs['rationales'],\n",
        "                'raw_text': \" \".join(value['post_tokens']),\n",
        "                'hard_label': hard_label,\n",
        "                'soft_label': soft_label,\n",
        "                'target_groups': filtered_targets\n",
        "            }\n",
        "\n",
        "            # Convert to ERASER format if it's hateful/offensive content\n",
        "            if hard_label == 1 and save_path:\n",
        "                input_ids_list = inputs['input_ids'].squeeze().tolist()\n",
        "                rationales_list = inputs['rationales'].squeeze().ceil().int().tolist()\n",
        "\n",
        "                # Build evidences\n",
        "                evidences = []\n",
        "                indexes = sorted([i for i, each in enumerate(rationales_list) if each == 1])\n",
        "                for span in find_ranges(indexes):\n",
        "                    if isinstance(span, int):\n",
        "                        start, end = span, span + 1\n",
        "                    else:\n",
        "                        start, end = span[0], span[1] + 1\n",
        "\n",
        "                    evidences.append({\n",
        "                        \"docid\": key,\n",
        "                        \"end_sentence\": -1,\n",
        "                        \"end_token\": end,\n",
        "                        \"start_sentence\": -1,\n",
        "                        \"start_token\": start,\n",
        "                        \"text\": ' '.join([str(x) for x in input_ids_list[start:end]])\n",
        "                    })\n",
        "\n",
        "                eraser_entry = {\n",
        "                    'annotation_id': key,\n",
        "                    'classification': str(hard_label),\n",
        "                    'evidences': [evidences],\n",
        "                    'query': \"What is the class?\",\n",
        "                    'query_type': None\n",
        "                }\n",
        "\n",
        "                # Save document\n",
        "                with open(os.path.join(save_path, 'docs', key), 'w') as fp:\n",
        "                    fp.write(' '.join([str(x) for x in input_ids_list if x > 0]))\n",
        "\n",
        "                # Write to appropriate split\n",
        "                if key in post_id_divisions['train']:\n",
        "                    train_fp.write(json.dumps(eraser_entry) + '\\n')\n",
        "                elif key in post_id_divisions['val']:\n",
        "                    val_fp.write(json.dumps(eraser_entry) + '\\n')\n",
        "                elif key in post_id_divisions['test']:\n",
        "                    test_fp.write(json.dumps(eraser_entry) + '\\n')\n",
        "\n",
        "            # Add to appropriate split list\n",
        "            if key in post_id_divisions['train']:\n",
        "                train_data.append(processed_entry)\n",
        "            elif key in post_id_divisions['val']:\n",
        "                val_data.append(processed_entry)\n",
        "            elif key in post_id_divisions['test']:\n",
        "                test_data.append(processed_entry)\n",
        "\n",
        "        except Exception as e:\n",
        "            dropped += 1\n",
        "            print(f\"Error processing {key}: {e}\")\n",
        "\n",
        "    if save_path:\n",
        "        train_fp.close()\n",
        "        val_fp.close()\n",
        "        test_fp.close()\n",
        "\n",
        "    print(f\"Train: {len(train_data)}, Val: {len(val_data)}, Test: {len(test_data)}, Dropped: {dropped}\")\n",
        "\n",
        "    return {\n",
        "        'train': train_data,\n",
        "        'val': val_data,\n",
        "        'test': test_data\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "43NMFuwv62Q4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366,
          "referenced_widgets": [
            "9ebb232a0b9649789495ec0c8a519e1a",
            "d755aa71d5a1461cb932778dc5deaf52",
            "30b2fc6ceebb4da083da94a206cdc32e",
            "f872f5f79a884eaca0f07e00bc1a2eca",
            "1ac419c560cb44149a6b54d2c256e1aa",
            "e40ac1cd6e2344ca8d8ede79e35791cb",
            "000d556ee6514ee4a84badad7cce732d",
            "222943fb54e5426d89b2d50051135306",
            "6168a397e2994c0289bf267be8bff0e7",
            "ed5921c0c4994155a08f060c98de0228",
            "0466e0260a5f4774af5c4b76b495d15f",
            "838074dd3de54ba28ff6888918598f54",
            "4108f81df6394bb1956157ffa08d5b19",
            "6dcba1504ae6444a9f8b1eb310b1c0ba",
            "2149e2e0f02d44be95617cf8c57dc8e8",
            "955bc92f2d6141949e91648f36940ed9",
            "01f0ad4c4f884d5ab4bae38a6d5b8f12",
            "658618d078554305b8d86b1a882f9ae9",
            "e8ac744ceb77418baeea1a40b2213bcf",
            "036b2b14b78a46beb319571bc408bf94",
            "42983923305347428db2a3acb7d8fa98",
            "95a276c276594117b51a9ed4361ad4d3",
            "d514fc32c3b14890866e4e340eb65ff7",
            "27983a49e3af4cc799d4b47291fa1d2e",
            "a8e708b18cdb40078565f90ca9aec543",
            "2ec85da142384b2499d0d5cec5516cb6",
            "c6a377b8f1994f7ea5853c5ea47c072c",
            "7cbf881bf084483594c4982f61ddee22",
            "555f094a99a448ed92206c2b2e40a48e",
            "e09f9d26e7554480be056472a37727b9",
            "85183a453759410090f84533fd6a94d4",
            "658de63d6be44d67abae51be45398ed2",
            "5a27b15a14d0485d9bec469f9fbc8297",
            "b0bf17679f9b4e74be363704aa7bf120",
            "624199c0f53447e7b4e32d371e4016eb",
            "84892e9a9e0f4dc8b74b975392618dc6",
            "662feb5d7e6946a18fba9a23145fc034",
            "1be37b84a1ae427fa1a636cdd98a6fd7",
            "199507e489064a3fb942faf130588a3d",
            "9cae2ba8f08e46108daebf7cf8de1618",
            "d4a64806bec5463c80471893e695e179",
            "df22cfc7b3de4d5595f10e4250223801",
            "0e1b35c05596461d8b8005b6a62506da",
            "e31b9ffcc76a4885a4b32e84a2fab004"
          ]
        },
        "id": "43NMFuwv62Q4",
        "outputId": "4dee724c-5765-4690-8a36-b839de5a29b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing and converting data...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 37%|███▋      | 7428/20148 [00:09<00:17, 736.73it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING: NEED TO TRUNCATE\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            " 89%|████████▉ | 17985/20148 [00:26<00:03, 629.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error processing 24439295_gab: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (2,) + inhomogeneous part.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 20148/20148 [00:29<00:00, 675.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 15382, Val: 1922, Test: 1924, Dropped: 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "with open(data_path, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "with open('Data/post_id_divisions.json') as file:\n",
        "    post_id_divisions = json.load(file)\n",
        "\n",
        "# Process everything in one pass\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "splits = process_and_convert_data(\n",
        "    data=data,\n",
        "    tokenizer=tokenizer,\n",
        "    post_id_divisions=post_id_divisions,\n",
        "    save_path='Data/explanations/',\n",
        "    drop_abnormal=False\n",
        ")\n",
        "\n",
        "# Access splits directly\n",
        "train_data = splits['train']\n",
        "val_data = splits['val']\n",
        "test_data = splits['test']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "87c63ecf",
      "metadata": {
        "id": "87c63ecf"
      },
      "outputs": [],
      "source": [
        "from HateDataset import HateDataset\n",
        "\n",
        "# Create datasets with pre-tokenized data\n",
        "train_dataset = HateDataset(data=train_data)\n",
        "val_dataset = HateDataset(data=val_data)\n",
        "test_dataset = HateDataset(data=test_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f4887f43",
      "metadata": {
        "id": "f4887f43"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False) # Use shuffle=False for validation\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False) # Use shuffle=False for testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "2f509bc0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a035f097a51b4e59b5f240e08a959dda",
            "3049e83d2f184ca2b7657db368f49a1d",
            "ee1a88ab031c4327ad18b3a48929bab0",
            "6ea187bc06d84f018673dfc981571a6e",
            "7c2b46d111114553b94fcd2351e9f7bd",
            "7f9d7443369a4cbf97048d9952396b58",
            "2642b7f8abf24bfabef78ef219768375",
            "e8ab7c8eca5442a88af511856f58b271",
            "effb7cca8a914550a68775728bdeb881",
            "606c6a11e2564fc68ec19f701e48c625",
            "c46fe7448c5a46428cd007265094139f"
          ]
        },
        "id": "2f509bc0",
        "outputId": "9e15bf1c-951e-4907-e4fd-4642209262cd"
      },
      "outputs": [],
      "source": [
        "from HateClassifier import HateClassifier\n",
        "model = HateClassifier(config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "aab913f8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aab913f8",
        "outputId": "2a659537-3fb8-436c-ef14-d43246861cff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on device: cuda\n",
            "Model: distilbert-base-uncased\n",
            "Epochs: 2\n",
            "Batch size: 32\n",
            "Gradient accumulation steps: 1\n",
            "Effective batch size: 32\n",
            "Learning rate: 1e-05\n",
            "Mixed precision (AMP): True\n",
            "Gradient clipping: 1.0\n",
            "============================================================\n",
            "\n",
            "Epoch 1/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 481/481 [01:38<00:00,  4.87batch/s, loss=0.519]\n",
            "Evaluating: 100%|██████████| 61/61 [00:03<00:00, 18.05batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 1 Summary:\n",
            "  Train Loss: 0.5186\n",
            "  Val Loss:   0.4852\n",
            "  Val Acc:    0.7570\n",
            "  Val F1:     0.7552\n",
            "  ✓ New best model saved! (F1: 0.7552)\n",
            "\n",
            "Epoch 2/2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training: 100%|██████████| 481/481 [01:37<00:00,  4.92batch/s, loss=0.422]\n",
            "Evaluating: 100%|██████████| 61/61 [00:03<00:00, 18.07batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch 2 Summary:\n",
            "  Train Loss: 0.4217\n",
            "  Val Loss:   0.4749\n",
            "  Val Acc:    0.7674\n",
            "  Val F1:     0.7615\n",
            "  ✓ New best model saved! (F1: 0.7615)\n",
            "\n",
            "============================================================\n",
            "Training completed!\n",
            "Best F1 Score: 0.7615\n",
            "Training history saved to: ./checkpoints/training_history.json\n"
          ]
        }
      ],
      "source": [
        "history = model.train(train_dataloader=train_loader, val_dataloader=val_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4103f12a",
      "metadata": {
        "id": "4103f12a"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "c2d21fc2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2d21fc2",
        "outputId": "dff170fc-bccf-4e00-e107-4b436ab1d221"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running inference on 61 batches...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 61/61 [00:03<00:00, 16.77batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Test Results:\n",
            "  Test Loss:     0.4589\n",
            "  Test Accuracy: 0.7937\n",
            "  Test F1:       0.7868\n",
            "============================================================\n",
            "Results saved to prediction_results.pkl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "result = model.predict(test_dataloader=test_loader, return_attentions=True)\n",
        "\n",
        "# Save the result to a file\n",
        "with open('prediction_results.pkl', 'wb') as f:\n",
        "  pickle.dump(result, f)\n",
        "\n",
        "print(\"Results saved to prediction_results.pkl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0caf972c",
      "metadata": {
        "id": "0caf972c"
      },
      "source": [
        "## Bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "40280671",
      "metadata": {
        "id": "40280671"
      },
      "outputs": [],
      "source": [
        "def get_bias_evaluation_samples(data, method, group):\n",
        "    \"\"\"\n",
        "    Get positive and negative sample IDs for bias evaluation based on method and group\n",
        "\n",
        "    Args:\n",
        "        data: list of data entries\n",
        "        method: Bias evaluation method ('subgroup', 'bpsn', or 'bnsp')\n",
        "        group: Target group to evaluate\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (positive_ids, negative_ids)\n",
        "    \"\"\"\n",
        "    positive_ids = []\n",
        "    negative_ids = []\n",
        "\n",
        "    for idx, row in enumerate(data):\n",
        "        target_groups = row['target_groups']\n",
        "        if target_groups is None:\n",
        "            continue\n",
        "\n",
        "        is_in_group = group in target_groups\n",
        "\n",
        "        # Convert various label formats to binary toxic/non-toxic\n",
        "        if 'hard_label' in row:\n",
        "            is_toxic = row['hard_label'] == 1\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "        if method == 'subgroup':\n",
        "            # Only consider samples mentioning the group\n",
        "            if is_in_group:\n",
        "                if is_toxic:\n",
        "                    positive_ids.append(idx)\n",
        "                else:\n",
        "                    negative_ids.append(idx)\n",
        "\n",
        "        elif method == 'bpsn':\n",
        "            # Compare non-toxic posts mentioning the group with toxic posts NOT mentioning the group\n",
        "            if is_in_group and not is_toxic:\n",
        "                negative_ids.append(idx)\n",
        "            elif not is_in_group and is_toxic:\n",
        "                positive_ids.append(idx)\n",
        "\n",
        "        elif method == 'bnsp':\n",
        "            # Compare toxic posts mentioning the group with non-toxic posts NOT mentioning the group\n",
        "            if is_in_group and is_toxic:\n",
        "                positive_ids.append(idx)\n",
        "            elif not is_in_group and not is_toxic:\n",
        "                negative_ids.append(idx)\n",
        "\n",
        "    return positive_ids, negative_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "79edfa7d",
      "metadata": {
        "id": "79edfa7d"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "def calculate_gmb_metrics(\n",
        "    test_data: List[Dict[str, Any]],\n",
        "    probabilities: np.ndarray,\n",
        "    target_groups: List[str]\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculate GMB (Generalized Mean of Bias) AUC metrics from model predictions\n",
        "\n",
        "    Args:\n",
        "        probabilities: Model's probability outputs\n",
        "        test_data: List of test data entries\n",
        "        target_groups: List of target groups to evaluate\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with GMB metrics\n",
        "    \"\"\"\n",
        "    # Create mappings from post_id to predictions and ground truth\n",
        "    prediction_scores = defaultdict(lambda: defaultdict(dict))\n",
        "    ground_truth = {}\n",
        "\n",
        "    for idx, row in enumerate(test_data):\n",
        "        prediction_scores[idx] = probabilities[idx, 1]\n",
        "        ground_truth[idx] = row['hard_label']\n",
        "\n",
        "    # Calculate metrics for each target group and method\n",
        "    bias_metrics = {}\n",
        "    methods = ['subgroup', 'bpsn', 'bnsp']\n",
        "\n",
        "    for method in methods:\n",
        "        bias_metrics[method] = {}  # Initialize nested dictionary for each method\n",
        "        for group in target_groups:\n",
        "            # Get positive and negative samples based on the method\n",
        "            positive_ids, negative_ids = get_bias_evaluation_samples(test_data, method, group)\n",
        "\n",
        "            if len(positive_ids) == 0 or len(negative_ids) == 0:\n",
        "                print(f\"Skipping {method} for group {group}: no samples found\")\n",
        "                continue  # Skip if no samples for this group/method\n",
        "\n",
        "            # Collect ground truth and predictions\n",
        "            y_true = []\n",
        "            y_score = []\n",
        "\n",
        "            for post_id in positive_ids:\n",
        "                if post_id in ground_truth and post_id in prediction_scores:\n",
        "                    y_true.append(ground_truth[post_id])\n",
        "                    y_score.append(prediction_scores[post_id])\n",
        "\n",
        "            for post_id in negative_ids:\n",
        "                if post_id in ground_truth and post_id in prediction_scores:\n",
        "                    y_true.append(ground_truth[post_id])\n",
        "                    y_score.append(prediction_scores[post_id])\n",
        "\n",
        "            # Calculate AUC if we have enough samples with both classes\n",
        "            if len(y_true) > 10 and len(set(y_true)) > 1:\n",
        "                try:\n",
        "                    auc = roc_auc_score(y_true, y_score)\n",
        "                    bias_metrics[method][group] = auc\n",
        "                except ValueError:\n",
        "                    print(f\"Could not compute AUC for {method} and group {group} due to ValueError\")\n",
        "                    pass\n",
        "\n",
        "    # Calculate GMB for each method\n",
        "    gmb_metrics = {}\n",
        "    power = -5  # Power parameter for generalized mean\n",
        "\n",
        "    for method in methods:\n",
        "        if not bias_metrics[method]:\n",
        "            continue\n",
        "\n",
        "        scores = list(bias_metrics[method].values())\n",
        "        if not scores:\n",
        "            continue\n",
        "\n",
        "        # Calculate generalized mean with p=-5\n",
        "        power_mean = np.mean([score ** power for score in scores]) ** (1/power)\n",
        "        gmb_metrics[f'GMB-{method.upper()}-AUC'] = power_mean\n",
        "\n",
        "    # Calculate a combined GMB score that includes all methods\n",
        "    all_scores = []\n",
        "    for method in methods:\n",
        "        all_scores.extend(list(bias_metrics[method].values()))\n",
        "\n",
        "    if all_scores:\n",
        "        gmb_metrics['GMB-COMBINED-AUC'] = np.mean([score ** power for score in all_scores]) ** (1/power)\n",
        "\n",
        "    return gmb_metrics, bias_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "ys17p1JQ_IUD",
      "metadata": {
        "id": "ys17p1JQ_IUD"
      },
      "outputs": [],
      "source": [
        "from itertools import chain\n",
        "\n",
        "all_target_groups = list(chain.from_iterable(d['target_groups'] for d in train_data + val_data + test_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "d156cab7",
      "metadata": {
        "id": "d156cab7"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "# Get top 10 most common target groups in the full dataset\n",
        "\n",
        "# Remove None\n",
        "all_target_groups = [group for group in all_target_groups if group != 'None' and group != 'Other']\n",
        "counter = Counter(all_target_groups)\n",
        "\n",
        "n_common = 10\n",
        "bias_target_groups = [tg[0] for tg in counter.most_common(n_common)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "ffc9f3e0",
      "metadata": {
        "id": "ffc9f3e0"
      },
      "outputs": [],
      "source": [
        "gmb_metrics, bias_details = calculate_gmb_metrics(\n",
        "  test_data=test_data,\n",
        "  probabilities=result['probabilities'],\n",
        "  target_groups=bias_target_groups\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "9c02645d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9c02645d",
        "outputId": "27ecdfff-4a9a-4c79-eb76-9621b5b444f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GMB-Metrics\n",
            "GMB-SUBGROUP-AUC: 0.8168437955918244\n",
            "GMB-BPSN-AUC: 0.783663273567452\n",
            "GMB-BNSP-AUC: 0.7816579558593832\n",
            "GMB-COMBINED-AUC: 0.7931047309021546\n"
          ]
        }
      ],
      "source": [
        "print('GMB-Metrics')\n",
        "for key, value in gmb_metrics.items():\n",
        "  print(f'{key}: {value}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "814a2279",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "814a2279",
        "outputId": "b9d659e6-97ee-4404-b0e8-c4b188f035df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bias Details\n",
            "\n",
            "Metrics: subgroup\n",
            "African: 0.91324200913242\n",
            "Jewish: 0.806390977443609\n",
            "Islam: 0.9649532710280373\n",
            "Homosexual: 0.8244047619047619\n",
            "Women: 0.6666666666666666\n",
            "Refugee: 0.7898989898989898\n",
            "Arab: 0.8620689655172413\n",
            "Hispanic: 1.0\n",
            "Asian: 0.9285714285714286\n",
            "Caucasian: 0.7666666666666666\n",
            "\n",
            "Metrics: bpsn\n",
            "African: 0.7222824124232576\n",
            "Jewish: 0.6082755203171457\n",
            "Islam: 0.9432367149758454\n",
            "Homosexual: 0.8163430420711975\n",
            "Women: 0.8074313906876349\n",
            "Refugee: 0.9458070333633906\n",
            "Arab: 0.7331536388140162\n",
            "Hispanic: 0.943649373881932\n",
            "Asian: 0.9787234042553191\n",
            "Caucasian: 0.958740017746229\n",
            "\n",
            "Metrics: bnsp\n",
            "African: 0.940363772779761\n",
            "Jewish: 0.9401202234377717\n",
            "Islam: 0.891135910434135\n",
            "Homosexual: 0.884639189437007\n",
            "Women: 0.8057398174750718\n",
            "Refugee: 0.6828651574414286\n",
            "Arab: 0.9395337542496357\n",
            "Hispanic: 0.9778862569355528\n",
            "Asian: 0.8391713919882935\n",
            "Caucasian: 0.5954700854700854\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print('Bias Details')\n",
        "print()\n",
        "for key, entry in bias_details.items():\n",
        "  print(f\"Metrics: {key}\")\n",
        "  for subgroup, value in entry.items():\n",
        "    print(f'{subgroup}: {value}')\n",
        "  print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39b1ac3f",
      "metadata": {
        "id": "39b1ac3f"
      },
      "source": [
        "# XAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "id": "8toApSDiIVlT",
      "metadata": {
        "id": "8toApSDiIVlT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from typing import List, Dict, Tuple\n",
        "import json\n",
        "from sklearn.metrics import (\n",
        "    precision_recall_curve,\n",
        "    auc,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "\n",
        "\n",
        "class FaithfulnessMetrics:\n",
        "    \"\"\"\n",
        "    Compute faithfulness metrics using the model's existing predict() method.\n",
        "    Creates modified datasets and uses DataLoader for efficient batched processing.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, tokenizer, dataset_class, batch_size=32):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.dataset_class = dataset_class\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        # Get special token IDs\n",
        "        self.special_token_ids = {\n",
        "            tokenizer.cls_token_id,\n",
        "            tokenizer.sep_token_id,\n",
        "        }\n",
        "        self.special_token_ids = {x for x in self.special_token_ids if x is not None}\n",
        "\n",
        "    def compute_all_metrics(\n",
        "        self,\n",
        "        test_data: List[Dict],  # Your original test data\n",
        "        test_results: Dict,  # Results from prediction\n",
        "        k: int = 5,  # Number of top tokens to consider\n",
        "    ) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Compute all ERASER metrics efficiently using DataLoader approach\n",
        "\n",
        "        Args:\n",
        "            test_data: List of test instances (each with input_ids, attention_mask, rationales, labels)\n",
        "            test_results: List of dictionaries containing attention scores for each instance\n",
        "\n",
        "        Returns:\n",
        "            Dictionary with all metrics\n",
        "        \"\"\"\n",
        "        print(\"Computing ERASER metrics using DataLoader approach...\")\n",
        "\n",
        "        # Extract lists for easier processing\n",
        "        input_ids_list = [item[\"input_ids\"] for item in test_data]\n",
        "        attention_masks_list = [item[\"attention_mask\"] for item in test_data]\n",
        "        human_rationales = [item[\"rationales\"] for item in test_data]\n",
        "        attention_scores = [item for item in test_results[\"attentions\"]]\n",
        "\n",
        "        # 1. Extract top-k as hard predictions\n",
        "        hard_predictions = self._extract_top_k_tokens(\n",
        "            attention_scores, attention_masks_list, input_ids_list, k\n",
        "        )\n",
        "\n",
        "        hard_rationale_predictions, soft_rationale_predictions = self._convert_attention_to_evidence_format(input_ids_list, attention_scores, hard_predictions)\n",
        "\n",
        "        # 2. PLAUSIBILITY METRICS\n",
        "        print(\"\\n[1/3] Computing plausibility metrics...\")\n",
        "        auprc = self._compute_auprc(\n",
        "            attention_scores, human_rationales, attention_masks_list, input_ids_list\n",
        "        )\n",
        "        token_f1, token_prec, token_rec = self._compute_token_f1(\n",
        "            hard_predictions, human_rationales, attention_masks_list\n",
        "        )\n",
        "\n",
        "        # 3. FAITHFULNESS METRICS\n",
        "        print(\"[2/3] Computing comprehensiveness scores...\")\n",
        "        raw_comprehensiveness, comprehensiveness_scores = (\n",
        "            self._compute_comprehensiveness(test_data, test_results, hard_predictions)\n",
        "        )\n",
        "\n",
        "        print(\"[3/3] Computing sufficiency scores...\")\n",
        "        raw_sufficiency, sufficiency_scores = self._compute_sufficiency(\n",
        "            test_data, test_results, hard_predictions\n",
        "        )\n",
        "        \n",
        "        # 4. Convert to eraser format\n",
        "        results_eraser = self._convert_result_to_eraser_format(test_results, hard_rationale_predictions, soft_rationale_predictions, raw_sufficiency, raw_comprehensiveness)\n",
        "        # Convert to JSONL format\n",
        "        jsonl_output = '\\n'.join([json.dumps(entry) for entry in results_eraser])\n",
        "        with open('Data/eraser_formatted_results.jsonl', 'w') as f:\n",
        "            f.write(jsonl_output)\n",
        "\n",
        "        return {\n",
        "            # Plausibility\n",
        "            \"auprc\": auprc,\n",
        "            \"token_f1\": token_f1,\n",
        "            \"token_precision\": token_prec,\n",
        "            \"token_recall\": token_rec,\n",
        "            # Faithfulness\n",
        "            \"comprehensiveness\": np.mean(comprehensiveness_scores),\n",
        "            \"sufficiency\": np.mean(sufficiency_scores),\n",
        "            # Additional\n",
        "            \"avg_rationale_length\": k,\n",
        "        }\n",
        "\n",
        "    def _convert_attention_to_evidence_format(self, input_ids_list, attention_scores, hard_predictions):\n",
        "        # 2. Collect evidence\n",
        "        hard_rationale_predictions = []\n",
        "        for idx, hp in enumerate(hard_predictions):\n",
        "            evidences = []\n",
        "            indexes = sorted([i for i, each in enumerate(hp.tolist()) if each == 1])\n",
        "            for span in find_ranges(indexes):\n",
        "                if isinstance(span, int):\n",
        "                    start, end = span, span + 1\n",
        "                else:\n",
        "                    start, end = span[0], span[1] + 1\n",
        "\n",
        "                evidences.append({\n",
        "                    \"start_token\": start,\n",
        "                    \"end_token\": end,\n",
        "                })\n",
        "            hard_rationale_predictions.append(evidences)\n",
        "\n",
        "        soft_rationale_predictions = []\n",
        "        for att in attention_scores:\n",
        "            pred = [x for x in att if x > 0]\n",
        "            soft_rationale_predictions.append(pred)\n",
        "        \n",
        "        return hard_rationale_predictions, soft_rationale_predictions\n",
        "    \n",
        "    def _convert_result_to_eraser_format(\n",
        "        self, \n",
        "        test_result: Dict,\n",
        "        hard_rationale_predictions,\n",
        "        soft_rationale_predictions,\n",
        "        sufficiency_scores: np.ndarray,\n",
        "        comprehensiveness_scores: np.ndarray,\n",
        "    ):\n",
        "        all_entries = []\n",
        "        for idx, data in enumerate(test_result[\"post_id\"]):\n",
        "            entry = {\n",
        "            'annotation_id': data,\n",
        "            'classification': str(int(test_result[\"predictions\"][idx])),\n",
        "            'classification_scores': {\n",
        "                0: float(test_result[\"probabilities\"][idx][0]),\n",
        "                1: float(test_result[\"probabilities\"][idx][1]),\n",
        "            },\n",
        "            'rationales': [\n",
        "                {\n",
        "                    \"docid\": data,\n",
        "                    \"hard_rationale_predictions\": hard_rationale_predictions[idx],\n",
        "                    \"soft_rationale_predictions\": [float(x) for x in soft_rationale_predictions[idx]],\n",
        "                }\n",
        "            ],\n",
        "            'sufficiency_classification_scores': {\n",
        "                0: float(sufficiency_scores[idx][0]),\n",
        "                1: float(sufficiency_scores[idx][1])\n",
        "            },\n",
        "            'comprehensiveness_classification_scores': {\n",
        "                0: float(comprehensiveness_scores[idx][0]),\n",
        "                1: float(comprehensiveness_scores[idx][1])\n",
        "            }\n",
        "            }\n",
        "            all_entries.append(entry)\n",
        "    \n",
        "        return all_entries\n",
        "    \n",
        "    def _calculate_average_rationale_length(\n",
        "        self,\n",
        "        human_rationales: List[torch.Tensor],\n",
        "        attention_masks_list: List[torch.Tensor],\n",
        "        input_ids_list: List[torch.Tensor],\n",
        "    ) -> int:\n",
        "        \"\"\"Calculate average number of content rationale tokens\"\"\"\n",
        "        lengths = []\n",
        "        for idx, (rat, mask) in enumerate(zip(human_rationales, attention_masks_list)):\n",
        "            valid_positions = mask.bool().cpu().numpy().flatten()\n",
        "\n",
        "            # Exclude special tokens\n",
        "            input_ids = input_ids_list[idx].cpu().numpy().flatten()\n",
        "            is_special = np.isin(input_ids, list(self.special_token_ids))\n",
        "            content_positions = valid_positions & ~is_special\n",
        "\n",
        "            rat_count = (rat.cpu().numpy().flatten()[content_positions] == 1).sum()\n",
        "            lengths.append(rat_count)\n",
        "\n",
        "        return max(1, int(np.mean(lengths)))\n",
        "\n",
        "    def _extract_top_k_tokens(\n",
        "        self,\n",
        "        attention_scores: List[np.ndarray],\n",
        "        attention_masks_list: List[torch.Tensor],\n",
        "        input_ids_list: List[torch.Tensor],\n",
        "        k: int,\n",
        "    ) -> List[np.ndarray]:\n",
        "        \"\"\"Extract top-k content tokens as hard predictions\"\"\"\n",
        "        hard_predictions = []\n",
        "\n",
        "        for idx, (attn, mask) in enumerate(zip(attention_scores, attention_masks_list)):\n",
        "            pred_mask = np.zeros_like(attn, dtype=int)\n",
        "            valid_positions = mask.bool().cpu().numpy().flatten()\n",
        "\n",
        "            # Exclude special tokens\n",
        "            input_ids = input_ids_list[idx].cpu().numpy().flatten()\n",
        "            is_special = np.isin(input_ids, list(self.special_token_ids))\n",
        "            content_positions = valid_positions & ~is_special\n",
        "\n",
        "            content_attn = attn[content_positions]\n",
        "\n",
        "            if k > 0 and len(content_attn) > 0:\n",
        "                k_actual = min(k, len(content_attn))\n",
        "                top_k_within_content = np.argsort(content_attn)[-k_actual:]\n",
        "                content_indices = np.where(content_positions)[0]\n",
        "                top_k_indices = content_indices[top_k_within_content]\n",
        "                pred_mask[top_k_indices] = 1\n",
        "\n",
        "            hard_predictions.append(pred_mask)\n",
        "\n",
        "        return hard_predictions\n",
        "\n",
        "    def _compute_auprc(\n",
        "        self,\n",
        "        attention_scores: List[np.ndarray],\n",
        "        human_rationales: List[torch.Tensor],\n",
        "        attention_masks_list: List[torch.Tensor],\n",
        "        input_ids_list: List[torch.Tensor],\n",
        "    ) -> float:\n",
        "        \"\"\"Compute AUPRC for soft attention scores\"\"\"\n",
        "        all_scores = []\n",
        "        all_labels = []\n",
        "\n",
        "        for idx, (attn, rat, mask) in enumerate(\n",
        "            zip(attention_scores, human_rationales, attention_masks_list)\n",
        "        ):\n",
        "            valid_positions = mask.bool().cpu().numpy().flatten()\n",
        "\n",
        "            # Exclude special tokens\n",
        "            input_ids = input_ids_list[idx].cpu().numpy().flatten()\n",
        "            is_special = np.isin(input_ids, list(self.special_token_ids))\n",
        "            content_positions = valid_positions & ~is_special\n",
        "\n",
        "            all_scores.extend(attn[content_positions].tolist())\n",
        "            all_labels.extend(\n",
        "                rat.cpu().numpy().flatten()[content_positions].astype(int).tolist()\n",
        "            )\n",
        "\n",
        "        all_scores = np.array(all_scores, dtype=float)\n",
        "        all_labels = np.array(all_labels, dtype=int)\n",
        "\n",
        "        if len(np.unique(all_labels)) < 2:\n",
        "            print(f\"Warning: Only one class in labels: {np.unique(all_labels)}\")\n",
        "            return 0.0\n",
        "\n",
        "        precision, recall, _ = precision_recall_curve(all_labels, all_scores)\n",
        "        return auc(recall, precision)\n",
        "\n",
        "    def _compute_token_f1(\n",
        "        self,\n",
        "        hard_predictions: List[np.ndarray],\n",
        "        human_rationales: List[torch.Tensor],\n",
        "        attention_masks_list: List[torch.Tensor],\n",
        "    ) -> Tuple[float, float, float]:\n",
        "        \"\"\"Compute token-level F1, Precision, Recall\"\"\"\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        for pred, rat, mask in zip(\n",
        "            hard_predictions, human_rationales, attention_masks_list\n",
        "        ):\n",
        "            valid_positions = mask.bool().cpu().numpy().flatten()\n",
        "            all_preds.extend(pred[valid_positions].astype(int).tolist())\n",
        "            all_labels.extend(\n",
        "                rat.cpu().numpy().flatten()[valid_positions].astype(int).tolist()\n",
        "            )\n",
        "\n",
        "        all_preds = np.array(all_preds, dtype=int)\n",
        "        all_labels = np.array(all_labels, dtype=int)\n",
        "\n",
        "        f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
        "        precision = precision_score(all_labels, all_preds, zero_division=0)\n",
        "        recall = recall_score(all_labels, all_preds, zero_division=0)\n",
        "\n",
        "        return f1, precision, recall\n",
        "\n",
        "    def _compute_comprehensiveness(\n",
        "        self,\n",
        "        test_data: List[Dict],\n",
        "        test_results: Dict,\n",
        "        hard_predictions: List[np.ndarray],\n",
        "    ) -> Tuple[float, List[float]]:\n",
        "        \"\"\"\n",
        "        Compute comprehensiveness: how much does REMOVING rationales hurt?\n",
        "        Uses DataLoader approach for efficiency\n",
        "        \"\"\"\n",
        "        # Create modified dataset (remove rationales from attention mask)\n",
        "        modified_data = []\n",
        "        for item, rationale_mask in zip(test_data, hard_predictions):\n",
        "            modified_item = self._create_comprehensiveness_instance(\n",
        "                item, rationale_mask\n",
        "            )\n",
        "            modified_data.append(modified_item)\n",
        "\n",
        "        # Create DataLoader\n",
        "        modified_dataset = self.dataset_class(modified_data)\n",
        "        modified_loader = DataLoader(\n",
        "            modified_dataset, batch_size=self.batch_size, shuffle=False\n",
        "        )\n",
        "\n",
        "        # Get predictions using model's predict method\n",
        "        results = self.model.predict(modified_loader, return_attentions=False)\n",
        "        modified_probs = results[\"probabilities\"]\n",
        "\n",
        "        # Calculate comprehensiveness scores\n",
        "        comprehensiveness_scores = []\n",
        "        for idx, (prob, label) in enumerate(zip(test_results[\"probabilities\"], test_results['labels'])):\n",
        "            original_prob = prob[\n",
        "                label\n",
        "            ]  # Probability from normal prediction process for the label\n",
        "            modified_prob = modified_probs[idx][label]\n",
        "\n",
        "            # Comprehensiveness = original - modified (higher is better)\n",
        "            comp_score = original_prob - modified_prob\n",
        "            comprehensiveness_scores.append(comp_score)\n",
        "\n",
        "        return modified_probs, comprehensiveness_scores\n",
        "\n",
        "    def _compute_sufficiency(\n",
        "        self,\n",
        "        test_data: List[Dict],\n",
        "        test_results: Dict,\n",
        "        hard_predictions: List[np.ndarray],\n",
        "    ) -> Tuple[List[float], List[float]]:\n",
        "        \"\"\"\n",
        "        Compute sufficiency: how well do ONLY rationales predict?\n",
        "        Uses DataLoader approach for efficiency\n",
        "        \"\"\"\n",
        "        # Create modified dataset (keep only rationales in attention mask)\n",
        "        modified_data = []\n",
        "        for item, rationale_mask in zip(test_data, hard_predictions):\n",
        "            modified_item = self._create_sufficiency_instance(item, rationale_mask)\n",
        "            modified_data.append(modified_item)\n",
        "\n",
        "        # Create DataLoader\n",
        "        modified_dataset = self.dataset_class(modified_data)\n",
        "        modified_loader = DataLoader(\n",
        "            modified_dataset, batch_size=self.batch_size, shuffle=False\n",
        "        )\n",
        "\n",
        "        # Get predictions using model's predict method\n",
        "        results = self.model.predict(modified_loader, return_attentions=False)\n",
        "        modified_probs = results[\"probabilities\"]\n",
        "\n",
        "        # Calculate sufficiency scores\n",
        "        sufficiency_scores = []\n",
        "        for idx, (prob, label) in enumerate(zip(test_results[\"probabilities\"], test_results['labels'])):\n",
        "            original_prob = prob[\n",
        "                label\n",
        "            ]  # Probability from normal prediction process for the label\n",
        "            modified_prob = modified_probs[idx][label]\n",
        "\n",
        "            # Sufficiency = original - modified (lower/negative is better)\n",
        "            suff_score = original_prob - modified_prob\n",
        "            sufficiency_scores.append(suff_score)\n",
        "\n",
        "        return modified_probs, sufficiency_scores\n",
        "\n",
        "    def _create_comprehensiveness_instance(\n",
        "        self, item: Dict, rationale_mask: np.ndarray\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Create instance for comprehensiveness: REMOVE rationales from attention\n",
        "        Keep: CLS + non-rationale content tokens + SEP\n",
        "        \"\"\"\n",
        "        input_ids = item[\"input_ids\"].cpu().numpy().flatten()\n",
        "        orig_mask = item[\"attention_mask\"].cpu().numpy().flatten()\n",
        "\n",
        "        # Start with original mask\n",
        "        new_mask = orig_mask.copy()\n",
        "\n",
        "        # Zero out rationale positions (except CLS and SEP)\n",
        "        for i in range(len(new_mask)):\n",
        "            if rationale_mask[i] == 1:  # This is a rationale\n",
        "                # Don't mask if it's CLS or SEP\n",
        "                if input_ids[i] not in self.special_token_ids:\n",
        "                    new_mask[i] = 0\n",
        "\n",
        "        return {\n",
        "            \"post_id\": item[\"post_id\"],\n",
        "            \"input_ids\": torch.tensor(input_ids).unsqueeze(0),\n",
        "            \"attention_mask\": torch.tensor(new_mask).unsqueeze(0),\n",
        "            \"rationales\": item[\"rationales\"],\n",
        "            \"hard_label\": item[\"hard_label\"],\n",
        "        }\n",
        "\n",
        "    def _create_sufficiency_instance(\n",
        "        self, item: Dict, rationale_mask: np.ndarray\n",
        "    ) -> Dict:\n",
        "        \"\"\"\n",
        "        Create instance for sufficiency: Keep ONLY rationales in attention\n",
        "        Keep: CLS + rationale tokens + SEP\n",
        "        \"\"\"\n",
        "        input_ids = item[\"input_ids\"].cpu().numpy().flatten()\n",
        "        orig_mask = item[\"attention_mask\"].cpu().numpy().flatten()\n",
        "\n",
        "        # Start with zeros\n",
        "        new_mask = np.zeros_like(orig_mask)\n",
        "\n",
        "        # Always keep CLS and SEP\n",
        "        for i in range(len(new_mask)):\n",
        "            if input_ids[i] in self.special_token_ids:\n",
        "                new_mask[i] = 1\n",
        "\n",
        "        # Keep rationale positions\n",
        "        for i in range(len(new_mask)):\n",
        "            if rationale_mask[i] == 1 and orig_mask[i] == 1:\n",
        "                new_mask[i] = 1\n",
        "\n",
        "        return {\n",
        "            \"post_id\": item[\"post_id\"],\n",
        "            \"input_ids\": torch.tensor(input_ids).unsqueeze(0),\n",
        "            \"attention_mask\": torch.tensor(new_mask).unsqueeze(0),\n",
        "            \"rationales\": item[\"rationales\"],\n",
        "            \"hard_label\": item[\"hard_label\"],\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "id": "fj7X5QtPCPqX",
      "metadata": {
        "id": "fj7X5QtPCPqX"
      },
      "outputs": [],
      "source": [
        "calculator = FaithfulnessMetrics(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset_class=HateDataset,\n",
        "    batch_size=32\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "id": "6b6f1ff4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total test samples: 1924\n",
            "Hate-only samples: 1142\n",
            "First hate sample post_id from test_data: 9867117_gab\n",
            "First hate sample post_id from results: 9867117_gab\n"
          ]
        }
      ],
      "source": [
        "# Skip all entries that are normal (label 0) in test data\n",
        "test_data_hate_only = []\n",
        "test_results_hate_only = {'attentions': [], 'probabilities': [], 'predictions': [], 'post_id': [], 'labels': []}\n",
        "for idx, td in enumerate(test_data):\n",
        "  if td['hard_label'] == 1:\n",
        "    test_data_hate_only.append(td)\n",
        "    test_results_hate_only['attentions'].append(result['attentions'][idx])\n",
        "    test_results_hate_only['probabilities'].append(result['probabilities'][idx])\n",
        "    test_results_hate_only['predictions'].append(result['predictions'][idx])\n",
        "    test_results_hate_only['post_id'].append(result['post_ids'][idx])\n",
        "    test_results_hate_only['labels'].append(result['labels'][idx])\n",
        "\n",
        "# Verify alignment\n",
        "print(f\"Total test samples: {len(test_data)}\")\n",
        "print(f\"Hate-only samples: {len(test_data_hate_only)}\")\n",
        "print(f\"First hate sample post_id from test_data: {test_data_hate_only[-1]['post_id']}\")\n",
        "print(f\"First hate sample post_id from results: {test_results_hate_only['post_id'][-1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "id": "5ac051f9",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.7936590436590436"
            ]
          },
          "execution_count": 120,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "accuracy_score(result['labels'], result['predictions'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "id": "b7OuPPFyiwBU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7OuPPFyiwBU",
        "outputId": "b633a0cc-4cea-42bd-cb90-1b44534272ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Computing ERASER metrics using DataLoader approach...\n",
            "\n",
            "[1/3] Computing plausibility metrics...\n",
            "[2/3] Computing comprehensiveness scores...\n",
            "Running inference on 36 batches...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 36/36 [00:02<00:00, 15.72batch/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Test Results:\n",
            "  Test Loss:     1.1028\n",
            "  Test Accuracy: 0.3660\n",
            "  Test F1:       0.2679\n",
            "============================================================\n",
            "[3/3] Computing sufficiency scores...\n",
            "Running inference on 36 batches...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Testing: 100%|██████████| 36/36 [00:02<00:00, 17.55batch/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "Test Results:\n",
            "  Test Loss:     0.6693\n",
            "  Test Accuracy: 0.6410\n",
            "  Test F1:       0.3906\n",
            "============================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "k = 5\n",
        "xai_results_hate_only = calculator.compute_all_metrics(test_data_hate_only, test_results_hate_only, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "id": "74ff2439",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'auprc': 0.3473608339765324,\n",
              " 'token_f1': 0.405433449674559,\n",
              " 'token_precision': 0.37668711656441717,\n",
              " 'token_recall': 0.4389297385620915,\n",
              " 'comprehensiveness': 0.34498736,\n",
              " 'sufficiency': 0.1346373,\n",
              " 'avg_rationale_length': 5}"
            ]
          },
          "execution_count": 123,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "xai_results_hate_only"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "id": "10265136",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'annotation_id': '13851720_gab', 'classification': '1', 'classification_scores': {'0': 0.008235630579292774, '1': 0.9917643666267395}, 'rationales': [{'docid': '13851720_gab', 'hard_rationale_predictions': [{'start_token': 14, 'end_token': 16}, {'start_token': 31, 'end_token': 33}, {'start_token': 35, 'end_token': 36}], 'soft_rationale_predictions': [0.052404653280973434, 0.014607744291424751, 0.010389694944024086, 0.011550650000572205, 0.020429085940122604, 0.006052706390619278, 0.01862703636288643, 0.0058250995352864265, 0.001619033282622695, 0.0015178765170276165, 0.0197729654610157, 0.0021406561136245728, 0.020396985113620758, 0.0211801715195179, 0.058572083711624146, 0.05138232931494713, 0.005330660380423069, 0.009531640447676182, 0.007873772643506527, 0.00492378044873476, 0.003486040746793151, 0.002853767480701208, 0.0019265580922365189, 0.004081204533576965, 0.019049987196922302, 0.018280990421772003, 0.023884471505880356, 0.026741070672869682, 0.02457568049430847, 0.012644847854971886, 0.012273828499019146, 0.03053266927599907, 0.027407564222812653, 0.014966500923037529, 0.018716996535658836, 0.0438690111041069, 0.37058025598526]}], 'sufficiency_classification_scores': {'0': 0.007770581636577845, '1': 0.9922294020652771}, 'comprehensiveness_classification_scores': {'0': 0.21601153910160065, '1': 0.7839884757995605}}\n",
            "{'classification_scores': {'accuracy': 0.819614711033275,\n",
            "                           'aopc_thresholds': None,\n",
            "                           'comprehensiveness': 0.3514832779895612,\n",
            "                           'comprehensiveness_aopc': None,\n",
            "                           'comprehensiveness_aopc_points': None,\n",
            "                           'comprehensiveness_entropy': -0.1743925090675903,\n",
            "                           'comprehensiveness_kl': 0.9406002192718066,\n",
            "                           'prf': {'0': {'f1-score': 0.0,\n",
            "                                         'precision': 0.0,\n",
            "                                         'recall': 0.0,\n",
            "                                         'support': 0.0},\n",
            "                                   '1': {'f1-score': 0.9008662175168431,\n",
            "                                         'precision': 1.0,\n",
            "                                         'recall': 0.819614711033275,\n",
            "                                         'support': 1142.0},\n",
            "                                   'accuracy': 0.819614711033275,\n",
            "                                   'macro avg': {'f1-score': 0.4504331087584216,\n",
            "                                                 'precision': 0.5,\n",
            "                                                 'recall': 0.4098073555166375,\n",
            "                                                 'support': 1142.0},\n",
            "                                   'weighted avg': {'f1-score': 0.900866217516843,\n",
            "                                                    'precision': 1.0,\n",
            "                                                    'recall': 0.819614711033275,\n",
            "                                                    'support': 1142.0}},\n",
            "                           'sufficiency': 0.13652717887265722,\n",
            "                           'sufficiency_aopc': None,\n",
            "                           'sufficiency_aopc_points': None,\n",
            "                           'sufficiency_entropy': -0.05306523447163887,\n",
            "                           'sufficiency_kl': 0.3193617514400618},\n",
            " 'iou_scores': [{'macro': {'f1': 0.26686047168935384,\n",
            "                           'p': 0.21085814360770577,\n",
            "                           'r': 0.3633683596030356},\n",
            "                 'micro': {'f1': 0.2111324376199616,\n",
            "                           'p': 0.1566505269154087,\n",
            "                           'r': 0.3237198351971748},\n",
            "                 'threshold': 0.5}],\n",
            " 'rationale_prf': {'instance_macro': {'f1': 0.05895741250382231,\n",
            "                                      'p': 0.048555166374781085,\n",
            "                                      'r': 0.08880618797431407},\n",
            "                   'instance_micro': {'f1': 0.054510556621881,\n",
            "                                      'p': 0.04044431785816007,\n",
            "                                      'r': 0.08357857563272514}},\n",
            " 'token_prf': {'instance_macro': {'f1': 0.44654032980221586,\n",
            "                                  'p': 0.5992994746059545,\n",
            "                                  'r': 0.4695402159702617},\n",
            "               'instance_micro': {'f1': 0.36765655261459007,\n",
            "                                  'p': 0.5989482909728309,\n",
            "                                  'r': 0.2652332531242723}},\n",
            " 'token_soft_metrics': {'auprc': 0.42698777612402133,\n",
            "                        'average_precision': 0.48709721435192294,\n",
            "                        'roc_auc_score': 0.5496917530992179}}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  4379 MainThread Error in instances: 0 instances fail validation: set()\n",
            "  6441 MainThread No sentence level predictions detected, skipping sentence-level diagnostic\n",
            "c:\\Users\\USER\\miniconda3\\envs\\transformersv2\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\USER\\miniconda3\\envs\\transformersv2\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "c:\\Users\\USER\\miniconda3\\envs\\transformersv2\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "!set PYTHONPATH=./eraserbenchmark;%PYTHONPATH% && python eraserbenchmark/rationale_benchmark/metrics.py --split test --strict --data_dir Data/explanations --results Data/eraser_formatted_results.jsonl --score_file model_explain_output.json"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5af905f2",
      "metadata": {},
      "source": [
        "# Experiment Management System\n",
        "\n",
        "This notebook now uses a systematic experiment tracking system that organizes all outputs by experiment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32db5dbf",
      "metadata": {},
      "outputs": [],
      "source": [
        "from ExperimentManager import ExperimentManager\n",
        "\n",
        "# Initialize experiment manager\n",
        "experiment_manager = ExperimentManager(base_dir=\"./experiments\")\n",
        "\n",
        "# Create a new experiment with custom name and description\n",
        "experiment_dir = experiment_manager.create_experiment(\n",
        "    config=config,\n",
        "    custom_name=\"baseline_distilbert\",  # Optional: give your experiment a meaningful name\n",
        "    description=\"Baseline model with distilbert-base-uncased, standard hyperparameters\"  # Optional: describe what this experiment is testing\n",
        ")\n",
        "\n",
        "print(f\"\\nExperiment directory: {experiment_dir}\")\n",
        "print(f\"All outputs will be saved to this directory automatically.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d28e55c0",
      "metadata": {},
      "source": [
        "## Example: Modified Training & Evaluation Pipeline with Experiment Tracking\n",
        "\n",
        "Below is how you would integrate the experiment manager into your existing pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4c5b1ffe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# FULL EXPERIMENT PIPELINE WITH TRACKING\n",
        "# ============================================================================\n",
        "\n",
        "# 1. CREATE EXPERIMENT\n",
        "experiment_manager = ExperimentManager(base_dir=\"./experiments\")\n",
        "experiment_dir = experiment_manager.create_experiment(\n",
        "    config=config,\n",
        "    custom_name=\"multi_layer_experiment\",  # Change this for each experiment\n",
        "    description=\"Testing multi-layer loss with distilbert\"\n",
        ")\n",
        "\n",
        "# 2. TRAIN MODEL (config.save_dir is automatically updated)\n",
        "model = HateClassifier(config)\n",
        "history = model.train(train_dataloader=train_loader, val_dataloader=val_loader)\n",
        "\n",
        "# Save training history\n",
        "experiment_manager.save_training_history(history)\n",
        "\n",
        "# 3. EVALUATE MODEL\n",
        "result = model.predict(test_dataloader=test_loader, return_attentions=True)\n",
        "\n",
        "# Save predictions\n",
        "experiment_manager.save_predictions(result, filename=\"test_predictions.pkl\")\n",
        "\n",
        "# 4. BIAS EVALUATION\n",
        "gmb_metrics, bias_details = calculate_gmb_metrics(\n",
        "    test_data=test_data,\n",
        "    probabilities=result['probabilities'],\n",
        "    target_groups=bias_target_groups\n",
        ")\n",
        "\n",
        "# Save bias metrics\n",
        "experiment_manager.save_bias_metrics(gmb_metrics, bias_details)\n",
        "\n",
        "# 5. XAI EVALUATION (only on hate samples)\n",
        "test_data_hate_only = []\n",
        "test_results_hate_only = {'attentions': [], 'probabilities': [], 'predictions': [], 'post_id': [], 'labels': []}\n",
        "for idx, td in enumerate(test_data):\n",
        "    if td['hard_label'] == 1:\n",
        "        test_data_hate_only.append(td)\n",
        "        test_results_hate_only['attentions'].append(result['attentions'][idx])\n",
        "        test_results_hate_only['probabilities'].append(result['probabilities'][idx])\n",
        "        test_results_hate_only['predictions'].append(result['predictions'][idx])\n",
        "        test_results_hate_only['post_id'].append(result['post_ids'][idx])\n",
        "        test_results_hate_only['labels'].append(result['labels'][idx])\n",
        "\n",
        "calculator = FaithfulnessMetrics(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    dataset_class=HateDataset,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "k = 5\n",
        "xai_results = calculator.compute_all_metrics(test_data_hate_only, test_results_hate_only, k)\n",
        "\n",
        "# Save XAI metrics\n",
        "experiment_manager.save_xai_metrics(xai_results)\n",
        "\n",
        "# Copy ERASER results\n",
        "experiment_manager.save_eraser_results('Data/eraser_formatted_results.jsonl')\n",
        "\n",
        "# 6. CREATE FINAL SUMMARY\n",
        "final_summary = {\n",
        "    \"test_accuracy\": float(result['accuracy']),\n",
        "    \"test_f1\": float(result['f1']),\n",
        "    \"test_loss\": float(result['loss']),\n",
        "    \"gmb_metrics\": gmb_metrics,\n",
        "    \"xai_metrics\": xai_results,\n",
        "    \"total_params\": sum(p.numel() for p in model.base_model.parameters()),\n",
        "}\n",
        "\n",
        "experiment_manager.save_final_metrics(final_summary)\n",
        "\n",
        "# 7. MARK EXPERIMENT AS COMPLETE\n",
        "experiment_manager.mark_complete(\n",
        "    status=\"completed\",\n",
        "    notes=\"Baseline experiment completed successfully\"\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPERIMENT COMPLETED!\")\n",
        "print(f\"All results saved to: {experiment_dir}\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9789d842",
      "metadata": {},
      "source": [
        "## Experiment Management Utilities\n",
        "\n",
        "Useful commands for managing and comparing experiments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba59f95b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# View all experiments\n",
        "experiment_manager = ExperimentManager()\n",
        "experiment_manager.print_experiment_summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b2ae2540",
      "metadata": {},
      "outputs": [],
      "source": [
        "# List only completed experiments\n",
        "completed_experiments = experiment_manager.list_experiments(status=\"completed\")\n",
        "print(f\"Found {len(completed_experiments)} completed experiments\")\n",
        "for exp in completed_experiments:\n",
        "    print(f\"  - {exp['experiment_id']}: {exp.get('description', 'No description')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bf5f661",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare multiple experiments\n",
        "experiment_ids = [\n",
        "    \"20241216_120000_baseline_a1b2c3d4\",  # Replace with actual experiment IDs\n",
        "    \"20241216_130000_multilayer_e5f6g7h8\"\n",
        "]\n",
        "comparison = experiment_manager.compare_experiments(experiment_ids)\n",
        "\n",
        "# Display comparison\n",
        "for exp in comparison[\"experiments\"]:\n",
        "    print(f\"\\nExperiment: {exp['experiment_id']}\")\n",
        "    print(f\"  Model: {exp['config'].get('model_name', 'N/A')}\")\n",
        "    print(f\"  Learning Rate: {exp['config'].get('learning_rate', 'N/A')}\")\n",
        "    print(f\"  Test F1: {exp['metrics'].get('test_f1', 'N/A')}\")\n",
        "    print(f\"  Test Accuracy: {exp['metrics'].get('test_accuracy', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "566b60e8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load results from a specific experiment\n",
        "experiment_id = \"20241216_120000_baseline_a1b2c3d4\"  # Replace with actual ID\n",
        "exp_dir = experiment_manager.get_experiment_path(experiment_id)\n",
        "\n",
        "if exp_dir:\n",
        "    # Load config\n",
        "    with open(exp_dir / \"config.json\", 'r') as f:\n",
        "        loaded_config = json.load(f)\n",
        "    \n",
        "    # Load final metrics\n",
        "    with open(exp_dir / \"metrics\" / \"final_summary.json\", 'r') as f:\n",
        "        loaded_metrics = json.load(f)\n",
        "    \n",
        "    print(f\"Loaded experiment: {experiment_id}\")\n",
        "    print(f\"Test F1: {loaded_metrics['test_f1']}\")\n",
        "    print(f\"Test Accuracy: {loaded_metrics['test_accuracy']}\")\n",
        "else:\n",
        "    print(f\"Experiment {experiment_id} not found\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42d559b8",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Export an experiment for sharing or backup\n",
        "experiment_id = \"20241216_120000_baseline_a1b2c3d4\"  # Replace with actual ID\n",
        "experiment_manager.export_experiment(experiment_id, export_path=\"./exported_experiments\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd18f25a",
      "metadata": {},
      "source": [
        "## Visualization Tools\n",
        "\n",
        "Visualize and compare experiment results:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a866b568",
      "metadata": {},
      "outputs": [],
      "source": [
        "from experiment_visualization import (\n",
        "    plot_training_curves,\n",
        "    plot_metrics_comparison,\n",
        "    plot_bias_metrics,\n",
        "    plot_xai_metrics,\n",
        "    create_experiment_report\n",
        ")\n",
        "\n",
        "# Example: Compare training curves across experiments\n",
        "experiment_ids = [\n",
        "    \"20241216_120000_baseline_a1b2c3d4\",  # Replace with your actual experiment IDs\n",
        "    \"20241216_130000_multilayer_e5f6g7h8\"\n",
        "]\n",
        "\n",
        "# Uncomment to use:\n",
        "# plot_training_curves(experiment_ids, save_path=\"training_curves.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5452b4ff",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare final metrics across experiments\n",
        "# plot_metrics_comparison(experiment_ids, save_path=\"metrics_comparison.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4fbfc7ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize bias metrics\n",
        "# plot_bias_metrics(experiment_ids, save_path=\"bias_comparison.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fda726e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize XAI metrics\n",
        "# plot_xai_metrics(experiment_ids, save_path=\"xai_comparison.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "643918a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate HTML report for a specific experiment\n",
        "# create_experiment_report(\"20241216_120000_baseline_a1b2c3d4\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "transformersv2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "000d556ee6514ee4a84badad7cce732d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "01f0ad4c4f884d5ab4bae38a6d5b8f12": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "036b2b14b78a46beb319571bc408bf94": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0466e0260a5f4774af5c4b76b495d15f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0e1b35c05596461d8b8005b6a62506da": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "199507e489064a3fb942faf130588a3d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ac419c560cb44149a6b54d2c256e1aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1be37b84a1ae427fa1a636cdd98a6fd7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2149e2e0f02d44be95617cf8c57dc8e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42983923305347428db2a3acb7d8fa98",
            "placeholder": "​",
            "style": "IPY_MODEL_95a276c276594117b51a9ed4361ad4d3",
            "value": " 483/483 [00:00&lt;00:00, 16.1kB/s]"
          }
        },
        "222943fb54e5426d89b2d50051135306": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2642b7f8abf24bfabef78ef219768375": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "27983a49e3af4cc799d4b47291fa1d2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7cbf881bf084483594c4982f61ddee22",
            "placeholder": "​",
            "style": "IPY_MODEL_555f094a99a448ed92206c2b2e40a48e",
            "value": "vocab.txt: 100%"
          }
        },
        "2ec85da142384b2499d0d5cec5516cb6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_658de63d6be44d67abae51be45398ed2",
            "placeholder": "​",
            "style": "IPY_MODEL_5a27b15a14d0485d9bec469f9fbc8297",
            "value": " 232k/232k [00:00&lt;00:00, 5.98MB/s]"
          }
        },
        "3049e83d2f184ca2b7657db368f49a1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f9d7443369a4cbf97048d9952396b58",
            "placeholder": "​",
            "style": "IPY_MODEL_2642b7f8abf24bfabef78ef219768375",
            "value": "model.safetensors: 100%"
          }
        },
        "30b2fc6ceebb4da083da94a206cdc32e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_222943fb54e5426d89b2d50051135306",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6168a397e2994c0289bf267be8bff0e7",
            "value": 48
          }
        },
        "4108f81df6394bb1956157ffa08d5b19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01f0ad4c4f884d5ab4bae38a6d5b8f12",
            "placeholder": "​",
            "style": "IPY_MODEL_658618d078554305b8d86b1a882f9ae9",
            "value": "config.json: 100%"
          }
        },
        "42983923305347428db2a3acb7d8fa98": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "555f094a99a448ed92206c2b2e40a48e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a27b15a14d0485d9bec469f9fbc8297": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "606c6a11e2564fc68ec19f701e48c625": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6168a397e2994c0289bf267be8bff0e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "624199c0f53447e7b4e32d371e4016eb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_199507e489064a3fb942faf130588a3d",
            "placeholder": "​",
            "style": "IPY_MODEL_9cae2ba8f08e46108daebf7cf8de1618",
            "value": "tokenizer.json: 100%"
          }
        },
        "658618d078554305b8d86b1a882f9ae9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "658de63d6be44d67abae51be45398ed2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "662feb5d7e6946a18fba9a23145fc034": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0e1b35c05596461d8b8005b6a62506da",
            "placeholder": "​",
            "style": "IPY_MODEL_e31b9ffcc76a4885a4b32e84a2fab004",
            "value": " 466k/466k [00:00&lt;00:00, 3.76MB/s]"
          }
        },
        "6dcba1504ae6444a9f8b1eb310b1c0ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8ac744ceb77418baeea1a40b2213bcf",
            "max": 483,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_036b2b14b78a46beb319571bc408bf94",
            "value": 483
          }
        },
        "6ea187bc06d84f018673dfc981571a6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_606c6a11e2564fc68ec19f701e48c625",
            "placeholder": "​",
            "style": "IPY_MODEL_c46fe7448c5a46428cd007265094139f",
            "value": " 268M/268M [00:03&lt;00:00, 64.7MB/s]"
          }
        },
        "7c2b46d111114553b94fcd2351e9f7bd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cbf881bf084483594c4982f61ddee22": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f9d7443369a4cbf97048d9952396b58": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "838074dd3de54ba28ff6888918598f54": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4108f81df6394bb1956157ffa08d5b19",
              "IPY_MODEL_6dcba1504ae6444a9f8b1eb310b1c0ba",
              "IPY_MODEL_2149e2e0f02d44be95617cf8c57dc8e8"
            ],
            "layout": "IPY_MODEL_955bc92f2d6141949e91648f36940ed9"
          }
        },
        "84892e9a9e0f4dc8b74b975392618dc6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4a64806bec5463c80471893e695e179",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_df22cfc7b3de4d5595f10e4250223801",
            "value": 466062
          }
        },
        "85183a453759410090f84533fd6a94d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "955bc92f2d6141949e91648f36940ed9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95a276c276594117b51a9ed4361ad4d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cae2ba8f08e46108daebf7cf8de1618": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9ebb232a0b9649789495ec0c8a519e1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d755aa71d5a1461cb932778dc5deaf52",
              "IPY_MODEL_30b2fc6ceebb4da083da94a206cdc32e",
              "IPY_MODEL_f872f5f79a884eaca0f07e00bc1a2eca"
            ],
            "layout": "IPY_MODEL_1ac419c560cb44149a6b54d2c256e1aa"
          }
        },
        "a035f097a51b4e59b5f240e08a959dda": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3049e83d2f184ca2b7657db368f49a1d",
              "IPY_MODEL_ee1a88ab031c4327ad18b3a48929bab0",
              "IPY_MODEL_6ea187bc06d84f018673dfc981571a6e"
            ],
            "layout": "IPY_MODEL_7c2b46d111114553b94fcd2351e9f7bd"
          }
        },
        "a8e708b18cdb40078565f90ca9aec543": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e09f9d26e7554480be056472a37727b9",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85183a453759410090f84533fd6a94d4",
            "value": 231508
          }
        },
        "b0bf17679f9b4e74be363704aa7bf120": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_624199c0f53447e7b4e32d371e4016eb",
              "IPY_MODEL_84892e9a9e0f4dc8b74b975392618dc6",
              "IPY_MODEL_662feb5d7e6946a18fba9a23145fc034"
            ],
            "layout": "IPY_MODEL_1be37b84a1ae427fa1a636cdd98a6fd7"
          }
        },
        "c46fe7448c5a46428cd007265094139f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6a377b8f1994f7ea5853c5ea47c072c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4a64806bec5463c80471893e695e179": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d514fc32c3b14890866e4e340eb65ff7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27983a49e3af4cc799d4b47291fa1d2e",
              "IPY_MODEL_a8e708b18cdb40078565f90ca9aec543",
              "IPY_MODEL_2ec85da142384b2499d0d5cec5516cb6"
            ],
            "layout": "IPY_MODEL_c6a377b8f1994f7ea5853c5ea47c072c"
          }
        },
        "d755aa71d5a1461cb932778dc5deaf52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e40ac1cd6e2344ca8d8ede79e35791cb",
            "placeholder": "​",
            "style": "IPY_MODEL_000d556ee6514ee4a84badad7cce732d",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "df22cfc7b3de4d5595f10e4250223801": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e09f9d26e7554480be056472a37727b9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e31b9ffcc76a4885a4b32e84a2fab004": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e40ac1cd6e2344ca8d8ede79e35791cb": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8ab7c8eca5442a88af511856f58b271": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e8ac744ceb77418baeea1a40b2213bcf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed5921c0c4994155a08f060c98de0228": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee1a88ab031c4327ad18b3a48929bab0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e8ab7c8eca5442a88af511856f58b271",
            "max": 267954768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_effb7cca8a914550a68775728bdeb881",
            "value": 267954768
          }
        },
        "effb7cca8a914550a68775728bdeb881": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f872f5f79a884eaca0f07e00bc1a2eca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed5921c0c4994155a08f060c98de0228",
            "placeholder": "​",
            "style": "IPY_MODEL_0466e0260a5f4774af5c4b76b495d15f",
            "value": " 48.0/48.0 [00:00&lt;00:00, 873B/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
