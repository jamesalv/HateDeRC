{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e65a4bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Auto reload modules when they change\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "b51cb4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "ca380a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from HateClassifier import HateClassifier\n",
    "from TrainingConfig import TrainingConfig\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9156bbc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = TrainingConfig()\n",
    "hc = HateClassifier(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "ce9e06e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    config.model_name,\n",
    "    num_labels=config.num_labels,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "864733db",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"I hate you so much\"\n",
    "human_rationale = torch.Tensor([[0, 0, 1, 1, 0, 0, 0]])  # Example rationale mask\n",
    "tokenized = tokenizer(sample_text, return_tensors=\"pt\")\n",
    "outputs = model(**tokenized, output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "b9bc477a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 1045, 5223, 2017, 2061, 2172,  102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8843782f",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_entropies = hc.compute_token_entropy(outputs.attentions, tokenized['attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "01dc54df",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_entropies_positive = -token_entropies[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4c650d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average across tokens\n",
    "token_entropies_positive = token_entropies_positive.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "82e1442f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.9459)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.log(tokenized['attention_mask'].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "e39c4583",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9847, 0.9911, 0.9876, 0.9916, 0.9900, 0.9889, 0.9715],\n",
       "       grad_fn=<DivBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_entropy = token_entropies_positive/torch.log(tokenized['attention_mask'].sum()) # --> \"percentage\" of entropy compared to its maximum possible value\n",
    "final_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e1ffbba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For rationale(d) token we want its entropy to be as low as possible but not more than the lower bound\n",
    "# Whereas for non-rationale tokens we want their entropy to be as high as possible\n",
    "\n",
    "attention_mask = tokenized['attention_mask'][0]\n",
    "valid_mask = attention_mask.bool()\n",
    "valid_indices = torch.where(valid_mask)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "956eefdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "human = human_rationale[0, valid_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6d43018a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 1., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "human"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "8e89f5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rationale token entropies: tensor([0.9876, 0.9916], grad_fn=<IndexBackward0>)\n",
      "Non-rationale token entropies: tensor([0.9847, 0.9911, 0.9900, 0.9889, 0.9715], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "rationale_mask = (human > 0)\n",
    "non_rationale_mask = (human == 0)\n",
    "\n",
    "print(\"Rationale token entropies:\", final_entropy[rationale_mask])\n",
    "print(\"Non-rationale token entropies:\", final_entropy[non_rationale_mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "74c8c8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_bound = 0.99\n",
    "upper_bound = 0.8\n",
    "\n",
    "rationale_entropies = final_entropy[rationale_mask]\n",
    "non_rationale_entropies = final_entropy[non_rationale_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "be733b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_violation = torch.relu(lower_bound - rationale_entropies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "90606eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "non_rationale_loss = -non_rationale_entropies.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5008ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(-0.9852, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_rationale_loss # This will be negative as we want to maximize entropy for non-rationale tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformersv2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
